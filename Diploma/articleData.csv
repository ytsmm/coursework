Title;Authors;Doi;Keywords;Abstract
A Dynamic Congestion Management System for InfiniBand Networks;Fabrice Mizero, Malathi Veeraraghavan, Qian Liu, Robert D. Russell, John M. Dennis;https://doi.org/10.14529/jsfi160201;InfiniBand, Congestion control, Link-by-link flow control, Cascading rate reductions, Dynamic parameter setting;While the InfiniBand link-by-link flow control helps avoid packet loss, it unfortunately causes the effects of congestion to spread through a network. Flows whose paths do not even pass through congested ports could suffer from reduced throughput. We propose a Dynamic Congestion Management System (DCMS) to address this problem. Without per-flow information, the DCMS leverages Perfomance counters of switch ports to detect onset of congestion, and determines whether-or-not victim flows are present. The DCMS then takes actions to cause an aggressive reduction in the sending rates of congestion-causing (contributor) flows if victim flows are present. On the other hand, in the absence of victim flows, the DCMS allows the contributor flows to maintain high sending rates and finish as quickly as possible.Our results show that dynamic congestion management can enable a network to serve both contributor flows and victim flows effectively. The DCMS solution operates within the constraints of the InfiniBand Standard.
Many-Core Approaches to Combinatorial Problems: case of the Langford Problem;Michael Krajecki, Julien Loiseau, François Alin, Christophe Jaillet;https://doi.org/10.14529/jsfi160202;Combinatorial problems, parallel algorithm, GPU accelerators, CUDA, Langford problem;As observed from the last TOP500 list - November 2015 -, GPUs-accelerated clusters emerge as clear evidence. But exploiting such architectures for combinatorial problem resolution remains a challenge. In this context, this paper focuses on the resolution of an academic combinatorial problem, known as Langford pairing problem, which can be solved using several approaches. We first focus on a general solving scheme based on CSP (Constraint Satisfaction Problem) formalism and backtrack called the Miller algorithm. This method enables us to compute instances up to L(2,21) using both CPU and GPU computational power with load balancing.As dedicated algorithms may still have better computation efficiency we took advantage of the Godfrey algebraic method to solve the Langford problem and implemented it using our multiGPU approach. This allowed us to recompute the last open instances, L(2, 27) and L(2, 28), respectively in less than 2 days and 23 days using best-effort computation on the ROMEO supercomputer with up to 500,000 GPU cores.
A Radical Approach to Computation with Real Numbers;John L. Gustafson;https://doi.org/10.14529/jsfi160203;Floating point, Unum computing, Computer arithmetic, Energy efficiency, Valid arithmetic;If we are willing to give up compatibility with IEEE 754 floats and design a number format with goals appropriate for 2016, we can achieve several goals simultaneously: Extremely high energy efficiency and information-per-bit, no penalty for decimal operations instead of binary, rigorous bounds on answers without the overly pessimistic bounds produced by interval methods, and unprecedented high speed up to some precision. This approach extends the ideas of unum arithmetic introduced two years ago by breaking completely from the IEEE float-type format, resulting in fixed bit size values, fixed execution time, no exception values or “gradual underflow” issues, no wasted bit patterns, and no redundant representations (like “negative zero”). As an example of the power of this format, a difficult 12-dimensional nonlinear robotic kinematics problem that has defied solvers to date is quickly solvable with absolute bounds. Also unlike interval methods, it becomes possible to operate on arbitrary disconnected subsets of the real number line with the same speed as operating on a simple bound.
Making Large-Scale Systems Observable - Another Inescapable Step Towards Exascale;Dmitry A. Nikitenko, Sergey A. Zhumatiy, Pavel A. Shvets;https://doi.org/10.14529/jsfi160205;scalable monitoring visualization, situational screen, supercomputer state visualization, joint monitoring sources, supercomputer dashboard, HPC instrument control board;The effective mastering of extremely parallel HPC system is impossible without deep understanding of all internal processes and behavior of the whole diversity of the components: computing processors and nodes, memory usage, interconnect, storage, whole software stack, cooling and so forth in detail. There are numerous visualization tools that provide information on certain components and system as a whole, but most of them have severe issues that limit appliance in real life, thus becoming inacceptable for the future system scales.Predefined monitoring systems and data sources, lack of dynamic on-the-fly reconfiguration, inflexible visualization and screening options are among the most popular issues.The proposed approach to monitoring data processing resolves the majority of known problems, providing a scalable and flexible solution based on any available monitoring systems and other data sources. The approach implementation is successfully used in every-day practice of the largest in Russia supercomputer center of Moscow State University.
Application of CUDA technology to calculation of ground states of few-body nuclei by Feynman's continual integrals method;Mikhail A. Naumenko, Vyacheslav V. Samarin;https://doi.org/10.14529/jsfi160206;NVIDIA CUDA, Feynman’s continual integrals method, few-body nuclei;The possibility of application of modern parallel computing solutions to speed up the calculations of ground states of few-body nuclei by Feynman's continual integrals method has been investigated. These calculations may sometimes require large computational time, particularly in the case of systems with many degrees of freedom. This paper presents the results of application of general-purpose computing on graphics processing units (GPGPU). The energy and the square modulus of the wave function of the ground states of several few-body nuclei have been calculated using NVIDIA CUDA technology. The results show that the use of GPGPU significantly increases the speed of calculations.
Hybrid CPU + Xeon Phi implementation of the Particle-in-Cell method for plasma simulation;Iosif B. Meyerov, Sergey I. Bastrakov, Igor A. Surmin, Alexey V. Bashinov, Evgeny S. Efimenko, Artem V. Korzhimanov, Alexander A. Muraviev, Arkady A. Gonoskov;https://doi.org/10.14529/jsfi160301;hybrid computing, Xeon Phi, Particle-in-Cell;This paper presents experimental results of Particle-in-Cell plasma simulation on a hybrid system with CPUs and Intel Xeon Phi coprocessors. We consider simulation of two relevant laser-driven particle acceleration regimes using the Particle-in-Cell code PICADOR. On a node of a cluster with 2 CPUs and 2 Xeon Phi coprocessors the hybrid CPU + Xeon Phi configuration allows to fully utilize the computational resources of the node. It outperforms both CPU-only and Xeon Phi-only configurations with the speedups between 1.36 x and 1.68 x.
Easy Access to HPC Resources through the Application GUI;Matthijs van Waveren, Ahmed Seif El Nawasany, Nasr Hassanein, David Moon, Niall O'Byrnes, Alain Clo, Karthikeyan Murugan, Antonio Arena;https://doi.org/10.14529/jsfi160302;KAUST, Remote Job Submission, Middleware, MATLAB, VASP, MedeA, ADF;The computing environment at the King Abdullah University of Science and Technology (KAUST) is growing in size and complexity. KAUST hosts the tenth fastest supercomputer in the world (Shaheen II) and several HPC clusters. Researchers can be inhibited by the complexity, as they need to learn new languages and execute many tasks in order to access the HPC clusters and the supercomputer. In order to simplify the access, we have developed an interface between the applications and the clusters and supercomputer that automates the transfer of input data and job submission and also the retrieval of results to the researcher’s local workstation. The innovation is that the user now submits his jobs from within the application GUI on his workstation, and does not have to directly log into the clusters or supercomputer anymore. This article details the solution and its benefits to the researchers.
Predicting I/O Perfomance in HPC Using Artificial Neural Networks;Jan Fabian Schmid, Julian M. Kunkel;https://doi.org/10.14529/jsfi160303;file system, Perfomance, modeling I/O, artificial neural networks;The prediction of file access times is an important part for the modeling of supercomputer's storage systems. These models can be used to develop analysis tools which support the users to integrate efficient I/O behavior.In this paper, we analyze and predict the access times of a Lustre file system from the client perspective. Therefore, we measure file access times in various test series and developed different models for predicting access times.  The evaluation shows that in models utilizing artificial neural networks the average prediciton error is about 30% smaller than in linear models. A phenomenon in the distribution of file access times is of particular interest: File accesses with identical parameters show several typical access times.The typical access times usually differ by orders of magnitude and can be explained with a different processing of the file accesses in the storage system - an alternative I/O path. We investigate a method to automatically determine the alternative I/O path and quantify the significance of knowledge about the internal processing. It is shown that the prediction error is improved significantly with this approach.
Analyzing Data Properties using Statistical Sampling – Illustrated on Scientific File Formats;Julian Martin Kunkel;https://doi.org/10.14529/jsfi160304;Scientific Data, Compression, Analyzing Data Properties;Understanding the characteristics of data stored in data centers helps computer scientists in identifying the most suitable storage infrastructure to deal with these workloads. For example, knowing the relevance of file formats allows optimizing the relevant formats but also helps in a procurement to define benchmarks that cover these formats. Existing studies that investigate Perfomance improvements and techniques for data reduction such as deduplication and compression operate on a subset of data. Some of those studies claim the selected data is representative and scale their result to the scale of the data center. One hurdle of running novel schemes on the complete data is the vast amount of data stored and, thus, the resources required to analyze the complete data set. Even if this would be feasible, the costs for running many of those experiments must be justified.This paper investigates stochastic sampling methods to compute and analyze quantities of interest on file numbers but also on the occupied storage space. It will be demonstrated that on our production system, scanning 1% of files and data volume is sufficient to deduct conclusions. This speeds up the analysis process and reduces costs of such studies significantly.
Spectral Domain Decomposition Using Local Fourier Basis: Application to Ultrasound Simulation on a Cluster of GPUs;Jiri Jaros, Filip Vaverka, Bradley E. Treeby;https://doi.org/10.14529/jsfi160305;domain decomposition, ultrasound simulation, spectral methods, GPU, FFT, local Fourier basis;The simulation of ultrasound wave propagation through biological tissue has a wide range of practical applications. However, large grid sizes are generally needed to capture the phenomena of interest. Here, a novel approach to reduce the computational complexity is presented. The model uses an accelerated k-space pseudospectral method which enables more than one hundred GPUs to be exploited to solve problems with more than 3*10^9 grid points. The classic communication bottleneck of Fourier spectral methods, all-to-all global data exchange, is overcome by the application of domain decomposition using local Fourier basis. Compared to global domain decomposition, for a grid size of 1536 x 1024 x 2048, this reduces the simulation time by a factor of 7.5 and the simulation cost by a factor of 3.8.
HCA aware Parallel Communication Library: A feasibility study for offloading MPI requirements;Kedar Kulkarni, Shreeya Badhe, Geetanjali Gadre;https://doi.org/10.14529/jsfi160306;MPI, HCA, Communication Pattern Offloading, High Perfomance Networks, Communication Library;Message Passing Interface (MPI) is a standardized message passing system, independent of underlying network, and the most widely used parallel programming paradigm. The communication library should make full use of the Host Channel Adapter (HCA) characteristics to maximize Perfomance of the HPC cluster. The communication library may not able to take full advantage of the underlying network adapter, if the library is made generalized. This can have a significant impact on the Perfomance.Our primary goal is to develop a network dependent message passing library called a Parallel Communication Library (PCL) that will exploit C-DAC's proprietary PARAMNet HCA features for efficient message transfer. Using PCL, we intend to observe the feasibility of the network and Perfomance enhancement for additional features. The objective is to carry out different trials by implementing additional features and analyze the implications which would give us more insight towards suitability of transport offload/onload mechanism. This experimentation would give us feedbacks for designing the next generation architecture.
Parallel Processing Model for Cholesky Decomposition Algorithm in AlgoWiki Project;Alexander S. Antonov, Alexey V. Frolov, Hiroaki Kobayashi, Igor N. Konshin, Alexey M. Teplov, Vadim V. Voevodin, Vladimir V. Voevodin;https://doi.org/10.14529/jsfi160307;AlgoWiki, algorithm properties, Cholesky decomposition, memory access locality, dynamic characteristics, scalability;The comprehensive analysis of algorithmic properties of well-known. Cholesky decomposition was performed on the basis of multifold AlgoWiki technologies. There was performed a detailed analysis of information graph, data structure, memory access profile, computation locality, scalability and other algorithm properties, that allow us to demonstrate a lot of unevident properties split up. into machine-independent and machine-dependent subsets. A comprehension of the parallel algorithm structure provide us with the possibility to efficiently implement the algorithm at hardware platform specified.
In Situ Exploration of Particle Simulations with CPU Ray Tracing;Will Usher, Ingo Wald, Aaron Knoll, Michael Papka, Valerio Pascucci;https://doi.org/10.14529/jsfi160401;in situ rendering, parallel systems, point-based data, CPU and GPU clusters;We present a system for interactive in situ visualization of large particle simulations, suitable for general CPU-based HPC architectures. As simulations grow in scale, in situ methods are needed to alleviate IO bottlenecks and visualize data at full spatio-temporal resolution. We use a lightweight loosely-coupled layer serving distributed data from the simulation to a data-parallel renderer running in separate processes. Leveraging the OSPRay ray tracing framework for visualization and balanced P-k-d trees, we can render simulation data in real-time, as they arrive, with negligible memory overhead. This flexible solution allows users to perform exploratory in situ visualization on the same computational resources as the simulation code, on dedicated visualization clusters or remote workstations, via a standalone rendering client that can be connected or disconnected as needed.  We evaluate this system on simulations with up to 227M particles in the LAMMPS and Uintah computational frameworks, and show that our approach provides many of the advantages of tightly-coupled systems, with the flexibility to render on a wide variety of remote and coprocessing resources.
In Situ Visualization and Production of Extract Databases;Brad Joseph Whitlock, Earl P. N. Duque;https://doi.org/10.14529/jsfi160402;In Situ, HPC, Visualization, Extract Database, SENSEI, Libsim, Workflow; Simulations running at high concurrency on HPC systems generate large volumes of data that are impractical to write to disk due to time and storage constraints. Applications often adapt by saving data infrequently, resulting in datasets with poor temporal resolution. This can make datasets difficult to interpret during post hoc visualization and analysis, or worse, it can lead to lost science. In Situ visualization and analysis can enable efficient production of small data products such as rendered images or surface extracts that consist of polygonal geometry plus fields. These data products are far smaller than their source data and can be processed much more economically in a traditional post hoc workflow using far fewer computational resources. We used the SENSEI and Libsim in situ infrastructures to implement rendering workflow and surface data extraction workflows in the AVF-LESLIE combustion code. These workflows were then demonstrated at high levels of concurrency and showed significant data reductions and limited impact on the simulation runtime. 
In situ, steerable, hardware-independent and data-structure agnostic visualization with ISAAC;Alexander Matthes, Axel Huebl, René Widera, Sebastian Grottel, Stefan Gumhold, Michael Bussmann;https://doi.org/10.14529/jsfi160403;"HPC, in situ, visualization, live rendering, petascale, particle-in-cell,
C++11, CUDA, Alpaka, FOSS";The computation power of supercomputers grows faster than the bandwidth of their storage and network. Especially applications using hardware accelerators like Nvidia GPUs cannot save enough data to be analyzed in a later step. There is a high risk of loosing important scientific information. We introduce the in situ template library ISAAC which enables arbitrary applications like scientific simulations to live visualize their data without the need of deep copy operations or data transformation using the very same compute node and hardware accelerator the data is already residing on. Arbitrary meta data can be added to the renderings and user defined steering commands can be asynchronously sent back to the running application. Using a aggregating server, ISAAC streams the interactive visualization video and enables user to access their applications from everywhere.
Preparing for In Situ Processing on Upcoming Leading-edge Supercomputers;James Kress, Randy Michael Churchill, Scott Klasky, Mark Kim, Hank Childs, David Pugmire;https://doi.org/10.14529/jsfi160404;Scientific Visualization, In Situ Methods, Data Staging Methods, Data Reductions, HPC;HPC applications are producing increasingly large amounts of data and placing enormous stress on current capabilities for traditional post-hoc visualization techniques. Because of the growing compute and I/O imbalance, data reductions, including in situ visualization, are required. These reduced data are used for analysis and visualization in a variety of different ways. Many of he visualization and analysis requirements are known a priori, but when they are not, scientists are dependent on the reduced data to accurately represent the simulation in post hoc analysis. The contributions of this paper is a description of the directions we are pursuing to assist a large scale fusion simulation code succeed on the next generation of supercomputers. These directions include the role of in situ processing for performing data reductions, as well as the tradeoffs between data size and data integrity within the context of complex operations in a typical scientific workflow.
Analysis of CPU Usage Data Properties and their possible impact on Perfomance Monitoring;Konstantin S. Stefanov, Alexey A. Gradskov;https://doi.org/10.14529/jsfi160405;Perfomance monitoring, sensor properties, sampling rate, CPU usage, CPU load level;CPU usage data (CPU user, system, iowait etc. load levels) are often the basic data used for Perfomance monitoring. The source of these data is  the operating system. In this paper we analyze some properties of CPU usage data provided by Linux kernel. We examine kernel source code and provide test results to find which level of accuracy and precision one may expect when using CPU load level data.
Parallel algorithm for 3D modeling of monochromatic acoustic field by the method of integral equations;Mikhail S. Malovichko, Nikolay E. Khokhlov, Nikolay B. Yavich, Michael S. Zhdanov;https://doi.org/10.14529/jsfi160406;integral equations, acoustics, seismics, MPI, OpenMP;We present a parallel algorithm for solution of the three-dimensional Helmholtz equation in the frequency domain by the method of volume integral equations. The algorithm is applied to seismic forward modeling. The method of integral equations reduces the size of the problem by dividing the geologic model into the anomalous and background parts, but leads to a dense system matrix. A tolerable memory consumption and numerical complexity were achieved by applying an iterative solver, accompanied by an effective matrix-vector multiplication operation, based on the fast Fourier transform. We used OpenMP to speed up the matrix-vector multiplication, while MPI was used to speed up the equation system solver, and also for parallelizing across multiple sources. Practical examples and efficiency tests are presented.
Design and Implementation of the PULSAR Programming System for Large Scale Computing;Jakub Kurzak, Piotr Luszczek, Ichitaro Yamazaki, Yves Robert, Jack Dongarra;https://doi.org/10.14529/jsfi170101;runtime scheduling, dataflow scheduling, distributed computing, massively parallel computing, multicore processors, hardware accelerators, virtualization, systolic arrays;The objective of the PULSAR project was to design a programming model suitable for large scale machines with complex memory hierarchies, and to deliver a prototype implementation of a runtime system supporting that model. PULSAR tackled the challenge by proposing a programming model based on systolic processing and virtualization. The PULSAR programming model is quite simple, with point-to-point channels as the main communication abstraction. The runtime implementation is very lightweight and fully distributed, and provides multithreading, message-passing and multi-GPU offload capabilities. Perfomance evaluation shows good scalability up to one thousand nodes with one thousand GPU accelerators.
Workflows for Science: a Challenge when Facing the Convergence of HPC and Big Data;Rosa M Badia, Eduard Ayguade, Jesus Labarta;https://doi.org/10.14529/jsfi170102;workflows, scientific applications, Big Data;"Workflows have been used traditionally as a mean to describe and implement the computing usually parametric studies and explorations searching for the best solution  that  scientific researchers want to perform. A workflow is not only the computing application, but a way of documenting a process.  Science workflows may be of very different nature depending on the area of research, matching the actual experiment that the scientist want to perform. Workflow Management Systems are environments that offer the researchers tools to define, publish, execute and document their workflows. In some cases, the science workflows are used to generate data; in other cases are used to analyse existing data; only in a few cases, workflows are used both to generate and analyse  data. The design of experiments is in some cases generated blindly, without a clear idea of which points are relevant to be computed/simulated, ending up with huge amount of computation that is performed following a brute-force strategy. However, the evolution of systems and the large amount of data generated by the applications require an in-situ analysis of the data, thus requiring new solutions to develop workflows that includes both the simulation/computational part and the analytic part. What is more, the fact that both components, computation and analytics, can be run together  will enable the possibility of defining more dynamic workflows, with new computations being decided by the analytics in a more efficient way.The first part of the paper will review current approaches that a set of scientific communities follows in the development of their workflows. Due to the election of several scientific communities and use cases using a specific Workflow Management System, this survey maybe incomplete with regard a complete revision of the literature about workflows, but we expect that the reader appreaciates the effort performed in trying to see the scientific communities needs and requirements. The second part of the paper will propose a new software architecture to develop a new  family of end-to-end workflows that enables the management of  dynamic workflows composed of simulations, analytics and visualization, including inputs/outputs from streams."
A Survey: Runtime Software Systems for HPC;Thomas Sterling, Matthew Anderson, Maciej Brodowicz;https://doi.org/10.14529/jsfi170103;runtime system, parallel computing, scalability, survey, HPC;HPC system design and operation are challenged by the critical requirements for signicant advances in eciency, scalability, user productivity, and Perfomance portability, even at the end of Moore's Law with approaching nano-scale semiconductor technology. Conventional practices employ distributed memory message passing programming interfaces, sometimes combining second level thread-based intra shared memory node interfaces such as OpenMP or with means of controlling heterogeneous components such as OpenCL for GPUs. While these methods include some modest runtime control, they are principally course grained and statically scheduled. Yet, Perfomance for many real-world applications yield eciencies of less than 10% although some benchmarks may achieve 80% eciency or better (e.g., HPL). To address these challenges, strategies employing runtime software systems are being pursued to exploit information about the status of the application and the system hardware operation throughout the execution for purposes of introspection to guide the task scheduling and resource management in support of dynamic adaptive control. Runtime systems provide adaptive means to reduce the eects of starvation, latency, overhead, and contention. While each is unique in its details, many share common properties such as multi-tasking either preemptive or non-preemptive, message-driven computation such as active messages, sophisticated ne-grain synchronization such as dataow and futures contructs, global name or address spaces, and control policies for optimizing task scheduling in part to address the uncertainty of asynchrony. This survey will identify key parameters and properties of modern and sometimes experimental runtime systems actively employed today and provide a detailed description, summary, and comparison within a shared space of dimensions. It is not the intent of this paper to determine which is better or worse but rather to provide sucient detail to permit the reader to select among them according to individual need. 
xSDK Foundations: Toward an Extreme-scale Scientific Software Development Kit;Roscoe Bartlett, Irina Demeshko, Todd Gamblin, Glenn Hammond, Michael Allen Heroux, Jeffrey Johnson, Alicia Klinvex, Xiaoye Li, Lois Curfman McInnes, J. David Moulton, Daniel Osei-Kuffuor, Jason Sarich, Barry Smith, James Willenbring, Ulrike Meier Yang;https://doi.org/10.14529/jsfi170104;xSDK, Extreme-scale scientific software development kit, numerical libraries, software interoperability, sustainability;Extreme-scale computational science increasingly demands multiscale and multiphysics formulations. Combining software developed by independent groups is imperative: no single team has resources for all predictive science and decision support capabilities. Scientific libraries provide high-quality, reusable software components for constructing applications with improved robustness and portability.  However, without coordination, many libraries cannot be easily composed.  Namespace collisions, inconsistent arguments, lack of third-party software versioning, and additional difficulties make composition costly.The Extreme-scale Scientific Software Development Kit (xSDK) defines community policies to improve code quality and compatibility across independently developed packages (hypre, PETSc, SuperLU, Trilinos, and Alquimia) and provides a foundation for addressing broader issues in software interoperability, Perfomance portability, and sustainability.  The xSDK provides turnkey installation of member software and seamless combination of aggregate capabilities, and it marks first steps toward extreme-scale scientific software ecosystems from which future applications can be composed rapidly with assured quality and scalability.
Perfomance Portability of HPC Discovery Science Software: Fusion Energy Turbulence Simulations at Extreme Scale;William Tang, Bei Wang, Stephane Ethier, Zhihong Lin;https://doi.org/10.14529/jsfi170105;Turbulence Simulations, Particle-In-Cell, Portability, HPC;As HPC R&D moves forward on a variety of “path to exascale” architectures today, an associated objective is to demonstrate Perfomance portability of discovery-science-capable software.  Important application domains, such as Magnetic Fusion Energy (MFE), have improved modelling of increasingly complex physical systems -- especially with respect to reducing “time-to-solution” as well as  “energy to solution.”  The emergence of new insights on confinement scaling in MFE systems has been aided significantly by efficient software capable of harnessing powerful supercomputers to carry out simulations with unprecedented resolution and temporal duration to address increasing problem sizes.  Specifically, highly scalable particle-in-cell (PIC) programing methodology is used in this paper to demonstrate how modern scientific applications can achieve efficient architecture-dependent optimizations of Perfomance scaling and code portability for path-to-exascale platforms.
Using HPC to Create and Freely Distribute the South Asian Genomic Database, Necessary for Precision Medicine in this Population;Asmi H. Shah, Jonathan D. Picker, Saumya S. Jamuar;https://doi.org/10.14529/jsfi170201;Precision Medicine, ggcINDIA, Beacon Network, South Asian Genome, Genome Data Sharing;Precision medicine is an emerging approach for disease treatment and prevention that takes into account individual variability in genes, environment, and lifestyle for each person”. Efforts to implement precision medicine have gained traction in recent years due to significantly increased understanding of the role of genetic variations in human disease over the past decade. However, delivery of precision medicine requires robust population specific reference genome datasets for full appreciation of existing natural variation. The majority of publicly available genomic databases are primarily derived from Caucasian populations and do not fully address the diversity of Asian populations. In an effort to address this problem, we have aggregated and built a genomic database, ggcINDIA, specifically for South Asian populations. In collaboration with Global Alliance for Genomics and Health (GA4GH), we have made this database publicly available to the community through the GA4GH's Beacon project. ggcINDIA represents the first Beacon for South Asian populations. As more data is generated and aggregated, the ggcINDIA beacon will provide the precise genomic data that is critical to the delivery of precision medicine within South Asia.
An Application of GPU Acceleration in CFD Simulation for Insect Flight;Yang Yao, Khoon-Seng Yeo;https://doi.org/10.14529/jsfi170202;insect flight, free flight, general-purpose computing on graphics processing units (GPGPU), computational fluid dynamics (CFD), flapping-wing aerodynamics;The mobility and maneuverability of winged insects have been attracting attention, but the knowledge on the behavior of free-flying insects is still far from complete. This paper presents a computational study on the aerodynamics and kinematics of a free-flying model fruit-fly. An existing integrative computational fluid dynamics (CFD) framework was further developed using CUDA technology and adapted for the free flight simulation on heterogenous clusters.The application of general-purpose computing on graphics processing units (GPGPU) significantly accelerated the insect flight simulation and made it less computational expensive to find out the steady state of the flight using CFD approach.A variety of free flight scenarios has been simulated using the present numerical approach, including hovering, fast rectilinear flight, and complex maneuvers. The vortical flow surrounding the model fly in steady flight was visualized and analyzed. The present results showed good consistency with previous studies.
Simultac Fonton: A Fine-Grain Architecture for Extreme Perfomance beyond Moore's Law;Maciej Brodowicz, Thomas Sterling;https://doi.org/10.14529/jsfi170203;HPC, parallel processing, exascale, non-von Neumann architecture, cellular architecture;"With nano-scale technology and Moore's Law end, architecture advance serves as the principal means of achieving enhanced efficiency and scalability into the exascale era. Ironically, the field that has demonstrated the greatest leaps of technology in the history of humankind, has retained its roots in its earliest strategy, the von Neumann architecture model which has imposed tradeoffs no longer valid for today's semiconductor technologies, although they were suitable through the 1980s. Essentially all commercial computers, including HPC, have been and are von Neumann derivatives. The bottlenecks imposed by this heritage are the emphasis on ALU/FPU utilization, single instruction issue and sequential consistency, and the separation of memory and processing logic (""von Neumann bottleneck""). Here the authors explore the possibility and implications of one class of non von Neumann architecture based on cellular structures, asynchronous multi-tasking, distributed shared memory, and message-driven computation. ""Continuum Computer Architecture"" is introduced as a genus of ultra-fine-grained architectures where complexity of operation is an emergent behavior of simplicity of design combined with highly replicated elements. An exemplar species of CCA, ""Simultac"" is considered comprising billions of simple elements, ""fontons"", of merged properties of data storage and movement combined with logical transformations. Employing the ParalleX execution model and a variation of the HPX+ runtime system software, the Simultac may provide the path to cost effective data analytics and machine learning as well as dynamic adaptive simulations in the trans-exaOPS Perfomance regime."
The Simultaneous Transmit And Receive (STAR) Message Protocol;Earle Jennings;https://doi.org/10.14529/jsfi170204;MPI, optical communications, exascale, HPC, security, router, memory wall, fault resilience;The STAR protocol is introduced, which solves three problems with MPI, a well known secur- ity problem, and three exascale communication problems. Optical implementations are developed compatible with 100 Gbit/sec Ethernet. Automatic fault resilience mechanisms are discussed, which improve HPC quality of service, and meet the exascale reliability and resilience challenges. The bandwidth problem for exascale computers interfacing with data centers is solved. The STAR protocol is combined with the SiMulPro core architecture (discussed in another article of this issue). The combination enables data centers, handheld computers, networked sensors, and super- computers, to be invulnerable to memory fault injection of viruses and rootkits. 
Core Module Optimizing PDE Sparse Matrix Models With HPCG Example;Earle Jennings;https://doi.org/10.14529/jsfi170205;HPCG, superscalar, sparse matrix, partial differential equation, memory wall, HPC; This paper introduces a fundamentally new computer architecture for supercomputers. The core module is application compatible with an existing superscalar microprocessor, with minimized energy use, and is optimized for local sparse matrix operations. Optimized sparse matrix manip- ulation is discussed by analyzing the High Perfomance Conjugate Gradient (HPCG) benchmark speci...cation. This analysis shows how the DRAM memory wall is removed for this benchmark, and for sparse matrix models of partial di¤erential equations (PDEs) for a wide cross section of applications. By giving the programmer improved control over the con...guration of the super- computer, the potential for communication problems is minimized. Application compatibility is achieved while removing the superscalar instruction interpreter and multi-thread controller from the existing microprocessor’s hardware. These are transformed into compile-time utilities. The instruction cache is removed through an innovation in VLIW instruction processing. The data caches are unnecessary and are turned o¤ in order to optimally implement sparse matrix models. 
Beating Floating Point at its Own Game: Posit Arithmetic;John L. Gustafson, Isaac T. Yonemoto;https://doi.org/10.14529/jsfi170206;"computer arithmetic, energy-efficient computing, floating point, posits, LINPACK,
linear algebra, neural networks, unum computing, valid arithmetic";"A new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers (floats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and “Not-a-Number” (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality. A comprehensive series of benchmarks compares floats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than “approximate computing” methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than floats of the same size; in some cases, a 32-bit posit may safely replace a 64-bit float. In other words, posits beat floats at their own game. "
Resilience Design Patterns: A Structured Approach to Resilience at Extreme Scale;Saurabh Hukerikar, Christian Engelmann;https://doi.org/10.14529/jsfi170301;HPC, resilience, fault tolerance, design patterns;Reliability is a serious concern for future extreme-scale HPC (HPC) systems. Projections based on the current generation of HPC systems and technology roadmaps suggest the prevalence of very high fault rates in future systems. The errors resulting from these faults will propagate and generate various kinds of failures, which may result in outcomes ranging from result corruptions to catastrophic application crashes. Therefore, the resilience challenge for extreme-scale HPC systems requires management of various hardware and software technologies that are capable of handling a broad set of fault models at accelerated fault rates. Also, due to practical limits on power consumption in HPC systems future systems are likely to embrace innovative architectures, increasing the levels of hardware and software complexities. As a result, the techniques that seek to improve resilience must navigate the complex trade-off space between resilience and the overheads to power consumption and Perfomance. While the HPC community has developed various resilience solutions, application-level techniques as well as system-based solutions, the solution space of HPC resilience techniques remains fragmented. There are no formal methods and metrics to investigate and evaluate resilience holistically in HPC systems that consider impact scope, handling coverage, and Perfomance & power efficiency across the system stack. Additionally, few of the current approaches are portable to newer architectures and software environments that will be deployed on future systems.In this paper, we develop a structured approach to the management of HPC resilience using the concept of resilience-based design patterns. A design pattern is a general repeatable solution to a commonly occurring problem. We identify the commonly occurring problems and solutions used to deal with faults, errors and failures in HPC systems. Each established solution is described in the form of a pattern that addresses concrete problems in the design of resilient systems. The complete catalog of resilience design patterns provides designers with reusable design elements. We also define a framework that enhances a designer's understanding of the important constraints and opportunities for the design patterns to be implemented and deployed at various layers of the system stack. This design framework may be used to establish mechanisms and interfaces to coordinate flexible fault management across hardware and software components. The framework also supports optimization of the cost-benefit trade-offs among Perfomance, resilience, and power consumption. The overall goal of this work is to enable a systematic methodology for the design and evaluation of resilience technologies in extreme-scale HPC systems that keep scientific applications running to a correct solution in a timely and cost-efficient manner despite frequent faults, errors, and failures of various types.
Perfomance Evaluation of Runtime Data Exploration Framework based on In-Situ Particle Based Volume Rendering;Takuma Kawamura, Tomoyuki Noda, Yasuhiro Idomura;https://doi.org/10.14529/jsfi170302;in-situ visualization, volume rendering, runtime steering, strong scaling, Perfomance evaluation;We examine the Perfomance of the in-situ data exploration framework based on the in-situ Particle Based Volume Rendering (In-Situ PBVR) on the latest many-core platform. In-Situ PBVR converts extreme scale volume data into small rendering primitive particle data via parallel Monte-Carlo sampling without costly visibility ordering. This feature avoids severe bottlenecks such as limited memory size per node and significant Perfomance gap between computation and inter-node communication. In addition, remote in-situ data exploration is enabled by asynchronous file-based control sequences, which transfer the small particle data to client PCs, generate view-independent volume rendering images on client PCs, and change visualization parameters at runtime.In-Situ PBVR shows excellent strong scaling with low memory usage up to ~100k cores on the Oakforest-PACS, which consists of 8,208 Intel Xeon Phi7250 (Knights Landing) processors. This Perfomance is compatible with the remote in-situ data exploration capability.
Development and Integration of an In-Situ Framework for Flow Visualization of Large-Scale, Unsteady Phenomena in ICON;Michael Vetter, Stephan Olbrich;https://doi.org/10.14529/jsfi170303;DSVR, ICON, in-situ visualization, visualization framework;With large-scale simulation models on massively parallel supercomputers generating increasingly large data sets, in-situ visualization is a promising way to avoid bottlenecks. Enabling in-situ visualization in a simulation model asks for special attention to the interface between a parallel simulation model and the data analysis part of the visualization, and to presentation and interaction scenarios. Modifications to scientific workflows would potentially result in a paradigm shift, which affects compute and data intensive applications generally. We present our approach for enabling in-situ visualization within the highly parallelized climate model ICON using the DSVR visualization framework. We focus on the requirements for generalized grid and data structures, and for universal, scalable algorithms for volume and flow visualization of time series. In-situ pathline extraction as a technique for the visualization of unsteady flows has been integrated in the climate simulation model ICON and verified in first studies.
In Situ Visualization for 3D Agent-Based Vocal Fold Inflammation and Repair Simulation;Nuttiiya Seekhao, Joseph JaJa, Luc Mongeau, Nicole Y.K. Li-Jessen;https://doi.org/10.14529/jsfi170304;"in situ, visualization, vocal fold, systems biology simulation, agent based modeling,
tissue inflammation and repair, computational steering";A fast and insightful visualization is essential in modeling biological system behaviors and understanding underlying inter-cellular mechanisms. High fidelity models produce billions of data points per time step, making in situ visualization techniques extremely desirable as they mitigate I/O bottlenecks and provide computational steering capability. In this work, we present a novel high-Perfomance scheme to couple in situ visualization with the simulation of the vocal fold inflammation and repair using little to no extra cost in execution time or computing resources. The visualization component is first optimized with an adaptive sampling scheme to accelerate the rendering process while maintaining the precision of the displayed visual results. Our software employs VirtualGL to perform visualization in situ. The scheme overlaps visualization and simulation, resulting in the optimal utilization of computing resources. This results in an in situ system biology simulation suite capable of remote simulation of 17 million biological cells and 1.2 billion chemical data points, remote visualization of the results, and delivery of visualized frames with aggregated statistics to remote clients in real-time.
Seismic Processing Perfomance Analysis on Different Hardware Environment;Ekaterina Olegovna Tyutlyaeva, Sergey Konyukhov, Igor Odintsov, Alexander Moskovsky;https://doi.org/10.14529/jsfi170305;Perfomance analysis, architecture comparison, seismic processing profiling, powerusage analysis, application behavior analysis;In this research we have used computational-intensive software that implements 2D and 3D seismic migrations to study mini-application behavior for a set of the computational architectures. In addition to three architecture type comparative analysis, two CPU generation comparisons have been done.The dynamic behavior of chosen mini-applications was studied using BSC Perfomance analysis tools to identify their common features. In summary, we observe the best Perfomance of mini-applications on Intel Xeon E5-2698 CPU generation 4.  Intel Xeon Phi 7250 peculiar architectural characteristics requires careful source code optimizations to help the compiler to effectively vectorize time-consuming loops and to improve the cache locality in order to achieve higher Perfomance level. Elbrus-4S CPU is theoretically suitable for such kind of applications, but currently observed Perfomance is an order of magnitude less than on Xeon E5 family, we believe that the frequency and RAM bandwidth increasing, as well as source code optimization work could improve it’s Perfomance.
Exploring Scheduling Effects on Task Perfomance with TaskInsight;Germán Ceballos, Andra Hugo, Erik Hagersten, David Black-Schaffer;https://doi.org/10.14529/jsfi170306;task-based scheduling, data reuse, data locality, cache model;The complex memory hierarchies of nowadays machines make it very difficult to estimate the execution time of the tasks as depending on where the data is placed in memory, tasks of the same type may end up having different Perfomance. Multiple scheduling heuristics have managed to improve Perfomance by taking into account memory-related properties such as data locality and cache sharing. However, we may see tasks in certain applications or phases of applications that take little or no advantage of these optimizations. Without understanding when such optimizations are effective, we may trigger unnecessary overhead at runtime level.In previous work, we introduced TaskInsight, a technique to characterize how the memory behavior of the application is affected by different task schedulers through the analysis of data reuse across tasks. We now use this tool to dynamically trace the scheduling decisions of multithreaded applications through their execution and analyze how memory reuse can provide information on when and why locality-aware optimizations are effective and impact Perfomance.We demonstrate how we can detect particular scheduling decisions that produced a variation in Perfomance, and the underlying reasons when applying TaskInsight to several of the Montblanc benchmarks. This flexible insight is key both for the programmer and runtime to allow assigning the optimal scheduling policy to certain executions or phases.
From Processing-in-Memory to Processing-in-Storage;Roman Kaplan, Leonid Yavits, Ran Ginosar;https://doi.org/10.14529/jsfi170307;Content Addressable Memory, Associative Processing, In-Storage Processing, Memristors;Near-data in-memory processing research has been gaining momentum in recent years. Typical processing-in-memory architecture places a single or several processing elements next to a volatile memory, enabling processing without transferring data to the host CPU. The increased bandwidth to and from volatile memory leads to Perfomance gain. However processing-in-memory does not alleviate von Neumann bottleneck for big data problems, where datasets are too large to fit in main memory.   We present a novel processing-in-storage system based on Resistive Content Addressable Memory (ReCAM). It functions simultaneously as a mass storage and as a massively parallel associative processor. ReCAM processing-in-storage resolves the bandwidth wall by keeping computation inside the storage arrays, without transferring it up the memory hierarchy.   We show that ReCAM based processing-in-storage architecture may outperform existing processing-in-memory and accelerator based designs. ReCAM processing-in-storage implementation of Smith-Waterman DNA sequence alignment reaches a speedup of almost five over a GPU cluster. An implementation of in-storage inline data deduplication is presented and shown to achieve orders of magnitude higher throughput than traditional CPU and DRAM based systems.
Towards A Data Centric System Architecture: SHARP;Richard Graham, Gil Bloch, Devendar Bureddy, Gilad Shainer, Brian Smith;https://doi.org/10.14529/jsfi170401;data centric architecture, SHARP, collectives, MPI;Increased system size and a greater reliance on utilizing system parallelism to achieve computational needs, requires innovative system architectures to meet the simulation challenges. The SHARP technology is a step towards a data-centric architecture, where data is manipulated throughout the system. This paper introduces a new SHARP optimization, and studies aspects that impact application Perfomance in a data-centric environment. The use of UD-Multicast to distribute aggregation results is introduced, reducing the latency of an eight-byte MPI Allreduce() across 128 nodes by 16%. Use of reduction trees that avoid the inter-socket bus further improves the eight-byte MPI Allreduce() latency across 128 nodes, with 28 processes per node, by 18%. The distribution of latency across processes in the communicator is studied, as is the capacity of the system to process concurrent aggregation operations.
Towards Decoupling the Selection of Compression Algorithms from Quality Constraints – An Investigation of Lossy Compression Efficiency;Julian Martin Kunkel, Anastasiia Novikova, Eugen Betke;https://doi.org/10.14529/jsfi170402;data reduction, compression, lossy, climate data;Data intense scientific domains use data compression to reduce the storage space needed. Lossless data compression preserves information accurately but lossy data compression can achieve much higher compression rates depending on the tolerable error margins. There are many ways of defining precision and to exploit this knowledge, therefore, the field of lossy compression is subject to active research. From the perspective of a scientist, the qualitative definition about the implied loss of data precision should only matter.With the Scientific Compression Library (SCIL), we are developing a meta-compressor that allows users to define various quantities for acceptable error and expected Perfomance behavior. The library then picks a suitable chain of algorithms yielding the user’s requirements, the ongoing work is a preliminary stage for the design of an adaptive selector. This approach is a crucial step towards a scientifically safe use of much-needed lossy data compression, because it disentangles the tasks of determining scientific characteristics of tolerable noise, from the task of determining an optimal compression strategy. Future algorithms can be used without changing application code.In this paper, we evaluate various lossy compression algorithms for compressing different scientific datasets (Isabel, ECHAM6), and focus on the analysis of synthetically created data that serves as blueprint for many observed datasets. We also briefly describe the available quantitiesof SCIL to define data precision and introduce two efficient compression algorithms for individualdata points. This shows that the best algorithm depends on user settings and data properties.
Adaptive Load Balancing Dashboard in Dynamic Distributed Systems;Seyedeh Leili Mirtaheri, Seyed Arman Fatemi, Lucio Grandinetti;https://doi.org/10.14529/jsfi170403;Load Balancing, Resource Management, Dynamic Variables, Communication Delay, Distributed System;Considering the dynamic nature of new generation scientific problems, load balancing is a necessity to manage the load in an efficient manner. Load balancing systems are use to optimize the resource consumption, maximize the throughput, minimize response time, and to prevent overload in resources. In current research, we consider operational distributed systems with dynamic variables caused by different nature of the applications and heterogeneity of the various levels in the system. Conducted studies indicate that many different factors should be considered to select the load balancing algorithm, including the processing power, load transfer and communication delay of nodes. In this work, We aim to design a dashboard that is capable to merge the load balancing algorithms in different environments. We design an adaptive system infrastructure with the ability to adjust various factors in the run time of a load balancing algorithm. We propose a task and a resource allocation mechanism and further introduce a mathematical model of load balancing process in the system. We calculate a normalized hardware score that determines the maturity of system according to the environmental conditions of the load balancing process. Evaluation results confirm that the proposed method performs well and reduces the probability of system failure.
Additivity: A Selection Criterion for Perfomance Events for Reliable Energy Predictive Modeling;Arsalan Shahid, Muhammad Fahad, Ravi Reddy, Alexey Lastovetsky;https://doi.org/10.14529/jsfi170404;Perfomance events, PMC, energy predictive models, Likwid, PAPI;Perfomance events or Perfomance monitoring counters (PMCs) are now the dominant predictor variables for modeling energy consumption. Modern hardware processors provide a large set of PMCs. Determination of the best subset of PMCs for energy predictive modeling is a non-trivial task given the fact that all the PMCs can not be determined using a single application run. Several techniques have been devised to address this challenge. While some techniques are based on a statistical methodology, some use expert advice to pick a subset (that may not necessarily be obtained in one application run) that, in experts' opinion, are significant contributors to energy consumption. However, the existing techniques have not considered a fundamental property of predictor variables that should have been applied in the first place to remove PMCs unfit for modeling energy. We address this oversight in this paper. We propose a novel selection criterion for PMCs called additivity, which can be used to determine the subset of PMCs that can potentially be used for reliable energy predictive modeling. It is based on the experimental observation that the energy consumption of a serial execution of two applications is the sum of energy consumptions observed for the individual execution of each application. A linear predictive energy model is consistent if and only if its predictor variables are additive in the sense that the vector of predictor variables for a serial execution of two applications is the sum of vectors for the individual execution of each application. The criterion, therefore, is based on a simple and intuitive rule that the value of a PMC for a serial execution of two applications is equal to the sum of its values obtained for the individual execution of each application. The PMC is branded as non-additive on a platform if there exists an application for which the calculated value differs significantly from the value observed for the application execution on the platform. The use of non-additive PMCs in a model renders it inconsistent. We study the additivity of PMCs offered by the popular state-of-the-art tools, Likwid and PAPI, by employing a detailed experimental methodology on a modern Intel Haswell multicore server CPU. We show that many PMCs in Likwid and PAPI that are widely used in models as key predictor variables are non-additive. This brings into question the reliability and the reported prediction accuracy of these models.
Cloud Service for Solution of Promising Problems of Nanotechnology;Marina A. Kornilina, Viktoriia O. Podryga, Sergey V. Polyakov, Dmitry V. Puzyrkov, Mikhail V. Yakoboskiy;https://doi.org/10.14529/jsfi170405;"cloud service, virtualization, supercomputer modeling in nanotechnology problems,
visualization";The paper presents the problem of creating a cloud service designed to solve promising nanotechnology problems on supercomputer systems. The motivation for creating such a service was the need to integrate ideas, knowledge and computing technologies related to this applied problem, as well as the need to involve specialists in solving problems of this class. The intermediate result of the work is a prototype of the cloud environment, implemented as a KIAM Multilogin service and an application software accessible from users virtual machines. The first applications of the service were the software packages GIMM_NANO and Flow_and_Particles, designed to solve the actual problems of nanoelectronics, laser nanotechnology, multiscale problems of applied gas dynamics. The implementation of the service took into account such aspects as support for parallel computations on the park of remote supercomputers, improving the efficiency of parallelization, very large data sets processing, visualization of supercomputer modeling results. With the help of the implemented service, it was possible to optimize the process of solving the applied problems associated with calculating the parameters of gas-dynamic flows in the microchannels of industrial spraying systems. In particular, it was possible to carry out the series of studies devoted to the analysis of gas-dynamic processes at the gas-metal boundary. In these studies it was shown that in the presence of microcapillaries in a technical system, it is necessary to use direct modeling of gas dynamic processes on the basis of the first principles in the Knudsen layers, for example, using molecular dynamics methods.
AlgoWiki Project as an Extension of the Top500 Methodology;Alexander S Antonov, Jack Dongarra, Vladimir Voevodin;https://doi.org/10.14529/jsfi180101;"AlgoWiki, parallel structure, algorithm’s properties, Top500 methodology, problems,
methods, algorithms, implementations, computing platforms";The AlgoWiki project is dedicated to describing the parallel structure and key features of various algorithms. The descriptions are intended to provide complete information about algorithm's properties, which are needed to adequately assess their implementation efficiency for any computing platform. This work sets out the key areas for further development of the project which were recently developed based on working with the AlgoWiki encyclopedia. We are suggesting an approach to extend the Top500 methodology, which is commonly used to compare various computing platforms.
Record-and-Replay Techniques for HPC Systems: A Survey;Dylan Chapp, Kento Sato, Dong H Ahn, Michela Taufer;https://doi.org/10.14529/jsfi180102;reproducibility, nondeterminism, fault-tolerance, exascale, message-passing, shared memory, proxy application, HPC benchmarks;Record-and-replay techniques provide the ability to record executions of nondeterministic applications and re-execute them identically. These techniques find use in the contexts of debugging, reproducibility, and fault-tolerance, especially in the presence of nondeterministic factors such as message races. Record-and-replay techniques are highly diverse in terms of the fidelity of replay they provide, the assumptions they make about the recorded application, the programming models they target, and the runtime overheads they impose.                                                                           In the HPC (HPC) environment, all the above factors must be considered in concert, thus presenting additional implementation challenges. In this manuscript, we survey record-and-replay techniques in terms of the programming models they target and the workloads on which they were evaluated, providing a categorization of these techniques benefiting application developers and researchers targeting exascale challenges. This manuscript answers three questions through this survey: What are the gaps in the existing space of record-and-replay techniques? What is the roadmap to widespread use of record-and-replay on production-scale HPC workloads? And, what are the critical open problems that must be addressed to make record-and-replay viable at exascale?                                             Keywords: Reproducibility, nondeterminism, fault-tolerance, exascale, message-passing, shared memory, proxy application, HPC benchmarks
Survey of Storage Systems for HPC;Jakob Lüttgau, Michael Kuhn, Kira Duwe, Yevhen Alforov, Eugen Betke, Julian Kunkel, Thomas Ludwig;https://doi.org/10.14529/jsfi180103;storage hierarchy, file system, storage system, I/O interface, data format;"In current supercomputers, storage is typically provided by parallel distributed file systems for hot data and tape archives for cold data. These file systems are often compatible with local file systems due to their use of the POSIX interface and semantics, which eases development and debugging because applications can easily run both on workstations and supercomputers. There is a wide variety of file systems to choose from, each tuned for different use cases and implementing different optimizations. However, the overall application Perfomance is often held back by I/O bottlenecks due to insufficient Perfomance of file systems or I/O libraries for highly parallel workloads. Perfomance problems are dealt with using novel storage hardware technologies as well as alternative I/O semantics and interfaces. These approaches have to be integrated into the storage stack seamlessly to make them convenient to use. Upcoming storage systems abandon the traditional POSIX interface and semantics in favor of alternative concepts such as object and key-value storage; moreover, they heavily rely on technologies such as NVM and burst buffers to improve Perfomance. Additional tiers of storage hardware will increase the importance of hierarchical storage management. Many of these changes will be disruptive and require application developers to rethink their approaches to data management and I/O. A thorough understanding of today's storage infrastructures, including their strengths and weaknesses, is crucially important for designing and implementing scalable storage systems suitable for demands of exascale computing."
The High-Q Club: Experience with Extreme-scaling Application Codes;Dirk Brömmel, Wolfgang Frings, Brian J. N. Wylie, Bernd Mohr, Paul Gibbon, Thomas Lippert;https://doi.org/10.14529/jsfi180104;JUQUEEN , IBM Blue Gene/Q, extreme scaling, application codes, High-Q Club;Jülich Supercomputing Centre (JSC) started running (extreme) scaling workshops with its first IBM Blue Gene supercomputer, finally spanning three generations each seeing an increase in the number of cores and available threads.  Over the years, this workshop series attracted numerous international code teams and resulted in many applications capable of running on all available cores of each system.This article reviews some of the knowledge gained with running and tuning highly-scalable applications, focussing on JUQUEEN, the IBM Blue Gene/Q at JSC. The ability to execute successfully on all 458752 cores with up to 1.8 million processes or threads may qualify codes for the High-Q Club, which serves as a showcase for diverse codes scaling to the entire 28 racks, effectively defining a collection of the highest scaling codes on JUQUEEN. The intention was to encourage other developers to invest in tuning and scaling their codes while identifying the necessary key aspects for that goal.As this era closes, it is timely to compare the characteristics of the 32 High-Q Club member codes, considering their strong and/or weak scaling, exploitation of hardware threading, and whether/how intra-node multi-threading is employed combined with message-passing.  We also identify the obstacles for scaling such as inefficient use of limited compute node memory and file I/O as key governing factors. Overall, the analysis provides guidance as to how applications may (need to) be designed in future to exploit expected exa-scale computer systems.
Exploiting the Perfomance Benefits of Storage Class Memory for HPC and HPDA Workflows;Michele Weiland, Adrian Jackson, Nick Johnson, Mark Parsons;https://doi.org/10.14529/jsfi180105;NVRAM, 3D XPoint, SCM, workflows, resource scheduling;"Byte-addressable storage class memory (SCM) is an upcoming technology that will transform the memory and storage hierarchy of HPC systems by dramatically reducing the latency gap between DRAM and persistent storage. In this paper, we discuss general SCM characteristics, including the different hardware configurations and data access mechanisms SCM is likely to provide. We outline the Perfomance challenges I/O requirements place on traditional scientific workflows and present how data access through SCM can have a beneficial impact on the Perfomance of such workflows, in particular those with large scale data dependencies. We describe the system software components that are required to enabled workflow and data aware resource allocation scheduling in order to optimise both system throughput and time to solution for individual applications; these include a data scheduler and data movers. We also present an illustration of the Perfomance improvement potential of the technology, based on initial workflow Perfomance benchmarks with I/O dependencies."
A General Guide to Applying Machine Learning to Computer Architecture;Daniel Nemirovsky, Tugberk Arkose, Nikola Markovic, Mario Nemirovsky, Osman Unsal, Adrian Cristal, Mateo Valero;https://doi.org/10.14529/jsfi180106;machine learning, computer architecture, data science, parameter engineering, Perfomance prediction, scheduling;The resurgence of machine learning since the late 1990s has been enabled by significant advancesin computing Perfomance and the growth of big data. The ability of these algorithms to detect complex patterns in data which are extremely difficult to achieve manually, helps to produce effective predictive models. Whilst computer architects have been accelerating the Perfomance of machine learning algorithms with GPUs and custom hardware, there have been few implementations leveraging these algorithms toimprove the computer system Perfomance. The work that has been conducted, however, has produced considerably promising results. The purpose of this paper is to serve as a foundational base and guide to future computer architecture research seeking to make use of machine learning models for improving system efficiency. We describe a method that highlights when, why, and how to utilize machine learning models for improving system Perfomance and provide a relevant example showcasing the effectiveness of applying machine learning in computer architecture. We describe a process of data generation every execution quantum and parameter engineering. This is followed by a survey of a set of popular machine learning models. We discuss their strengths and weaknesses and provide an evaluation of implementations for the purpose of creating a workload Perfomance predictor for different core types in an x86 processor. The predictions can then be exploited by a scheduler for heterogeneous processors to improve the system throughput. The algorithms of focus are stochastic gradient descent based linear regression, decision trees, random forests, artificial neural networks, and $k$-nearest neighbors. 
Deep Analysis of Job State Statistics on Lomonosov-2 Supercomputer;Dmitry A. Nikitenko, Vadim V. Voevodin, Sergey A. Zhumatiy;https://doi.org/10.14529/jsfi180201;HPC, supercomputer, parallel computing, efficiency analysis, job state;It is a common knowledge that the increasingly growing capabilities of HPC systems are always limited by a number of efficiency related issues. The reasons can be very different: hardware failures, incorrect job scheduling, peculiarities of algorithm, chosen programming technology specifics, etc. Most of these issues can be detected after precise analysis, but is a very resourceful way to study every application run. Therefore we performed less complicated analysis of the whole supercomputer job flow. In this paper we share our experience of analyzing user applications’ job states assigned by the SLURM resource manager that is used on the Lomonosov-2 system at Supercomputing center of Lomonosov Moscow State University. The statistics on job states was collected and it revealed that the ratio of correctly finished jobs (with the COMPLETED state) was rather low. The jobs owners were asked if the distribution of their jobs’ states is normal regarding their applications. This user feedback was processed, and some new ways of efficiency gain were revealed as the result.
Scalability Evaluation of Cimmino Algorithm for Solving Linear Inequality Systems on Multiprocessors with Distributed Memory;Leonid B. Sokolinsky, Irina M. Sokolinskaya;https://doi.org/10.14529/jsfi180202;system of linear inequalities, iterative algorithm, projection algorithm, Cimmino algorithm, parallel computation model, bulk synchronous farm, scalability estimation, speedup, parallel efficiency, cluster computing systems;The paper is devoted to a scalability study of Cimmino algorithm for linear inequality systems. This algorithm belongs to the class of iterative projection algorithms. For the analytical analysis of the scalability, the BSF (Bulk Synchronous Farm) parallel computation model is used. An implementation of the Cimmino algorithm in the form of operations on lists using higher-order functions Map and Reduce is presented. An analytical estimation of the scalability boundary of the algorithm for cluster computing systems is derived. An information about the implementation of Cimmino algorithm on lists in C++ language using the BSF program skeleton and MPI parallel programming library is given. The results of large-scale computational experiments performed on a cluster computing system are demonstrated. A conclusion about the adequacy of the analytical estimations by comparing them with the results of computational experiments is made.
On the Inversion of Multiple Matrices on GPU in Batched Mode;Nikolay M. Evstigneev, Oleg I. Ryabkov, Eugene A. Tsatsorin;https://doi.org/10.14529/jsfi180203;QR algorithm, LU Matrix Inversion, Batched Solver, Matrix solver, GPU Batched Solver;In this research we are considering the benchmarking of batched matrix inversion and solution of linear systems. The problem of multiple matrix inversion with the same fill sparsity is usually considered in problems of fluid mechanics with chemistry. In this case the system is stiff, and an implicit method is required to solve the problem. The core of such method is the multiple matrix inversion. We benchmark different methods based on cuSPARSE and MAGMA libraries and CPU LAPACK version depending on the matrix filling. We also provide our own experimental code that implements GaussJordan elimination on GPU using register shuffle. It is shown that the fastest method is the QR matrix inversion for single precision calculations. We also show that the suggested Gauss–Jordan elimination method looks promising being about 8–10 times faster than cuSPARSE QR method. We also demonstrate the application of batch solvers in the coupled reactive flow problem.
Parallel Numerical Algorithm for Solving Advection Equation for Coagulating Particles;Sergey Alexandrovich Matveev, Rishat R. Zagidullin, Alexander P. Smirnov, Eugene E. Tyrtyshnikov;https://doi.org/10.14529/jsfi180204;aggregation equations, parallel algorithms, low-rank matrices, convolution;In this work we present a parallel implementation of numerical algorithm solving the Cauchy problem for equation of advection of coagulating particles. This equation describes time-evolution of the concentration f(x, v, t) of particles of size v at the point x at the time-moment t. Our numerical algorithm is based on use of total variation diminishing (TVD) scheme and perfectly matching layers (PML) for approximation of advection operator along spatial coordinate x and utilization of the fast numerical method for evaluation of coagulation integrals exploiting low-rank decomposition of coagulation kernel coefficients and fast FFT-based implementation of convolution operation along particle size coordinate v. In our work we exploit one-dimensional domain decomposition approach along spatial coordinate x because it allows to avoid use of parallel FFT implementations which are very expensive in terms of data exchanges and have poor parallel scalability. Moreover, locality of finite-difference operator from TVD-scheme along x coordinate allows to obtain good scalability even for computing clusters with slow network interconnect due to modest volumes of data necessary for synchronization exchanges between times integration steps.
Generation of Multiple Turbulent Flow States for the Simulations with Ensemble Averaging;Boris I. Krasnopolsky;https://doi.org/10.14529/jsfi180205;"Incompressible turbulent flow, Generalized sparse matrix vector multiplication,
Multiple right-hand sides, Ensemble averaging";The paper deals with the problem of improving the Perfomance of high-fidelity incompressible turbulent flow simulations on HPC systems. The ensemble averaging approach, combining averaging in time together with averaging over multiple ensembles, allows to speedup the corresponding simulations by increasing the computing intensity of the numerical method (flops per byte ratio). The current paper focuses on further improvement of the proposed computational methodology, and particularly, on the optimization of procedure to generate multiple independent turbulent flow states.
HPC with Coarse Grained Model of Biological Macromolecules;Emilia Agnieszka Lubecka, Adam Kazimierz Sieradzan, Cezary Czaplewski, Paweł Krupa, Adam Liwo;https://doi.org/10.14529/jsfi180206;Unified Coarse Grained Model, UNRES force field, molecular dynamics simulations, fine-grained parallelization, coarse-grained parallelization;The Unified Coarse Grained Model of biological macromolecules (UCGM) that is being developed in our laboratory is a model designed to carry out large-scale simulations of biological macromolecules. The simplified chain representation used in the model allows to obtain 3-4 orders of magnitude extention of the time-scale of simulations, compared to that of all-atom simulations. Unlike most of the other coarse-grained force fields, UCGM is a physics-based force field, independent of structural databases and applicable to treat non-standard systems. In this communication, the efficiency and scalability of the new version of UCGM package with Fortran 90, with two parallelization levels: coarse-grained and fine-grained, is reported for systems with various size and oligomeric state. The Perfomance was tested in the canonical- and replica exchange MD mode, with small- and moderate-size proteins and protein complexes (20 to 1,636 amino-acid residues), as well as with large systems such as, e.g., human proteosome 20S with size over 6,200 aminoacid residues, which show the advantage of using coarse-graining. It is demonstrated that, with using massively parallel architectures, and owing to the physics-based nature of UCGM, real-time simulations of the behavior of subcellular systems are feasible.
3D Problems of Rotating Detonation Wave in a Ramjet Engine Modeled on a Supercomputer;Valeriy F. Nikitin, Yurii G. Filippov, Lyuben I. Stamov, Elena V. Mikhalchenko;https://doi.org/10.14529/jsfi180207;Mathematical Modeling, Detonation, Deflagration, RDE, Ramjet;A rotating detonation engine (RDE) combustion chamber was modeled in the work numerically using 3D geometry. The RDE is a new type of engines capable to create higher thrust than the traditional ones, which are based on the combustible mixture deflagration process. In the numerical experiment, different scenarios of the engine Perfomance were obtained. The calculations were made at a compact super-computer APK-5 with a peak Perfomance of 5.5 Tera Flops.
Numerical Simulations of Black Hole Accretion Flows;Agnieszka Janiuk, Konstantinos Sapountzis, Jeremy Mortier, Ireneusz Janiuk;https://doi.org/10.14529/jsfi180208;"astrophysical flows, black hole accretion, hydrodynamics, numerical simulations,
general relativistic MHD";We model the structure and evolution of black hole accretion disks using numerical simulations. The numerics is governed by the equations of general relativistic magneto-hydrodynamics (GRMHD). Accretion disks and outflows can be found at the base of very energetic ultra-relativistic jets produced by cosmic explosions, so called gamma-ray bursts (GRBs). Another type of phenomena are blazars, with jets emitted from the centers of galaxies.Long-lasting, detailed computations are essential to determine the physics of these explosions, and confront the theory with potential observables. From the point of view of numerical methods and techniques, three ingredients need to be considered. First, the numerical scheme must work in a conservative manner, which is achieved by solving a set of non-linear equations to advance the conserved quantities from one time step to the next. Second, the efficiency of computations depends on the code parallelization methods. Third, the analysis of results is possible via the post-processing of computed physical quantities, and visualization of the flow properties. This is done via implementing packages and libraries that are standardized in the field of computational astrophysics and supported by community developers.In this paper, we discuss the physics of the cosmic sources. We also describe our numerical framework and some technical issues, in the context of the GRMHD code which we develop. We also present a suite of Perfomance tests, done on the High-Perfomance Computer cluster (HPC) in the Center for Mathematical Modeling of the Warsaw University.
Continuum Computing - on a New Perfomance Trajectory beyond Exascale;Maciej Brodowicz, Thomas Sterling, Matthew Anderson;https://doi.org/10.14529/jsfi180301;HPC, parallel computing, exascale, non-von Neumann architecture;The end of Moore's Law is a cliche that none the less is a hard barrier to future scaling of HPC systems. A factor of about 4x in device density is all that is left of this form of improved throughput with a 5x gain required just to get to the milestone of exascale. The remaining sources of Perfomance improvement are better delivered efficiency of more than 10x and alternative architectures to make better use of chip real estate. This paper will discuss the set of principles guiding a potential future of non-von Neumann architectures as adopted by the experimental class of Continuum Computer Architecture (CCA). It is being explored by the Semantic Memory Architecture Research Team (SMART) at Indiana University. CCA comprises a homogeneous aggregation of cellular components (function cells) which are orders of magnitude smaller than lightweight cores and individually is unable to accomplish a computation but in combination can do so with extreme cost efficiency and unprecedented scalability. It will be seen that a path exists based on such unconventional methods like neuromorphic computing or dataflow that not only will meet the likely exascale milestone in the same time with much better power, cost, and size but also will set a new Perfomance trajectory leading to Zetaflops capability before 2030.
"Three-dimensional Inversion of Electromagnetic Geophysical Data with Parallel Computational Code on Supercomputer Complex ""Lomonosov""";Sergey V. Zaytsev, Viktor A. Kulikov, Andrei G. Yakovlev, Denis V. Yakovlev;https://doi.org/10.14529/jsfi180302;magnetotelluric, three-dimensional inversion, supercomputer, geophysics;"Usage of 2D inversion of magnetotelluric data for real geological objects can cause distortion, but it is more often used in commercial projects, because of its effectiveness and great experience. Whereas in the case of 3D inversion is not such a great experience and there are a number of global problems. When switching to 3D inversion of MT data, the requirement for computer technology is significantly increased. In this paper we will discuss a few examples of 3D inversion of electromagnetic geophysical field data with the usage of ""Lomonosov"" supercomputer and show its effectiveness on several geological objects. Each object is associated with a variety of problems: from search for shallow ore to regional hydrocarbon exploration. But all these objects contain a large volume of measurements obtaining qualitative results for which requires a huge amount of time. So that the use of 3D inversion with a high-Perfomance computational complex makes it possible to obtain a qualitative result of solving a wide range of problems."
Simulating the Long-timescale Structural Behavior of Bacterial and Influenza Neuraminidases with Different HPC Resources;Yana A. Sharapova, Dmitry A. Suplatov, Vytas K. Švedas;https://doi.org/10.14529/jsfi180303;neuraminidases, molecular dynamics, long-timescale trajectories, GPU, co-design;"Understanding the conformational dynamics which affects ligand binding by Neuraminidases is needed to improve the in silico selection of novel drug candidates targeting these pathogenicity factors and to adequately estimate the efficacy of potential drugs. Conventional molecular dynamics (MD) is a powerful tool to study conformational sampling, drug-target recognition and binding, but requires significant computational effort to reach timescales relevant for biology. In this work the advances in a computer power and specialized architectures were evaluated at simulating long MD trajectories of the structural behavior of Neuraminidases. We conclude that modern GPU accelerators enable calculations at the timescales that would previously have been intractable, providing routine access to microsecond-long trajectories in a daily laboratory practice. This opens an opportunity to move away from the ""static"" affinity-driven strategies in drug design towards a deeper understanding of ligand-specific conformational adaptation of target sites in protein structures, leading to a better selection of efficient drug candidates in silico. However, the Perfomance of modern GPUs is yet far behind the deeply-specialized supercomputers co-designed for MD. Further development of affordable specialized architectures is needed to move towards the much-desired millisecond timescale to simulate large proteins at a daily routine."
Parallel GPU-based Implementation of One-Way Wave Equation Migration;Alexander L. Pleshkevich, Vadim V. Lisitsa, Dmitry M. Vishnevsky, Vadim D. Levchenko, Boris M. Moroz;https://doi.org/10.14529/jsfi180304;GPU, nested OMP, MPI, seismic imaging;"We present an original algorithm for seismic imaging, based on the depth wavefield extrapolation by the  one-way wave equation. Parallel implementation of the algorithm is based on the several levels of parallelism. The input data parallelism allows processing full coverage for some area (up to one square km); thus, data are divided into several subsets and each subset is processed by a single MPI process. The mathematical approach allows dealing with each frequency independently and treating solution layer-by-layer; thus, a set of 2D cross-sections instead of the initial 3D common-offset vector gathers are processed simultaneously. This part of the algorithm is implemented suing GPU. Next, each common-offset vector image can be stacked, processed and stored independently. As a result, we designed and implemented the parallel algorithm based on the use of CPU-GPU architecture which allows computing common-offset vector images using one-way wave equation-based amplitude preserving migration. The algorithm was used to compute seismic images from real seismic land data."
High-Perfomance Computational Modeling of Chromosome Structure;Yuri A. Eidelman, Svetlana V. Slanina, Oleg A. Gusev, Sergey G. Andreev;https://doi.org/10.14529/jsfi180305;chromosome conformation capture, chromosome structure, computational modeling, mouse chromosome 18;We present a polymer modeling approach to generate the ensemble of 3D chromosome conformations at different time points of mitosis-interphase transition. Dynamics of structure during mitosis-G1 transition indicates quick and slow stages of chromosome shape alterations. At intermediate and late time scale the changes in chromosome compaction are small. To assess time dependence of contact map establishment during G1 we calculate contact maps at different times after mitotic decondensation. We demonstrate that the patterns of contacts observed soon after mitotic decondensation remain similar during G1. Whole contact map for mouse chromosome 18 at late G1 time correlates with the experimental chromosome conformation capture data. The simulations reproduce the main experimental findings, contact map persistence during G1 as well as specific pattern of long-range interactions in interphase chromosome. Our results suggest that spatial compartmentalization of an interphase chromosome is driven by interactions between different types of megabase sized chromatin domains during the formation of globular chromosome state at the end of mitotis to G1 transition.
Numerical Simulations of Structural Chromosomal Instability;Yuri A. Eidelman, Svetlana V. Slanina, Valentina S. Pyatenko, Sergey G. Andreev;https://doi.org/10.14529/jsfi180306;chromosomal instability, ionizing radiation, delayed chromosomal damage, prediction, dose response;The origin of dose-response curves for radiation-induced chromosomal instability (CI) is studied using the mechanistic CI model. The model takes into account DNA damage generation and repair in the progeny of irradiated cells and cell passage through mitotic cycle. We consider the formation of DNA double-strand breaks (DSBs) de novo in the S phase, where predominantly chromatid-type aberrations are formed. Among them sister chromatid exchanges of the “isochromatid deletion” type, or “chromatid dicentrics” are of primary interest. When the cell enters mitosis, the fate of chromosomal aberrations depends on their types. Chromosomal and chromatid fragments, having entered mitosis, either are transmitted into one of the daughter cells, or are lost. A chromatid dicentric in mitosis forms an anaphase bridge. These mechanistic assumptions were used to demonstrate that the dose-response curves are closely related to the dynamic curves for CI. The principles underlying this relationship are analyzed.
GPU-based Implementation of Discrete Element Method for Simulation of the Geological Fault Geometry and Position;Vadim V. Lisitsa, Vladimir A. Tcheverda, Victoria V. Volianskaia;https://doi.org/10.14529/jsfi180307;Discrete Element Method, geological faults, CUDA, statistical simulation;We present an algorithm for numerical simulation of the geological fault formation. The approach is based on the discrete elements method, which allows modeling of the deformations and structural discontinuity of the Upper part of the Earth crust. In the discrete elements method, the medium is represented as an combination of discrete particles which interact as elastic or viscoelastic bodies. Additionally, external potential forces, for example gravitational forces, may be introduced. At each time step the full set of forces acting at each particle is computed, after that the position of the particle is evaluated on the base of Newtonian mechanics. We implement the algorithm using CUDA technology to simulate single statistical realization of the model, whereas MPI is used to parallelize with respect to different statistical realizations. Obtained numerical results show that for low dip angles of the tectonic displacements relatively narrow faults form, whereas high dip angles of the tectonic displacements lead to a wide V-shaped deformation zones.
Modelling of Quantum Qubit Behaviour for Future Quantum Computers;Andrey N. Chibisov, Mary A. Chibisova;https://doi.org/10.14529/jsfi180308;quantum qubit, quantum computer, quantum-mechanical calculations, spin;This work deals with quantum qubit modelling based on a silicon material with embedded phosphorus atoms because a future quantum computer can be built on the basis of this qubit. The building of atomic models of bulk crystalline silicon and silicene, as well as calculation of their total energies, were performed using the Quantum ESPRESSO software package, using highPerfomance computing (HPC). For silicon and phosphorus atoms the generalized gradient approximation (GGA) was used in terms of the spin-orbit non-collinear interaction by means of the Quantum ESPRESSO package. The equilibrium orientations of the phosphorus qubit spins and localization of the wave functions in the 2D and bulk crystalline silicon phases were theoretically investigated by means of quantum-mechanical calculations. The existence of an exchange interaction between qubits has been confirmed, which leads to a change in the wave function’s localization and spin orientation, and in the case of silicene, this interaction was stronger.
Multiscale Simulations Approach: Crosslinked Polymer Matrices;Pavel V. Komarov, Daria V. Guseva, Vladimir Yu. Rudyak, Alexander V. Chertovich;https://doi.org/10.14529/jsfi180309;polymers, networks, atomistic molecular dynamics, mesoscale simulations, multiscale simulations;Atomistic molecular dynamics simulations can usually cover only a very limited range in space and time. Thus, the materials like polymer resin networks, the properties of which are formed on macroscopic scale, are hard to study thoroughly using only molecular dynamics. Our work presents a multiscale simulation methodology to overcome this shortcoming. To demonstrate its effectiveness, we conducted a study of thermal and mechanical properties of complex polymer matrices and establish a direct correspondence between simulations and experimental results. We believe this methodology can be successfully used for predictive simulations of a broad range of polymer matrices in glassy state.
Application of HPC for Comparison of Two Highly Branched Lysine Molecules of Different Topology;Igor M. Neelov, Oleg V. Shavykin, Maxim Y. Ilyash, Valeriy V. Bezrodnyi, Sofia E. Mikhtaniuk, Anna A. Marchenko, Emil I. Fatullaev, Anatolii A. Darinskii, Frans A. M. Leermakers;https://doi.org/10.14529/jsfi180310;HPC, dendrimer, dendritic brush, Poly-L-lysine;High Perfomance computations were performed for comparison of size and other properties of big heavily charged biocompatible molecules of complex topology in water. Lysine dendrimer and short dendritic brush of the same molecular weight were studied by molecular dynamics simulation method and GROMACS software package. The size and structural properties of these two systems were compared. It was shown that dendritic brush has smaller size and more dense core than the dendrimer. Radial density profile for both molecules is not monotonous and has minimum near core of molecules. This minimum is wider and deeper for dendrimer than for dendritic brush. Thus dendrimer has larger region of low density than dendritic brush and is more suitable for use for encapsulation and delivery of hydrophobic drugs.
Developing Efficient Implementations of Bellman–Ford and Forward-Backward Graph Algorithms for NEC SX-ACE;Ilya V. Afanasyev, Alexander S. Antonov, Dmitry A. Nikitenko, Vadim V. Voevodin, Vladimir V. Voevodin, Kazuhiko Komatsu, Osamu Watanabe, Akihiro Musa, Hiroaki Kobayashi;https://doi.org/10.14529/jsfi180311;graph algorithms, NEC SX-ACE, vector computing, data-intensive applications;The main goal of this work is to demonstrate that the development of data-intensive appli- cations for vector systems is not only important and interesting, but is also very possible. In this paper we describe possible implementations of two fundamental graph-processing algorithms for an NEC SX-ACE vector computer: the Bellman–Ford algorithm for single source shortest paths computation and the Forward-Backward algorithm for strongly connected components detection. The proposed implementations have been developed and optimised in accordance with features and properties of the target architecture, which allowed them to achieve Perfomance comparable to other traditional platforms, such as Intel Skylake, Intel Knight Landing or IBM Power processors.
Applications of HPC: Born–Oppenheimer Molecular Dynamics of Complex Formation in Aqueous Solutions;Maria G. Khrenova, Dmitry P. Kapusta, Ilya V. Babchuk, Yulia I. Meteleshko;https://doi.org/10.14529/jsfi180312;"Born-Oppenheimer molecular dynamic, free binding energy, parallel algorithms,
calixarene";The progress of supercomputer technologies initiated the development of methods of computational chemistry and their applications, particularly molecular dynamic simulations with ab initio potentials. These new methods allow to solve important problems of chemistry and technology. Particularly, solvent extraction and separation techniques are widely used to decrease the amount of radioactive wastes, especially radioactive caesium isotopes present in liquid phases. We demonstrated that the calculated binding constants between the alkali cation and calix[4]arene differ 103 times for Cs+ and Na+ ions, that is in good agreement with the experimental value. We report the results of benchmark calculations of our model system composed of 929 atoms described in the  density functional theory approximation with the GGA-type functional PBE with the empirical dispersion correction D3 and combined basis of Gaussian functions and plane waves DZVP with Goedecker-Teter-Hutter pseudopotentials. We demonstrate that efficiency of calculations decrease to about half if the amount of nodes is 16 on the Lomonosov-2 supercomputer.
Analysis of the Effect of Dispersion Forces on the Dielectric Film Properties Using Parallel Computing;Kirill A. Emelyanenko, Ludmila B. Boinovich, Alexandre M. Emelyanenko;https://doi.org/10.14529/jsfi180313;CFDM, thin film properties, dispersion forces, local polarizability;The paper presents the analysis of dispersion forces effect on local properties in thin free films. Using a Coupled Fluctuated Dipole Method with developed methods for numerical calculations of dielectric properties, the films with different lateral sizes and thicknesses were studied. In particular, the molecular polarizabilities at different distance from the film interface were analyzed. It was shown that dispersion interaction between the molecules, even for the case of nonpolar liquid with weak intermolecular interactions, causes a notable variation in dielectric properties of thin film, which is associated with the boundary layer formation. This variation, in turn, causes a strong dependence of polarizability accuracy on the cut-off radius. It is demonstrated that parallel computing algorithms can be effectively applied for obtaining the reliable data on properties of liquids in wetting films and boundary layers even under resource-imposed constraint on the size of ensemble of molecules to be handled in the numerical studies.
Quantum Chemistry Research of Interaction between 3D-Transition Metal Ions and a Defective Graphene on the Supercomputer Base;Nikolai V. Khokhriakov, Santiago Melchor;https://doi.org/10.14529/jsfi180314;graphene, transition metals, quantum chemistry, density functional;Quantum chemistry research is presented in the article, and it concerns the interaction within the complexes formed by the defective graphene clusters and ions of 3d-transition metals V,Cr,Mn, Fe,Co,Ni,Cu. The charges of all regarded ions were +1. All calculations were made at UDFT B3LYP/6-31G level of theory with the BSSE error taken into account. The strongest interaction with the defective clusters is observed in the case of Co+ ion. At the same time, this ion has demonstrated rather weak interaction with the defect-free graphene. Thus, the presence of Co+ in the reaction media increases probability of defect formation with the further forming of short nanotubes and curved carbon clusters with complex topology of their own.
A Flux Splitting Method for the SHTC Model for High-Perfomance Simulations of Two-phase Flows;Nadezhda S. Smirnova, Michael Dumbser, Mikhail N. Petrov, Alexander V. Chikitkin, Evgeniy I. Romenski;https://doi.org/10.14529/jsfi180315;"flux splitting, two-phase compressible flow, complete Riemann solver, finite-volume
method, hyperbolic equations, supercomputer computations";In this paper we propose a new flux splitting approach for the symmetric hyperbolic thermodynamically compatible (SHTC) equations of compressible two-phase flow which can be used in finite-volume methods. The approach is based on splitting the entire model into acoustic and pseudo-convective submodels. The associated acoustic system is numerically solved applying HLLC-type Riemann solver for its Lagrangian form. The convective part of the pseudo-convective submodel is solved by a standart upwind scheme. For other parts of the pseudo-convective submodel we apply the FORCE method. A comparison is carried out with unsplit methods. Numerical results are obtained on several test problems. Results show good agreement with exact solutions and reference calculations.
Magnetic Properties of LaAlO3/SrTiO3 Heterostructure Modelled on a Supercomputer;Irina Piyanzina, Volker Eyert, Thilo Kopp, Dmitrii Tayurskii;https://doi.org/10.14529/jsfi180316;DFT, LaAlO 3/SrTiO 3heterostructure, defects, magnetic properties;The oxide heterostructure composed of LaAlO3 (LAO) thin film on top of SrTiO3 (STO) substrate is the best known example of a system where a metallic state is formed in the STO layers next to the interface [1]. In the frame of present work we analyze an impact of oxygen vacancies and hydrogen dopants located in the AlO2 surface layer and in the TiO2 interfacial plane of LAO/STO heterostructure onto the magnetic properties by performing spin-polarized calculations based on density functional theory (DFT). We found stable local magnetic moments formed within atomically thin magnetic layers at the interface. We confirmed that agnetism can be generated by oxygen vacancies located either at the surface or at the interface. In addition, we demonstrate magnetic moments formation by hydrogen dopants located at the interface. Finally, the case of two defects combination was investigated, when negligibly small magnetic moment induction was found to take place.
Optimization of BWB Aircraft Using Parallel Computing;Kirill S. Anisimov, Andrey A. Savelyev, Innocentiy A. Kursakov, Alexander V. Lysenkov, Prajwal S. Prakasha;https://doi.org/10.14529/jsfi180317;optimization, CFD, Blended Wing Body, nacelle, power plant;Nacelle shape optimization for Blended Wing Body (BWB) is performed. The optimization procedure is based on numerical calculations of the Reynolds–averaged Navier–Stokes equations. For the Top Level Aircraft Requirements, formulated in AGILE project, the propulsion system was designed. The optimization procedure was divided in two steps. At first step, the isolated nacelle was designed and optimized for cruise regimes. This step is listed in paragraph 3. At second step the nacelles positions over airframe were optimized. To find the optimum solution, surrogate–based Efficient Global Optimization algorithm is used. An automatic structural computational mesh creation is realized for the effective optimization algorithm working. This whole procedure is considered in the context of the third generation multidisciplinary optimization techniques, developed within AGILE project. During the project, new techniques should be implemented for the novel aircraft configurations, chosen as test cases for application of AGILE technologies. It is shown that the optimization technology meets all requirements and is suitable for using in the AGILE project.
Supercomputer Simulations of Nondestructive Tomographic Imaging with Rotating Transducers;Sergey Y. Romanov;https://doi.org/10.14529/jsfi180318;supercomputer, ultrasound tomography, nondestructive testing, inverse problems;A method of nondestructive ultrasound tomographic imaging employing a rotating transducer system is proposed. The rotating transducer system increases the number of emitters and detectors in a tomographic scheme by several times and makes it possible to neutralize image artifacts resulting from incomplete-data tomography. The inverse problem of tomographic reconstructing the velocity structure inside the inspected object is considered as a nonlinear coefficient inverse problem for a scalar wave equation. Scalable iterative algorithms for reconstructing the longitudinal wave velocity inside the object are discussed. The methods are based on the explicit representation for the gradient of the residual functional. The algorithms employ parallelizing the computations over emitter positions. Numerical simulations performed on the “Lomonosov-2” supercomputer showed that the tomographic methods developed can not only detect boundaries of defects, but also determine the wave velocity distribution inside the defects with high accuracy provided that both reflected and transmitted waves are registered.
Reverse Mapping Algorithm for Multi-scale Numerical Simulation of Polylactic Acid;Mikhail K. Glagolev, Valentina V. Vasilevskaya;https://doi.org/10.14529/jsfi180319;molecular dynamics, multiscale simulation, reverse mapping, poly(lactic acid);An algorithm is proposed to convert the coarse-grained A-graft-B model of polylactic acid into the atomistic representation. In the A-graft-B model the atoms of the backbone are mapped onto A beads, which form the linear backbone of the coarse-grained macromolecule, the methyl groups are mapped onto B side pendants. The algorithm restores atomic positions based on positions of coarse-grained beads with the help of pre-defined chain fragments, called templates. The dimensions of the templates are adjusted by affine transformation to ensure coincidence of the backbone in coarse-grained and atomistic representation. The transition between coarse-grained and atomistic models conserves information about the fine structure of polymer chains. The restored configurations are suitable for further molecular-dynamic simulations. Both atomistic and coarse-grained representations require standard GROMACS software. The algorithm can be used for reverse mapping of other A-graft-B polymer models.
Supercomputer Technologies as a Tool for High-resolution Atmospheric Modelling towards the Climatological Timescales;Vladimir S. Platonov, Mikhail I. Varentsov;https://doi.org/10.14529/jsfi180320;"regional climate model, long-term simulations, supercomputer technologies, extreme
wind, urban climate, urban precipitation, COSMO";Estimation of the recent and future climate changes is the most important challenge in the modern Earth sciences. Numerical climate models are an essential tool in this field of research. However, modelling results are highly sensitive to the spatial resolution of the model. The most of the climate change studies utilize the global atmospheric models with a grid cell size of tens of kilometres or more. High-resolution mesoscale models are much more detailed, but require significantly more computational resources. Applications of such high-resolution models in climate studies are usually limited by regional simulations and by relatively short timespan. In this paper we consider the experience of the long-term regional climate studies based on the mesoscale modelling. On the examples of urban climate studies and extreme wind assessments, we demonstrate the principle advantage of long-term high-resolution simulations, which were carried out on the modern supercomputers.
Supercomputer Simulations in Design of Ultrasound Tomography Devices;Alexander V. Goncharsky, Sergey Y. Seryozhnikov;https://doi.org/10.14529/jsfi180321;ultrasound tomography, coefficient inverse problem, spatial resolution, supercomputer, GPU;The paper considers the use of supercomputers in design of medical ultrasound tomography devices. The mathematical models describing the wave propagation in ultrasound tomography should take into account such physical phenomena as diffraction, multiple scattering, and so on. The inverse problem of wave tomography is posed as a coefficient inverse problem with respect to the wave propagation velocity and the absorption factor. Numerous simulations made it possible to determine the optimal parameters of an ultrasound tomograph in order to obtain a spatial resolution of 1.5 mm suitable for early-stage breast cancer diagnosis. The developed methods were tested both on model problems and on real data obtained at the experimental test bench for tomographic studies. The computations were performed on GPU devices of Lomonosov-2 supercomputer at Lomonosov Moscow State University.
Recent Progress on Supercomputer Modelling of High-Speed Rarefied Gas Flows Using Kinetic Equations;Anna A. Frolova, Vladimir A. Titarev;https://doi.org/10.14529/jsfi180322;Boltzmann kinetic equation, S-model, rarefied, high-speed, unstructured;Numerical solution of the Boltzmann equation for stationary high-speed flows around complex three-dimensional bodies is an extremely difficult computational problem. This is because of high dimension of the equation and lack of efficient implicit methods for the calculation of the collision integral on arbitrary non-uniform velocity grids. Therefore, the use of the so-called model (approximate) kinetic equations appears to be more appropriate and attractive. This article uses the numerical methodology recently developed by the second author which includes an implicit method for solving the approximating kinetic equation of E.M. Shakhov (S-model) on arbitrary unstructured grids in both velocity and physical spaces. Since most of model equations have a well-known drawback associated with the velocityindependent collision frequency it is important to determine the deviations of solutions of these equations from the solution of the complete Boltzmann equation or DSMC for high-speed gas flows. Our recent comparison of the DSMC and S-model solutions for monatomic gases with a soft interaction potential shows good agreement of surface coefficients of the pressure, heat transfer and friction, which are most important for industrial applications. In this paper, we compare the solution of model equations and the Boltzmann equation for the problem of supersonic gas flow around a cylinder when molecules interact according to the law of hard spheres. Since this law of molecular interaction is the most rigid, the difference in solutions can show the maximum error that can be obtained by using model equations instead of the exact Boltzmann equation in such problems. Our high-fidelity computations show that the use of model kinetic equations with adaptation in phase space is very promising for industrial applications.
Supercomputer Modeling of Parachute Flight Dynamics;Alexey V. Setukha, Vladimir A. Aparinov, Andrey A. Aparinov;https://doi.org/10.14529/jsfi180323;"parallel algorithms, numerical simulation methods, fluid dynamics, vortex methods,
parachute aerodynamics";In this article the authors present parallel implementation of numerical method for computer modeling of dynamics of a parachute with filled canopy. To solve the 3D problem of parachute free motion numerically, authors formulate tied problem of dynamics and aerodynamics where aerodynamic characteristics are found with discrete vortices method on each step of integration in time, and to find motion law the corresponding motion equations have to be solved. The solution of such problems requires high computational resources because it is important to model parachute motion during a long physical time period. Herewith the behavior of vortex wake behind the parachute is important and has to be modeled. In the approach applied by the authors the wake is modeled as a set of flexible vortex elements. So to increase computational efficiency, the authors used methods of low-rank matrix approximations, as well as parallel implementations of algorithms. Short description of numerical method is presented, as well as the examples of numerical modeling.
Supercomputer Simulation of MATIS-H Problem;Mikhail A. Zaitsev, Vasilij M. Goloviznin, Sergej A. Karabasov;https://doi.org/10.14529/jsfi180324;Navier-Stokes equations, parallel computation, modelling, Cabaret method;A supercomputer simulation of the benchmark MATiS-H problem is considered. A highresolution CABARET code is applied for solving Navier-Stokes equations in the framework of the Monotonically Integrated LES approach for the MATiS-H problem. The code is based on a generalisation of low-dissipative, low-dispersive and non-oscillatory CABARET scheme to hybrid topology meshes in the supercomputing framework. The solutions for the time-averaged fields are reported. These show a relatively small sensitivity to the grid density. Comparison with the experiment data available is provided.
High-Perfomance Full-atomistic Simulation of Optical Thin Films;Fedor Grigoriev, Vladimir Sulimov, Alexander Tikhonravov;https://doi.org/10.14529/jsfi180325;thin film structure, deposition process, molecular dynamic simulation, silicon dioxide;The experimental study of the dependence of thin film properties on the deposition conditions may be still a great challenge. Today the progress in HPC allows one to perform the investigation of these dependencies on the atomistic level using the classical molecular dynamics (MD) simulation. In the present work the computational cost and efficiency of classical full-atomistic simulation of thin film deposition process using the Lonmonosov-2 supercomputer facilities is discussed. It is demonstrated that using 512 computational cores of the Lomonosov-2 supercomputer ensures the simulation of thin film cluster with technologically meaningful thickness of an optical film. Because of a relatively slow growth of the simulation time with the increase of film thickness we guess that simulations clusters with thicknesses that are several times higher than the currently achieved thicknesses about one hundred nanometers is quite realistic if the number of available computational cores will be increased up to several thousands.
Supercomputer Docking: Investigation of Low Energy Minima of Protein-Ligand Complexes;Danil C. Kutov, Alexey V. Sulimov, Vladimir B. Sulimov;https://doi.org/10.14529/jsfi180326;docking, protein-ligand, global minimum, force field, quantum-chemical method;It is shown that the global energy minimum of a protein-ligand complex, when the energy is calculated by the PM7 quantum-chemical semiempirical method with the COSMO implicit solvent model, can be determined as follows. First, the low energy minima are found by a docking program when the protein-ligand energy is calculated with the MMFF94 force field in vacuum. Second, energies of all these minima are recalculated with the PM7 method and the COSMO implicit solvent model. Third, among these recalculated energies the minimal energy is determined and the respective minimum is the global energy minimum when the energy is calculated with the PM7 method and the COSMO implicit solvent model. The optimal width of the spectrum of low energy minima found with MMFF94 in vacuum is determined to perform minimal quantity of quantum-chemical recalculations. The proposed approach allows to perform docking in solvent with the quantum-chemical method and to increase the docking positioning accuracy.
Toward Exascale Resilience: 2014 update;Franck Cappello, Al Geist, William Gropp, Sanjay Kale, Bill Kramer, Marc Snir;https://doi.org/10.14529/jsfi140101;Exascale, Resilience, Fault-tolerance techniques;Resilience is a major roadblock for HPC executions on future exascale systems. These systems will typically gather millions of CPU cores running up to a billion threads. Projections from current large systems and technology evolution predict errors will happen in exascale systems many times per day. These errors will propagate and generate various kinds of malfunctions, from simple process crashes to result corruptions.The past five years have seen extraordinary technical progress in many domains related to exascale resilience. Several technical options, initially considered inapplicable or unrealistic in the HPC context, have demonstrated surprising successes. Despite this progress, the exascale resilience problem is not solved, and the community is still facing the difficult challenge of ensuring that exascale applications complete and generate correct results while running on unstable systems. Since 2009, many workshops, studies, and reports have improved the definition of the resilience problem and provided refined recommendations. Some projections made during the previous decades and some priorities established from these projections need to be revised. This paper surveys what the community has learned in the past five years and summarizes the research problems still considered critical by the HPC community.
Runtime-Aware Architectures: A First Approach;Mateo Valero, Miquel Moreto, Marc Casas, Eduard Ayguade, Jesus Labarta;https://doi.org/10.14529/jsfi140102;Parallel architectures, runtime system, hardware-software co-design;In the last few years, the traditional ways to keep the increase of hardware Perfomance at the rate predicted by Moore's Law have vanished. When uni-cores were the norm, hardware design was decoupled from the software stack thanks to a well defined Instruction Set Architecture (ISA). This simple interface allowed developing applications without worrying too much about the underlying hardware, while hardware designers were able to aggressively exploit instruction-level parallelism (ILP) in superscalar processors. With the irruption of multi-cores and parallel applications, this simple interface started to leak. As a consequence, the role of decoupling again applications from the hardware was moved to the runtime system. Efficiently using the underlying hardware from this runtime without exposing its complexities to the application has been the target of very active and prolific research in the last years.Current multi-cores are designed as simple symmetric multiprocessors (SMP) on a chip. However, we believe that this is not enough to overcome all the problems that multi-cores already have to face. It is our position that the runtime has to drive the design of future multi-cores to overcome the restrictions in terms of power, memory, programmability and resilience that multi-cores have. In this paper, we introduce a first approach towards a Runtime-Aware Architecture (RAA), a massively parallel architecture designed from the runtime's perspective.
Towards a Perfomance portable, architecture agnostic implementation strategy for weather and climate models;Oliver Fuhrer, Carlos Osuna, Xavier Lapillonne, Tobias Gysi, Ben Cumming, Mauro Bianco, Andrea Arteaga, Thomas Christoph Schulthess;https://doi.org/10.14529/jsfi140103;"numerical weather prediction, climate modeling, hybrid computing, programming
models";We propose a software implementation strategy for complex weather and climate models that produces Perfomance portable, architecture agnostic codes. It relies on domain and data structure specific tools that are usable within common model development frameworks -- Fortran today and possibly high-level programming environments like Python in the future. We present the strategy in terms of a refactoring project of the atmospheric model COSMO, where we have rewritten the dynamical core and refactored the remaining Fortran code. The dynamical core is built on top of the domain specific ``Stencil Loop Language'' for stencil computations on structured grids, a generic framework for halo exchange and boundary conditions, as well as a generic communication library that handles data exchange on a distributed memory system. All these tools are implemented in C++ making extensive use of generic programming and template metaprogramming. The refactored code is shown to outperform the current production code and is Perfomance portable to various hybrid CPU-GPU node architectures.
Communication Complexity of the Fast Multipole Method and its Algebraic Variants;Rio Yokota, George Turkiyyah, David Keyes;https://doi.org/10.14529/jsfi140104;"communication complexity, hierarchical low-rank approximation, fast multipole
methods, H-matrices, sparse solvers";A combination of hierarchical tree-like data structures and data access patterns from fast multipole methods and hierarchical low-rank approximation of linear operators from H-matrix methods appears to form an algorithmic path forward for efficient implementation of many linear algebraic operations of scientific computing at the exascale. The combination provides asymptot- ically optimal computational and communication complexity and applicability to large classes of operators that commonly arise in scientific computing applications. A convergence of the mathe- matical theories of the fast multipole and H-matrix methods has been underway for over a decade. We recap this mathematical unification and describe implementation aspects of a hybrid of these two compelling hierarchical algorithms on hierarchical distributed-shared memory architectures, which are likely to be the first to reach the exascale. We present a new communication complexity estimate for fast multipole methods on such architectures. We also show how the data structures and access patterns of H-matrices for low-rank operators map onto those of fast multipole, leading to an algebraically generalized form of fast multipole that compromises none of its architecturally ideal properties.
Model-Driven One-Sided Factorizations on Multicore Accelerated Systems;Jack Dongarra, Azzam Haidar, Jakub Kurzak, Piotr Luszczek, Stanimire Tomov, Asim YarKhan;https://doi.org/10.14529/jsfi140105;hardware accelerators, dense linear algebra, task superscalar scheduling;Hardware heterogeneity of the HPC platforms is no longer considered unusual but instead have become the most viable way forward towards Exascale.  In fact, the multitude of the heterogeneous resources available to modern computers are designed for different workloads and their efficient use is closely aligned with the specialized role envisaged by their design.  Commonly in order to efficiently use such GPU resources, the workload in question must have a much greater degree of parallelism than workloads often associated with multicore processors (CPUs).  Available GPU variants differ in their internal architecture and, as a result, are capable of handling workloads of varying degrees of complexity and a range of computational patterns.  This vast array of applicable workloads will likely lead to an ever accelerated mixing of multicore-CPUs and GPUs in multi-user environments with the ultimate goal of offering adequate computing facilities for a wide range of scientific and technical workloads.  In the following paper, we present a research prototype that uses a lightweight runtime environment to manage the resource-specific workloads, and to control the dataflow and parallel execution in hybrid systems.  Our lightweight runtime environment uses task superscalar concepts to enable the developer to write serial code while providing parallel execution.  This concept is reminiscent of dataflow and systolic architectures in its conceptualization of a workload as a set of side-effect-free tasks that pass data items whenever the associated work assignment have been completed.  Additionally, our task abstractions and their parametrization enable uniformity in the algorithmic development across all the heterogeneous resources without sacrificing precious compute cycles.  We include Perfomance results for dense linear algebra functions which demonstrate the practicality and effectiveness of our approach that is aptly capable of full utilization of a wide range of accelerator hardware.
Exascale Storage Systems -- An Analytical Study of Expenses;Julian Martin Kunkel, Michael Kuhn, Thomas Ludwig;https://doi.org/10.14529/jsfi140106;Parallel I/O, Exascale, data center, storage expenses;"The computational power and storage capability of supercomputers are growing at a different pace, with storage lagging behind; the widening gap necessitates new approaches to keep the investment and running costs for storage systems at bay. In this paper, we aim to unify previous models and compare different approaches for solving these problems.By extrapolating the characteristics of the German Climate Computing Center's previous supercomputers to the future, cost factors are identified and quantified in order to foster adequate research and development.Using models to estimate the execution costs of two prototypical use cases, we are discussing the potential of three concepts: re-computation, data deduplication and data compression."
Adaptive Load Balancing in the Modified Mind Evolutionary Computation Algorithm;Maxim K. Sakharov, Anatoly P. Karpenko;https://doi.org/10.14529/jsfi180401;load balancing, landscape analysis, mind evolutionary computation, global optimization;The paper presents an adaptive load balancing method for the modified parallel Mind Evolutionary Computation (MEC) algorithm. The proposed method takes into account an objective function's topology utilizing the information obtained during the landscape analysis stage as well as the information on available computational resources. The modified MEC algorithm and proposed static load balancing method are designed for loosely coupled parallel computing systems and imply a minimal number of interactions between computational nodes when solving global optimization problems. A description of the proposed method is presented in this work along with the results of computational experiments, which were carried out with a use of multi-dimensional benchmark functions of various classes. Obtained results demonstrate that an effective use of available computational resources in the proposed method helps finding a better solution comparing to the traditional parallel MEC algorithm balancing. Further development of the proposed method requires more advanced termination criteria in order to avoid excessive iterations.
Multicore Platform Efficiency Across Remote Sensing Applications;Ekaterina O. Tyutlyaeva, Alexander A. Moskovsky, Igor O. Odintsov, Sergey S. Konyukhov, Alexey A. Poyda, Mikhail N. Zhizhin, Igor V. Polyakov;https://doi.org/10.14529/jsfi180402;energy consumption, Perfomance analysis and optimization, cross-platform analysis, energy consumption analysis, multispectral image processing, nighttime remote sensing;"A wide range of modern system architectures and platforms targeted for different algorithms and application areas is now available.Even general-purpose systems have advantages in some computation areas and bottlenecks in another. Scientific applications on specific areas, on the other hand, have different requirements for CPU Perfomance, scalability and power consumption.The best practice now is algorithm/architecture co-exploration approach, where scientific problem requirements influence the hardware configuration; on the other hand, algorithm implementation is re factored and optimized in accordance with the platform architectural features.In this research, two typical modules used for multispectral nighttime satellite image processing are studied:• measurement of local perceived sharpness in visible band using the Fourier transform;• cross-correlation in a moving window between visible and infrared bands.Both modules are optimized and studied on wide range of up-to-date testbeds, based on different architectures. Our testbeds include computational nodes based on Intel Xeon E5-2697A v4, Intel Xeon Phi, Texas Instruments Sitara AM5728 dual-core ARM Cortex-A15, and NVIDIA JETSON TX2.The study includes Perfomance testing and energy consumption measurements. The results achieved can be used for assessing serviceability for multispectral nighttime satellite image processing by two key parameters: execution time and energy consumption."
A Study on Cross-Architectural Modelling of Power Consumption Using Neural Networks;Vadim V. Elisseev, Milos Puzovic, Eun Kyung Lee;https://doi.org/10.14529/jsfi180403;hpc, power consumption, modelling, exascale, cross-architectural, neural networks;On the path to Exascale, the goal of HPC (HPC) to achieve maximum Perfomance becomes the goal of achieving maximum Perfomance under strict power constraint. Novel approaches to hardware and software co-design of modern HPC systems have to be developed to address such challenges. In this paper, we study prediction of power consumption of HPC systems using metrics obtained from hardware Perfomance counters. We argue that this methodology is portable across different micro architecture implementations and compare results obtained on Intel 64, IBMR and Cavium ThunderXR ARMv8 microarchitectures.We discuss optimal number and type of hardware Perfomance counters required to accurately predict power consumption.We compare accuracy of power predictions provided by models based on Linear Regression (LR) and Neural Networks (NN). We find that the NN-based model provides better accuracy of predictions than the LR model. We also find, that presently it is not yet possible to predict power consumption on a given microarchitecture using data obtained on a different microarchitecture. Results of our work can be used as a starting point for developing unified, cross-architectural models for predicting power consumption.
Autotuning Techniques for Perfomance-Portable Point Set Registration in 3D;Piotr Luszczek, Jakub Kurzak, Ichitaro Yamazaki, David Keffer, Vasileios Maroulas, Jack Dongarra;https://doi.org/10.14529/jsfi180404;"portable Perfomance engineering, point set registration, autotuning with code generation,
combinatorial optimization";We present an autotuning approach applied to exhaustive Perfomance engineering of the EM-ICP algorithm for the point set registration problem with a known reference. We were able to achieve progressively higher Perfomance levels through a variety of code transformations and an automated procedure of generating a large number of implementation variants. Furthermore, we managed to exploit code patterns that are not common when only attempting manual optimization but which yielded in our tests better Perfomance for the chosen registration algorithm. Finally, we also show how we maintained high levels of the Perfomance rate in a portable fashion across a wide range of hardware platforms including multicore, manycore coprocessors, and accelerators. Each of these hardware classes is much different from the others and, consequently, cannot reliably be mastered by a single developer in a short time required to deliver a close-to-optimal implementation. We assert in our concluding remarks that our methodology as well as the presented tools provide a valid automation system for software optimization tasks on modern HPC hardware.
Benchmarking Quantum Chemistry Methods in Calculations of Electronic Excitations;Bella L. Grigorenko, Vladimir A. Mironov, Igor V. Polyakov, Alexander V. Nemukhin;https://doi.org/10.14529/jsfi180405;quantum chemistry, multi-scale approaches, parallel algorithms, fluorescent proteins;Quantum chemistry methods are applied to obtain numerical solutions of the Schr¨odinger equation for molecular systems. Calculations of transitions between electronic states of large molecules present one of the greatest challenges in this field which require the use of supercomputer resources. In this work we describe the results of benchmark calculations of electronic excitation in the protein domains which were designed to engineer novel fluorescent markers operating in the near-infrared region. We demonstrate that such complex systems can be efficiently modeled with the hybrid qunatum mechanics/molecular mechanics approach (QM/MM) using the modern supercomputers. More specifically, the time-dependent density functional theory (TD-DFT) method was primarily tested with respect to its Perfomance and accuracy. GAMESS (US) and NWChem software were benchmarked in direct and storage-based TDDFT calculations with the hybrid B3LYP density functional, both showing good scaling up to 32 nodes. We note that conventional SCF calculations greatly outperform direct SCF calculations for our test system. Accuracy of TD-DFT excitation energies was estimated by a comparison to the more accurate ab initio XMCQDPT2 method.
Developing Quasi-Steady Model for Studying Hemostatic Response Using Supercomputer Technologies;Petr V. Trifanov, Valeria N. Kaneva, Sergei V. Strijhak, Mikhail A. Panteleev, Fazoil I. Ataullakhanov, Joanne Dunster, Vadim V. Voevodin, Dmitry Yu. Nechipurenko;https://doi.org/10.14529/jsfi180406;cfd, particle-based model, biorheology, thrombosis, program profiling;Formation of the platelet plug represents a primary response to the vessel wall injury, but may also result in vessel occlusion. The decrease of the local blood flow due to platelet thrombus formation may lead to serious complications, such as ischemic stroke and myocardial infarction. However, mechanisms responsible for regulation of thrombus dynamics are not clear. In order to get a deeper insight into the role of blood flow and platelet interactions in the formation of the primary platelet plug we developed a particle-based model of microvascular thrombosis using quasisteady flow approximation. In order to simulate thrombus dynamics at physiologically relevant timescales of several minutes, we took advantage of the supercomputer technologies. Our in silico analysis revealed the importance of platelet size heterogeneity for describing experimental data on microvascular thrombus formation. Thus, our model represents a useful tool for the supercomputeraided computational analysis of thrombus dynamics in the microvessels on physiologically relevant timescales.
New Binding Mode of SLURP Protein to a7 Nicotinic Acetylcholine Receptor Revealed by Computer Simulations;Igor D. Diankin, Denis S. Kudryavtsev, Arthur O. Zalevsky, Victor I. Tsetlin, Andrey V. Golovin;https://doi.org/10.14529/jsfi180407;"molecular dynamics, gromacs, clustering, affinity propagation, protein docking,
biomolecules"; SLURP-1 is a member of three-finger toxin-like proteins. Their characteristic feature is a set of three beta strands extruding from hydrophobic core stabilized by disulfide bonds. Each beta-strand carries a flexible loop, which is responsible for recognition. SLURP-1 was recently shown to act as an endogenous growth regulator of keratinocytes and tumor suppressor by reducing cell migration and invasion by antagonizing the pro-malignant effects of nicotine. This effect is achieved through allosteric interaction with alpha7 nicotinic acetylcholine receptors (alpha-7 nAChRs) in an antagonist-like manner. Moreover, this interaction is unaffected by several well-known agents specifically alpha-bungarotoxin.In this work, we carry out the conformational analysis of the SLURP-1 by a microsecond-long full-atom explicit solvent molecular dynamics simulations followed by clustering, to identify representative states. To achieve this timescale we employed a GPU-accelerated version of GROMACS modeling package. To avoid human bias in clustering we used a non-parametric clustering algorithm Affinity Propagation adapted for biomolecules and HPC environments. Then, we applied protein-protein molecular docking of the ten most massive clusters to alpha7-nAChRs in order to test if structural variability can affect binding. Docking simulations revealed the unusual binding mode of one of the minor SLURP-1 conformations.  
Supercomputer Simulations of Fluid-Structure Interaction Problems Using an Immersed Boundary Method;Natalya S. Zhdanova, Andrey V. Gorobets, Ilya V. Abalakin;https://doi.org/10.14529/jsfi180408;"Parallel CFD, immersed boundary method, unstructured mesh, turbulent flow,
MPI+OpenMP";The paper describes a supercomputer application in simulations of fluid-structure interaction problems. A compressible flow solver based on a high-accuracy scheme for unstructured hybrid meshes is considered. It combines an immersed boundary method with a dynamic mesh adaptation method in order to represent motion of solid objects in a turbulent flow. The use of immersed boundaries allows you to dynamically adapt the mesh resolution near moving solid surfaces without changing the mesh topology. Multilevel MPI + OpenMP parallelization of these components fits well with the architecture of modern cluster systems. The proposed implementation can engage thousands of CPU cores in one simulation efficiently. An example application is presented in which a high-speed turbulent flow around a cavity with a deflector is simulated.
Test of Computational Approaches for Gold-Thiolate Clusters Calculation;Nadezhda N. Nikitina, Daria A. Pichugina, Alexander V. Oleynichenko, Oxana N. Ryzhova, Kirill E. Kopylov, Vladimir V. Krotov, Nikolay E. Kuz’menko;https://doi.org/10.14529/jsfi180409;"density functional theory, parallel calculation, cluster, gold, dispersion correction,
relativistic effects, computational chemistry";High-level procedures (MP2, CCSD, CCSD(T)) and reliable experimental data have been used to assess the Perfomance of a variety of exchange-correlation functionals for the calculation of structures and energies of small models of thiolate-protected gold clusters. Clusters represent rather complicated objects for examination, therefore the simple models including Au2, AuS were considered to find an appropriate method to calculate Au-Au and Au-S interactions in protected clusters. The mean unsigned errors of the quantum chemical methods were evaluated via reliable experimental bond distances and dissociation energies of Au2 and AuS. Based on the calculation, the SVWN5, TPSS+D3, PBE96+D3, and PBE0+D3 were found to give the most reliable results and can be recommended for calculation of the structure and properties of thiolate-protected gold clusters. The influence of the relativistic corrections calculated in Dirac-Coulomb-Breit framework and inclusion of dispersion corrections on the structure and energy of thiolate-protected gold clusters have been analyzed.
Supercomputer Modeling of Dual-Site Acetylcholinesterase (AChE) Inhibition;Sofya V. Lushchekina, Galina F. Makhaeva, Dana A. Novichkova, Irina V. Zueva, Nadezhda V. Kovaleva, Rudy R. Richardson;https://doi.org/10.14529/jsfi180410;acetylcholinesterase, Alzheimer’s disease, molecular docking, atomic charges;Molecular docking is one of the most popular tools of molecular modeling. However, in certain cases, like development of inhibitors of cholinesterases as therapeutic agents for Alzheimer's disease, there are many aspects, which should be taken into account to achieve accurate docking results. For simple molecular docking with popular software and standard protocols, a personal computer is sucient, however quite often the results are irrelevant. Due to the complex biochemistry and biophysics of cholinesterases, computational research should be supported with quantum mechanics (QM) and molecular dynamics (MD) calculations, what requires the use of supercomputers. Experimental studies of inhibition kinetics can discriminate between dierent types of inhibition—competitive, non-competitive or mixed type—that is quite helpful for assessment of the docking results. Here we consider inhibition of human acetylcholinesterase (AChE) by the conjugate of MB and 2,8-dimethyl-tetrahydro-y-carboline, study its interactions with AChE in relation to the experimental data, and use it as an example to elucidate crucial points for reliable docking studies of bulky AChE inhibitors. Molecular docking results were found to be extremely sensitive to the choice of the X-ray AChE structure for the docking target and the scheme selected for the distribution of partial atomic charges. It was demonstrated that exible docking should be used with an additional caution, because certain protein conformational changes might not correspond with available X-ray and MD data.
Supercomputer Simulations of Dopamine-Derived Ligands Complexed with Cyclooxygenases;Valentina D. Maslova, Roman V. Reshetnikov, Vladimir V. Bezugolov, Igor I. Lyubimov, Andrey V. Golovin;https://doi.org/10.14529/jsfi180411;"molecular docking, non-steroidal anti-inflammatory drugs, ibuprofen, dopamine,
cyclooxygenase";An in silico approach was adopted to identify potential cyclooxygenase inhibitors through molecular docking studies. Four potentially active molecules were generated by fusion of dopamine with ibuprofen or ketorolac derivatives. The binding mode of the considered ligands to cyclooxygenase-1 and cyclooxygenase-2 isoforms was described using Autodock Vina. Preliminary docking to full cyclooxygenase isoforms’ structures was used to determine possible binding sites for the described dopamine-derived ligands. The following more accurate docking iteration to the described binding sites was used to achieve better conformational sampling. Among the studied molecules, IBU-GABA-DA showed preferable binding to cyclooxygenase active site of cyclooxygenase-1, while IBU-DA bound to peroxidase site of cyclooxygenase-1, making these ibuprofen-comprising ligands a base for further research and design of selective cyclooxygenase-1 inhibitors. Keterolac-derived ligands KET-DA and KET-GABA-DA demonstrated binding to both cyclooxygenase isoforms at a side pocket, which does not relate to any known functional site of cyclooxygenases and needs to be further investigated.
HPC of Magnetized Galactic Disks;Sergey A. Khoperskov, Yulia A. Venichenko, Sergey S. Khrapov, Eugene O. Vasiliev;https://doi.org/10.14529/jsfi180412;magnetohydrodynamics, galactic dynamics, galactic magnetic field, parallelization;A parallel implementation of the magneto-hydrodynamical code for global modeling of the galactic evolution is reported. The code is parallelized by using MPI interface, and it shows ideal scaling up to 200–300 cores on Lomonosov supercomputer with fast interconnect. In the benchmarking of this code, we study the dynamics of a magnetized gaseous disk of a galaxy with a bar. We run a high-resolution 3D magnetohydrodynamic simulation taking into account the Milky Way-like gravitational potential, gas self-gravity and a network of cooling and heating processes in the interstellar medium. By using this simulation the evolution of morphology and enhancement of the magnetic field are explored. In agreement to hydrodynamical models, when the bar is strong enough, the gas develops sharp shocks at the leading side of the bar. In such a picture we found that when typically the magnetic field strength traces the location of the largescale shocks along the bar major axis, the magnetic field pressure weakens the shocks and reduces the inflow of gas towards the galactic center.
Regional Climate Model for the Lower Volga: Parallelization Efficiency Estimation;Alexander V. Titov, Alexander V. Khoperskov;https://doi.org/10.14529/jsfi180413;regional climate model, domain size, simulations, parallelization;We have deployed the regional climate model (RCM) RegCM 4.5 for the Lower Volga and adjacent territories with a horizontal spatial resolution of 20 km. The problems of choosing the computational domain in the RCM RegCM version 4.5 are considered. We demonstrate the influence of this factor on the forecast of rainfall distribution in the numerical simulations. The study of rainfall and snowfall is a more demanding test in comparison with temperature or pressure distributions. We investigate dependencies of calculation time, parallel speedup and parallelization efficiency on the number of processes for different multi-core CPUs. Our analysis of the efficiency of parallel implementation of RegCM for various multi-core and multi-processor systems show a strong dependence of the simulation speed on the CPU type. The best effect is achieved when the number of CPU threads and the number of parallel processes are equal. The parallel code speedup is in the range of 1.8 – 11 for different CPUs.
Perfomance Analysis of Different Computational Architectures: Molecular Dynamics in Application to Protein Assemblies, Illustrated by Microtubule and Electron Transfer Proteins;Vladimir A. Fedorov, Ekaterina G. Kholina, Ilya B. Kovalenko, Nikita B. Gudimchuk;https://doi.org/10.14529/jsfi180414;molecular dynamics, tubulin, microtubule, plastocyanin-cytochrome f;"All-atom molecular dynamics simulation represents a computationally challenging, but powerful approach for studying conformational changes and interactions of biomolecules and their assemblies of different kinds. Usually, the numbers of simulated particles in modern molecular dynamics studies range from thousands to tens of millions, while the simulated timescales span from nanoseconds to microseconds.  For cost and computation efficiency, it is important to determine the optimal computer hardware for simulations of biomolecular systems of different size and timescale. Here we compare Perfomance and scalability of 17 commercially available computational architectures, using molecular dynamics simulations of water and two different protein systems in GROMACS-5 package as computing benchmarks. We report typical single-node Perfomance of various combinations of modern CPUs and GPUs, as well as multiple-node Perfomance of ""Lomonosov-2"" supercomputer in molecular dynamics simulations of different protein systems in nanoseconds per day. These data can be used as practical guidelines for selection of optimal computer hardware for various molecular dynamics simulation tasks. "
Algorithm of the Parallel Sweep Method for Numerical Solution of the Gross–Pitaevskii Equation with Highest Nonlinearities;Andrey D. Bulygin;https://doi.org/10.14529/jsfi180415;nonlinear Schrodinger equation, fast parallel algorithm, fully conservative numerical scheme, motion integral;In this paper, we for the first time introduce a numerical scheme the solution of a nonlinear equation of the Gross–Pitaevskii type (GP) or the nonlinear Schrodinger equation (NLSE) with highest nonlinearities, which provides implementation of a complete set of motion integrals. This scheme was parallelly implemented on a non-uniform grid. Propagation of a ring laser beam with non-zero angular momentum in the filamentation mode is studied using the implemented numerical scheme. It is shown, that filaments under exposure to centrifugal forces escape to the periphery. Based on a number of numerical experiments, we have found the universal property of motion integrals in the non-conservative case for a given class of equations. Research of dynamics of angular momentum for a dissipative case are also presented. We found, that angular moment, particularly normed by initial energy during filamentation process, is quasi-constant.
Efficient Parallel Implementation of Multi-Arrival 3D Prestack Seismic Depth Migration;Alexander L. Pleshkevich, Anton V. Ivanov, Vadim D. Levchenko, Sergey A. Khilkov, Boris P. Moroz;https://doi.org/10.14529/jsfi190101;seismic imaging, multi-arrival seismic migration, HPC, aiwlib;"The goal of seismic migration is to reconstruct the image of Earth's depth inhomogeneities on the base of seismic data. Seismic data is obtained using shots in shallow wells that are located in a dense grid points. Those shots could be considered as special point sources. A reflected and scattered seismic waves from the depth inhomogeneities are received by geophones located also in a dense grid points on a surface. A seismic image of depth inhomogeneities can be constructed based on these waves. The implementation of 3-D seismic migration implies the solution of about 104÷5 3-D direct problems of wave propagation. Hence efficient asymptotic methods are of a great practical importance. The multi-arrival 3-D seismic migration program is implemented based on a new asymptotic method. It takes into account multi-pass wave propagation and caustics. The program uses parallel calculations in an MPI environment on hundreds and thousands of processor cores. The program was successfully tested on an international synthetic ""SEG salt"" data set and on real data. A seismic image cube for Timan-Pechora region is given as an example."
LAMMPS Code Simulation of the Defect Formation Induced by Ion Incidence in Carbon Nanotubes;Andrey A. Shemukhin, Anton V. Nazarov, Anton V. Stepanov;https://doi.org/10.14529/jsfi190102;ion irradiation, multiwall carbon nanotube, defects, molecular dynamics, sputtering, thermal mechanism;A molecular dynamic calculation of the multi-walled carbon nanotube thermal sputtering induced by ion irradiation is carried out. Sputtering results comparable to experimental data are obtained. There are two models of ion and thermal sputtering discussed in the paper. The simulation tested the model of thermal amorphization and revealed that the disordering of multi-walled carbon nanotubes structure occurs as a result of their heating under ion irradiation. Classical molecular dynamic simulation was performed using LAMMPS code. Simulation cell with 14 layers multi-walled carbon nanotube 12×12×30 nm size contains 285600 atoms. Multi-walled carbon nanotube was irradiated by 80 keV energy Ar+ ions in cumulative mode. Simulation was performed on the Lomonosov-1 supercomputer. About 24600 nodes-hours were spent on one simulation as a whole. The balancing of MPI ows for a spatial grid of counting nodes occurred according to the scheme 8×8×128 MPI-stream. LAMMPS code was built with Intel 12.0 compiler. This configuration allowed to speed up the calculation in comparison with the calculation on a single-processor Xeon CPU X5570 2.93 GHz machine by 60 times.
A Fully Conservative Parallel Numerical Algorithm with Adaptive Spatial Grid for Solving Nonlinear Diffusion Equations in Image Processing;Andrey D. Bulygin, Denis A. Vrazhnov;https://doi.org/10.14529/jsfi190103;"Perona{Malik method, nonlinear Schr odinger equation, fast parallel algorithm,
fully conservative numerical scheme";In this paper we present simple yet efficient parallel program implementation of grid-difference method for solving nonlinear parabolic equations, which satisfies both fully conservative property and second order of approximation on non-uniform spatial grid according to geometrical sanity of a task. The proposed algorithm was tested on Perona–Malik method for image noise ltering task based on differential equations. Also in this work we propose generalization of the Perona–Malik equation, which is a one of diffusion in complex-valued region type. This corresponds to the conversion to such types of nonlinear equations like Leontovich–Fock equation with a dependent on the gradient field according to the nonlinear law coefficient of diffraction. This is a special case of generalization of the Perona–Malik equation to the multicomponent case. This approach makes noise removal process more flexible by increasing its capabilities, which allows achieving better results for the task of image denoising.
Parametrization of the Elastic Network Model Using High-Throughput Parallel Molecular Dynamics Simulations;Philipp S. Orekhov, Ilya V. Kirillov, Vladimir A. Fedorov, Ilya B. Kovalenko, Nikita B. Gudimchuk, Artem A. Zhmurov;https://doi.org/10.14529/jsfi190104;molecular dynamics, coarse-grained models, tubulin, elastic network model, highthroughput simulations, parallel simulations;Even when modern computational platforms and parallel techniques are used, conventional all-atom simulations are limited both in terms of reachable timescale and number of atoms in the biomolecular system of interest. On the other hand, coarse-grained models, which allow to overcome this limitation, rely on proper and rigorous parametrization of the underlying force field. Here, we present a novel iterative approach for parametrization of coarse-grained models based on direct comparison of equilibrium simulations at all-atom and coarse-grained resolutions. In order to assess the accuracy of our method, we have built and parametrized an elastic network model (ENM) of the tubulin protolament consisting of four monomers. For this system, our method shows good convergence and the parametrized ENM reproduces protein dynamics in a finer way when compared to ENMs parametrized using the conventional approach. The presented method can be extended to other coarse-grained models with a slight adjustment of the equations describing the iterative scheme.
Facilitating HPC Operation and Administration via Cloud;Chaoqun Sha, Jingfeng Zhang, Lei An, Yongsheng Zhang, Zhipeng Wang, Tomi Ilijas, Nejc Bat, Miha Verlic, Qing Ji;https://doi.org/10.14529/jsfi190105;"HPC, supercomputer, monitoring, notifications, cloud, operation, administration,
EasyOP";Experiencing a tremendous growth, Cloud Computing offers a number of advantages over other distributed platforms. Introducing the advantages of HPC (HPC) also brought forward the development of HPCaaS (HPC as a Service), which has mainly focused on flexible access to resources, cost-effectiveness, and the no-maintenance-needed for end-users. Besides providing and using HPCaaS, HPC centers could leverage more from Cloud Computing technology, for instance to facilitate operation and administration of deployed HPC systems, commonly faced by most supercomputer centers.This paper reports the product, EasyOP, developed to realize the idea that one or more Cloud or HPC facilities can be run over a centralized and unified control platform. The main purpose of EasyOP is that the information of HPC systems hardware and system software, failure alarms, jobs scheduling, etc. is sent to the Wuxi cloud computing center. After a series of analysis and processing, we are able to share many valuable data, including alarm and job scheduling status, to HPC users through SMS, email, and WeChat. More importantly, with the data accumulated on the cloud computing center, EasyOP can offer several easy-to-use functions, such as user(s) management, monthly/yearly reports, one-screen monitoring and so on. By the end of 2016, EasyOP successfully served more than 50 HPC systems with almost 10000 nodes and over of 300 regular users.
Perfomance Evaluation of Different Implementation Schemes of an Iterative Flow Solver on Modern Vector Machines;Kenta Yamaguchi, Takashi Soga, Yoichi Shimomura, Thorsten Reimann, Kazuhiko Komatsu, Ryusuke Egawa, Akihiro Musa, Hiroyuki Takizawa, Hiroaki Kobayashi;https://doi.org/10.14529/jsfi190106;"Perfomance evaluation, legacy code, numerical uid dynamics simulation, vectorization, hyperplane method, red-black method";Modern supercomputers consist of multi-core processors, and these processors have recently employed vector instructions, or so-called SIMD instructions, to improve Perfomances. Numerical simulations need to be vectorized in order to achieve higher Perfomance on these processors. Various legacy numerical simulation codes that have been utilized for a long time often contain two versions of source codes: a non-vectorized version and a vectorized version that is optimized for old vector supercomputers. It is important to clarify which version is better for modern supercomputers in order to achieve higher Perfomance. In this paper, we evaluate the Perfomances of a legacy fluid dynamics simulation code called FASTEST on modern supercomputers in order to provide a guidepost for migrating such codes to modern supercomputers. The solver has a nonvectorized version and a vectorized version, and the latter uses the hyperplane ordering method for vectorization. For the evaluation, we also implement the red-black ordering method, which is another way to vectorize the solver. Then, we examine the Perfomance on NEC SX-ACE, SXAurora TSUBASA, Intel Xeon Gold, and Xeon Phi. The results show that the shortest execution times are with the red-black ordering method on SX-ACE and SX-Aurora TSUBASA, and with the non-vectorized version on Xeon Gold and Xeon Phi. Therefore, achieving a higher Perfomance on multiple modern supercomputers potentially requires maintenance of multiple code versions. We also show that the red-black ordering method is more promising to achieve high Perfomance on modern supercomputers.
Comparative Analysis of Virtualization Methods in Big Data Processing;Gleb I. Radchenko, Ameer B. A. Alaasam, Andrei N. Tchernykh;https://doi.org/10.14529/jsfi190107;"Big Data, visualization, containerization, cloud computing, Xen, KVM, Docker,
orchestration";Cloud computing systems have become widely used for Big Data processing, providing access to a wide variety of computing resources and a greater distribution between multi-clouds. This trend has been strengthened by the rapid development of the Internet of Things (IoT) concept. Virtualization via virtual machines and containers is a traditional way of organization of cloud computing infrastructure. Containerization technology provides a lightweight virtual runtime environment. In addition to the advantages of traditional virtual machines in terms of size and flexibility, containers are particularly important for integration tasks for PaaS solutions, such as application packaging and service orchestration. In this paper, we overview the current state-of-the-art of virtualization and containerization approaches and technologies in the context of Big Data tasks solution. We present the results of studies which compare the efficiency of containerization and virtualization technologies to solve Big Data problems. We also analyze containerized and virtualized services collaboration solutions to support automation of the deployment and execution of Big Data applications in the cloud infrastructure.
Supercomputer Lomonosov-2: Large Scale, Deep Monitoring and Fine Analytics for the User Community;Vladimir V. Voevodin, Alexander S. Antonov, Dmitry A. Nikitenko, Pavel A. Shvets, Sergey I. Sobolev, Igor Yu. Sidorov, Konstantin S. Stefanov, Vadim V. Voevodin, Sergey A. Zhumatiy;https://doi.org/10.14529/jsfi190201;supercomputer, peak Perfomance, sustained Perfomance, eciency, parallel computing, supercomputer center, software tools, scalability, monitoring, system level data, data analytics;The huge number of hardware and software components, together with a large number of parameters affecting the Perfomance of each parallel application, makes ensuring the efficiency of a large scale supercomputer extremely difficult. In this situation, all basic parameters of the supercomputer should be constantly monitored, as well as many decisions about its functioning should be made by special software automatically. In this paper we describe the tight connection between complexity of modern large HPC systems and special techniques and tools required to ensure their efficiency in practice. The main subsystems of the developed complex (Octoshell, DiMMoN, Octotron, JobDigest, and an expert software system to bring fine analytics on parallel applications and the entire supercomputer to users and sysadmins) are actively operated on the large supercomputer systems at Lomonosov Moscow State University. A brief description of the architecture of Lomonosov-2 supercomputer is presented, and questions showing both a wide variety of emerging complex issues and the need for an integrated approach to solving the problem of effectively supporting large supercomputer systems are discussed.
HPC Processors Benchmarking Assessment for Global System Science Applications;Damian Kaliszan, Norbert Meyer, Sebastian Petruczynik, Michael Gienger, Sergiy Gogolenko;https://doi.org/10.14529/jsfi190202;"Global Systems Science, HPC benchmarks, parallel applications, e-Infrastructure
evaluation";The work undertaken in this paper was done in the Centre of Excellence for Global Systems Science (CoeGSS) – an interdisciplinary project funded by the European Commission. CoeGSS project provides a computer-aided decision support in the face of global challenges (e.g. development of energy, water and food supply systems, urbanisation processes and growth of the cities, pandemic control, etc.) and tries to bring together HPC and global systems science. This paper presents a proposition of GSS benchmark which evaluates HPC architectures with respect to GSS applications and seeks for the best HPC system for typical GSS software environments. The outcome of the analysis is defining a benchmark which represents the average GSS environment and its challenges in a good way: spread of smoking habits and development of tobacco industry, development of green cars market and global urbanisation processes. Results of the tests that have been run on a number of recently appeared HPC platforms allow comparing processors’ architectures with respect to different applications using execution times, TDPs3 and TCOs4 as the basic metrics for ranking HPC architectures. Finally, we believe that our analysis of the results conveys a valuable information to the broadened GSS audience which might help to determine the hardware demands for their specific applications, as well as to the HPC community which requires a mature benchmark set reflecting requirements and traits of the GSS applications. Our work can be considered as a step into direction of development of such mature benchmark.
How File-access Patterns Influence the Degree of I/O Interference between Cluster Applications;Aamer Shah, Chih-Song Kuo, Akihiro Nomura, Satoshi Matsuoka, Felix Wolf;https://doi.org/10.14529/jsfi190203;Perfomance, I/O, file-access pattern, interference, benchmarking;On large-scale clusters, tens to hundreds of applications can simultaneously access a parallel file system, leading to contention and, in its wake, to degraded application Perfomance. In this article, we analyze the influence of file-access patterns on the degree of interference. As it is by experience most intrusive, we focus our attention on write-write contention. We observe considerable differences among the interference potentials of several typical write patterns. In particular, we found that if one parallel program writes large output files while another one writes small checkpointing files, then the latter is slowed down when the checkpointing files are small enough and the former is vice versa. Moreover, applications with a few processes writing large output files already can significantly hinder applications with many processes from checkpointing small files. Such effects can seriously impact the runtime of real applications—up to a factor of five in one instance. Our insights and measurement techniques offer an opportunity to automatically classify the interference potential between applications and to adjust scheduling decisions accordingly.
Investigating the Dirac Operator Evaluation with FPGAs;Grzegorz Korcyl, Piotr Korcyl;https://doi.org/10.14529/jsfi190204;HPC, FPGA, lattice QCD, Dirac operator evaluation;In recent years, computational capacity of single Field Programmable Gate Array (FPGA) devices as well as their versatility have increased significantly. Adding to that fact, the High Level Synthesis frameworks allowing to program such processors in a high-level language like C++, makes modern FPGA devices a serious candidate as building blocks of a general-purpose HPC solution. In this contribution we describe benchmarks which we performed using a kernel from the Lattice QCD code, a highly compute-demanding HPC academic code for elementary particle simulations on the newest device from Xilinx, the U250 accelerator card. We describe the architecture of our solution and benchmark its Perfomance on a single FPGA device running in two modes: using either external or embedded memory. We discuss both approaches in detail and provide assessment for the necessary memory throughput and the minimal amount of resources needed to deliver optimal Perfomance depending on the available hardware. Our considerations can be used as guidelines for estimating the Perfomance of some larger, manynode systems.
Development of a RISC-V-Conform Fused Multiply-Add Floating-Point Unit;Felix Kaiser, Stefan Kosnac, Ulrich Brüning;https://doi.org/10.14529/jsfi190205;"oating-point, multiply-add, risc-v, hardware-design, verification, uvm, synthesis,
asic, gf22fdx, ieee754";"Despite the fact that the open-source community around the RISC-V instruction set architecture is growing rapidly, there is still no high-speed open-source hardware implementation of the IEEE 754-2008 floating-point standard available. We designed a Fused Multiply-Add Floating-Point Unit compatible with the RISC-V ISA in SystemVerilog, which enables us to conduct detailed optimizations where necessary. The design has been verified with the industry standard simulation-based Universal Verification Methodology using the Specman e Hardware Verification Language. The most challenging part of the verification is the reference model, for which we integrated the Floating-Point Unit of an existing Intel processor using the Function Level Interface provided by Specman e. With the use of Intel's Floating-Point Unit we have a ``known good"" and fast reference model. The Back-End flow was done with Global Foundries' 22 nm Fully-Depleted Silicon-On-Insulator (GF22FDX) process using Cadence tools. We reached 1.8 GHz over PVT corners with a 0.8 V forward body bias, but there is still a large potential for further RTL optimization. A power analysis was conducted with stimuli generated by the verification environment and resulted in 212 mW."
Fully Implicit Time Stepping Can Be Efficient on Parallel Computers;Brandon Cloutier, Benson K. Muite, Matteo Parsani;https://doi.org/10.14529/jsfi190206;"incompressible Navier{Stokes equations, parallel computing, spectral methods, time
stepping";Benchmarks in HPC often involve a single component used in the full solution of a computational problem, such as the solution of a linear system of equations. In many cases, the choice of algorithm, which can determine the components used, is also important when solving a full problem. Numerical evidence suggests that for the Taylor-Green vortex problem at a Reynolds number of 1600, a second order implicit midpoint rule method can require less computational time than the often used linearly implicit Carpenter-Kennedy method for solving the equations of incompressible fluid dynamics for moderate levels of accuracy at the beginning of the flow evolution. The primary reason is that even though the implicit midpoint rule is fully implicit, it can use a small number of iterations per time step, and thus require less computational work per time step than the Carpenter-Kennedy method. For the same number of timesteps, the Carpenter-Kennedy method is more accurate since it uses a higher order timestepping method.
Perfomance Limits Study of Stencil Codes on Modern GPGPUs;Ilya S. Pershin, Vadim D. Levchenko, Anastasia Y. Perepelkina;https://doi.org/10.14529/jsfi190207;"stencil computations, parallel algorithm, GPU, CUDA, Rooine model";We study the Perfomance limits of different algorithmic approaches to the implementation of a sample problem of wave equation solution with a cross stencil scheme. With this, we aim to find the highest limit of the achievable Perfomance efficiency for stencil computing.To estimate the limits, we use a quantitative Roofline model to make a thorough analysis of the Perfomance bottlenecks and develop the model further to account for the latency of different levels of GPU memory. These estimates provide an incentive to use spatial and temporal blocking algorithms. Thus, we study stepwise, domain decomposition, and domain decomposition with halo algorithms in that order. The knowledge of the limit incites the motivation to optimize the implementation. This led to the analysis of the block synchronization methods in CUDA, which is also provided in the text.  After all optimizations, we have achieved 90% of the peak Perfomance, which amounts to more than 1 trillion cell updates per second on one consumer level GPU device.
Distinct Element Simulation of Mechanical Properties of Hypothetical CNT Nanofabrics;Igor A. Ostanin;https://doi.org/10.14529/jsfi190208;nanofibers, carbon nanotubes, distinct element method, parallel computing;A universal framework for modeling composites and fabrics of micro- and nanofibers, such as carbon nanotubes, carbon fibers and amyloid fibrils, is presented. Within this framework, fibers are represented with chains of rigid bodies, linked with elastic bonds. Elasticity of the bonds utilizes recently developed enhanced vector model formalism. The type of interactions between fibers is determined by their nature and physical length scale of the simulation. The dynamics of fibers is computed using the modification of rigid particle dynamics module of the waLBerla multiphysics framework. Our modeling system demonstrates exceptionally high parallel Perfomance combined with the physical accuracy of the modeling. The efficiency of our technique is demonstrated with an illustrative mechanical test on a hypothetical carbon nanotube textile. In this example, the elasticity of the fibers represents the coarse-grained covalent bond within CNT surface, whereas interfiber interactions represent coarse-grained van der Waals forces between cylindrical segments of nanotubes. Numerical simulation demonstrates stability and extremal strength of a hypothetical carbon nanotube fabric.
Collecting and Presenting Reproducible Intranode Stencil Perfomance: INSPECT;Julian Hornich, Julian Hammer, Georg Hager, Thomas Gruber, Gerhard Wellein;https://doi.org/10.14529/jsfi190301;"Perfomance modeling, Perfomance analysis, stencils, single-node, multi-core,
ECM, Roofline, memory hierarchy, cache effects";"Stencil algorithms have been receiving considerable interest in HPC research for decades. The techniques used to approach multi-core stencil Perfomance modeling and engineering span basic runtime measurements, elaborate Perfomance models, detailed hardware counter analysis, and thorough scaling behavior evaluation. Due to the plurality of approaches and stencil patterns, we set out to develop a generalizable methodology for reproducible measurements accompanied by state-of-the-art Perfomance models. Our open-source toolchain and collected results are publicly available in the ""Intranode Stencil Perfomance Evaluation Collection"" (INSPECT). We present the underlying methods, models and tools involved in gathering and documenting the Perfomance behavior of a collection of typical stencil patterns across multiple architectures and hardware configuration options. Our aim is to endow Perfomance-aware application developers with reproducible baseline Perfomance data and validated models to initiate a well-defined process of Perfomance assessment and optimization. All data is available for inspection: source code, produced assembly, Perfomance measurements, hardware Perfomance counter data, single-core and multicore Roofline and ECM (execution-cache-memory) Perfomance models, and machine properties. Deviations between measured Perfomance and Perfomance models become immediately evident and can be investigated. We also give hints as to how INSPECT can be used in practice for custom code analysis."
Supercomputer Docking;Alexey V. Sulimov, Danil C. Kutov, Vladimir B. Sulimov;https://doi.org/10.14529/jsfi190302;"docking, protein–ligand, global optimization, tensor train, force field, quantum–
chemical method";This review is based on the peer–reviewed research literature including the author’s own publications devoted to supercomputer docking. The general view on docking and its role at the initial stage of the rational drug design is presented. Molecules of medicine compounds selectively bind to the active site of a protein, which is responsible for the disease progression, and stop it. Docking programs perform positioning of molecules (ligands) in the active site of the protein and estimate the protein–ligand binding energy. The larger this energy is, the less concentration of the respective compound should be used to observe the desired effect. Several classical docking programs are described in short. Examples of the adaptation of existing docking programs to supercomputing and using them for virtual screening of millions of ligands are presented. Two novel generalized docking programs specially designed for multi–core docking of a single ligand on a supercomputer are described shortly. These programs find a sufficiently wide spectrum of low energy minima of a protein–ligand complex in the frame of a given force field. The quasi–docking procedure using the generalized docking program is described. Quasi–docking allows to perform docking with quantum–chemical semiempirical methods. Finally a summary is made based on the materials presented.
Automatic Port to OpenACC/OpenMP for Physical Parameterization in Climate and Weather Code Using the CLAW Compiler;Valentin Clement, Philippe Marti, Xavier Lapillonne, Oliver Fuhrer, William Sawyer;https://doi.org/10.14529/jsfi190303;compiler, directive, GPU, OpenACC, OpenMP, automatic port;In order to benefit from emerging HPC systems, weather and climate models need to be adapted to run efficiently on different hardware architectures such as accelerators. This is a major challenge for existing community models that represent extremely large codebase written in Fortran. Large parts of the code can be ported using OpenACC compiler directives but for time-critical components such as physical parameterizations, code restructuring and optimizations specific to a hardware architecture are necessary to obtain high Perfomance. In an effort to retain a single source code for multiple target architectures, the CLAW Compiler and the CLAW Single Column Abstraction were introduced. We report on the extension of the CLAW SCA to handle ELEMENTAL functions and subroutines. We demonstrate the new capability on the JSBACH land surface scheme of the ICON climate model. With the extension, JSBACH can be automatically ported to OpenACC or OpenMP for accelerators with minimal to no change to the original code.
Optimizing Deep Learning RNN Topologies on Intel Architecture;Kunal Banerjee, Evangelos Georganas, Dhiraj D. Kalamkar, Barukh Ziv, Eden Segal, Cristina Anderson, Alexander Heinecke;https://doi.org/10.14529/jsfi190304;LSTM, Intel Xeon, GEMM, compute-bound kernel, bandwidth-bound kernel;"Recurrent neural network (RNN) models have been found to be well suited for processing temporal data. In this work, we present an optimized implementation of vanilla RNN cell and its two popular variants: LSTM and GRU for Intel Xeon architecture. Typical implementations of these RNN cells employ one or two large matrix multiplication (GEMM) calls and then apply the element-wise operations (sigmoid/tanh) onto the GEMM results. While this approach is easy to implement by exploiting vendor-optimized GEMM library calls, the data reuse relies on how GEMMs are parallelized and is sub-optimal for GEMM sizes stemming from small minibatch. Also, the element-wise operations are exposed as a bandwidth-bound kernel after the GEMM which is typically a compute-bound kernel. To address this discrepancy, we implemented a parallel blocked matrix GEMM in order to (a) achieve load balance, (b) maximize weight matrix reuse, (c) fuse the element-wise operations after partial GEMM blocks are computed and while they are hot in cache. Additionally, we bring the time step loop in our cell to further increase the weight reuse and amortize the overhead to transform the weights into blocked layout. The results show that our implementation is generally faster than Intel MKL-DNN library implementations, e.g. for RNN, forward pass is up to ~3× faster whereas the backward/weight update pass is up to ~5× faster. Furthermore, we investigate high-Perfomance implementations of sigmoid and tanh activation functions that achieve various levels of accuracy. These implementations rely on minimax polynomial approximations, rational polynomials, Taylor expansions and exponential approximation techniques. Our vectorized implementations can be flexibly integrated into deep learning computations with different accuracy requirements without compromising Perfomance; in fact, these are able to outperform vectorized and reduced accuracy vendor-optimized (Intel SVML) libraries by 1.6–2.6× while speep up over GNU libm is close to two orders of magnitude. All our experiments are conducted on Intel’s latest CascadeLake architecture."
A Skewed Multi-banked Cache for Many-core Vector Processors;Hikaru Takayashiki, Masayuki Sato, Kazuhiko Komatsu, Hiroaki Kobayashi;https://doi.org/10.14529/jsfi190305;HPC, vector architecture, cache, skewed-associativity;As the number of cores and the memory bandwidth have increased in a balanced fashion, modern vector processors achieve high sustained Perfomances, especially in memory-intensive applications in the fields of science and engineering. However, it is difficult to significantly increase the off-chip memory bandwidth owing to the limitation of the number of input/output pins integrated on a single chip. Under the circumstances, modern vector processors have adopted a shared cache to realize a high sustained memory bandwidth. The shared cache can effectively reduce the pressure to the off-chip memory bandwidth by keeping reusable data that multiple vector cores require. However, as the number of vector cores sharing a cache increases, more different blocks requested from multiple cores simultaneously use the same set. As a result, conflict misses caused by these blocks degrade the Perfomance.In order to avoid increasing the conflict misses in the case of the increasing number of cores, this paper proposes a skewed cache for many-core vector processors. The skewed cache prevents the simultaneously requested blocks from being stored into the same set. This paper discusses how the most important two features of the skewed cache should be implemented in modern vector processors: hashing function and replacement policy. The proposed cache adopts the oddmultiplier displacement hashing for effective skewing and the static re-reference interval prediction policy for reasonable replacing. The evaluation results show that the proposed cache significantly improves the Perfomance of a many-core vector processor by eliminating conflict misses.
An Energy-aware Dynamic Data Allocation Mechanism for Many-channel Memory Systems;Masayuki Sato, Takuya Toyoshima, Hikaru Takayashiki, Ryusuke Egawa, Hiroaki Kobayashi;https://doi.org/10.14529/jsfi190401;DRAM, main memory, low-power mode, address-mapping scheme, energy consumption;A modern memory system is equipped with many memory channels to obtain a high memory bandwidth. To take the advantage of this organization, applications’ data are distributed among the channels and transferred in an interleaved fashion. Although memory-intensive applications benefit from a high bandwidth by many memory channels, applications such as compute-intensive ones do not need the high bandwidth. To reduce the energy consumption for such applications, the memory system has low-power modes. During no memory request, the main memory can enter these modes and reduce energy consumption. However, these applications often cause intermittent memory requests to the channels that handle their data, resulting in not entering the low-power modes. Hence, the memory system cannot enter the low-power modes even though the applications do not need the high bandwidth. To solve this problem, this paper proposes a dynamic data allocation mechanism for many-channel memory systems. This mechanism forces data of such applications to use the specified channels by dynamically changing the address-mapping schemes and migrating the data. As a result, the other channels to which the data are not allocated can have a chance to enter the low-power modes for a long time. Therefore, the proposed mechanism has the potential to reduce the energy consumption of many-channel memory systems. The evaluation results show that this mechanism can reduce the energy consumption by up to 11.8% and 1.7% on average.
Towards Heterogeneous Multi-scale Computing on Large Scale Parallel Supercomputers;Saad A. Alowayyed, Maxime Vassaux, Ben Czaja, Peter V. Coveney, Alfons G. Hoekstra;https://doi.org/10.14529/jsfi190402;multi-scale modelling, surrogate model, computational science, heterogeneous multiscale computing, HPC, exascale;New applications that can exploit emerging exascale computing resources efficiently, while providing meaningful scientific results, are eagerly anticipated. Multi-scale models, especially multi-scale applications, will assuredly run at the exascale. We have established that a class of multi-scale applications implementing the heterogeneous multi-scale model follows, a heterogeneous multi-scale computing (HMC) pattern, which typically features a macroscopic model synchronising numerous independent microscopic model simulations. Consequently, communication between microscopic simulations is limited. Furthermore, a surrogate model can often be introduced between macro-scale and micro-scale models to interpolate required data from previously computed micro-scale simulations, thereby substantially reducing the number of micro-scale simulations. Nonetheless, HMC applications, though versatile, remain constrained by load balancing issues. We discuss two main issues: the a priori unknown and variable execution time of microscopic simulations, and the dynamic number of micro-scale simulations required. We tackle execution time variability using a pilot job mechanism to handle internal queuing and multiple sub-model execution on large-scale supercomputers, together with a data-informed execution time prediction model. To dynamically select the number of micro-scale simulations, the HMC pattern automatically detects and identifies three surrogate model phases that help control the available and used core amount. After relevant phase detection and micro-scale simulation scheduling, any idle cores can be used for surrogate model update or for processor release back to the system. We demonstrate HMC Perfomance by testing it on two representative multi-scale applications. We conclude that, considering the subtle interplay between the macroscale model, surrogate models and micro-scale simulations, HMC provides a promising path towards exascale for many multiscale applications.
Improving Reliability of Supercomputer CFD Codes on Unstructured Meshes;Andrey V. Gorobets, Pavel A. Bakhvalov;https://doi.org/10.14529/jsfi190403;CFD, supercomputer, unstructured mesh, data structure, MPI, OpenMP, OpenCL;The paper describes a particular technical solution targeted at improving reliability and quality of a highly-parallel computational fluid dynamics code written in C++. The code considered is based on rather complex high-accuracy numerical methods and models for simulation of turbulent flows on unstructured hybrid meshes. The cost of software errors is very high in largescale supercomputer simulations. Reproducing and localizing errors, especially “magic” unstable bugs related with wrong memory access, are extremely problematic due to the large amount of computing resources involved. In order to prevent, or at least notably filter out memory bugs, an approach of increased reliability is proposed for representing mesh data and organizing memory access. A set of containers is proposed, which causes no overhead in the release configuration compared to plain arrays. At the same time, it provides throughout access control in the safe mode configuration and additional compile-time protection from programming errors. Furthermore, it is fully compatible with heterogeneous computing within the OpenCL standard. The proposed approach provides internal debugging capabilities that allow us to localize problems directly in a supercomputer simulation.
Survey on Software Tools that Implement Deep Learning Algorithms on Intel/x86 and IBM/Power8/Power9 Platforms;Denis Shaikhislamov, Andrey Sozykin, Vadim Voevodin;https://doi.org/10.14529/jsfi190404;HPC, neural networks, deep learning frameworks, distributed training;Neural networks are becoming more and more popular in scientific field and in the industry. It is mostly because new solutions using neural networks show state-of-the-art results in the domains previously occupied by traditional methods, eg. computer vision, speech recognition etc. But to get these results neural networks become progressively more complex, thus needing a lot more training. The training of neural networks today can take weeks. This problems can be solved by parallelization of the neural networks training and using modern clusters and supercomputers, which can significantly reduce the learning time. Today, a faster training for data scientist is essential, because it allows to get the results faster to make the next decision.In this paper we provide an overview of distributed learning provided by the popular modern deep learning frameworks, both in terms of provided functionality and Perfomance. We consider multiple hardware choices: training on multiple GPUs and multiple computing nodes.
State of the Art and Future Trends in Data Reduction for HPC;Kira Duwe, Jakob Lüttgau, Georgiana Mania, Jannek Squar, Anna Fuchs, Michael Kuhn, Eugen Betke, Thomas Ludwig;https://doi.org/10.14529/jsfi200101;"data reduction, lossless compression, lossy compression, dimensionality reduction,
adaptive approaches, deduplication, in situ, recomputation, scientific data set";Research into data reduction techniques has gained popularity in recent years as storage capacity and Perfomance become a growing concern. This survey paper provides an overview of leveraging points found in HPC (HPC) systems and suitable mechanisms to reduce data volumes. We present the underlying theories and their application throughout the HPC stack and also discuss related hardware acceleration and reduction approaches. After introducing relevant use-cases, an overview of modern lossless and lossy compression algorithms and their respective usage at the application and file system layer is given. In anticipation of their increasing relevance for adaptive and in situ approaches, dimensionality reduction techniques are summarized with a focus on non-linear feature extraction. Adaptive approaches and in situ compression algorithms and frameworks follow. The key stages and new opportunities to deduplication are covered next. An unconventional but promising method is recomputation, which is proposed at last. We conclude the survey with an outlook on future developments.
Development of Computational Pipeline Software for Genome/Exome Analysis on the K Computer;Kento Aoyama, Masanori Kakuta, Yuri Matsuzaki, Takashi Ishida, Masahito Ohue, Yutaka Akiyama;https://doi.org/10.14529/jsfi200102;"pipeline software, software development, K computer, message passing interface,
genome analysis, exome analysis";Pipeline software that comprise tool and application chains for specific data processing have found extensive utilization in the analysis of several data types, such as genome, in bioinformatics research. Recent trends in genome analysis require use of pipeline software for optimum utilization of computational resources, thereby facilitating efficient handling of large-scale biological data accumulated on a daily basis. However, use of pipeline software in bioinformatics tends to be problematic owing to their large memory and storage capacity requirements, increasing number of job submissions, and a wide range of software dependencies. This paper presents a massive parallel genome/exome analysis pipeline software that addresses these difficulties. Additionally, it can be executed on a large number of K computer nodes. The proposed pipeline incorporates workflow management functionality that performs effectively when considering the task-dependency graph of internal executions via extension of the dynamic task distribution framework. Perfomance results pertaining to the core pipeline functionality, obtained via evaluation experiments performed using an actual exome dataset, demonstrate good scalability when using over a thousand nodes. Additionally, this study proposes several approaches to resolve Perfomance bottlenecks of a pipeline by considering the domain knowledge pertaining to internal pipeline executions as a major challenge facing pipeline parallelization. 
Supercomputing Technologies as Drive for Development of Enterprise Information Systems and Digital Economy;Oleg V. Loginovsky, Alexander L. Shestakov, Alexander A. Shinkarev;https://doi.org/10.14529/jsfi200103;"enterprise information systems, parallel computing, supercomputing technologies,
big data, machine learning, scalability, event stream, analysis, digital economy";The article presents an analysis of approaches to the development of enterprise information systems that are in use today. One of the major trends that predetermines the agenda of information technology is the focus on parallel computing of large volumes of data using supercomputing technologies. The article considers the resulting ubiquitous move to distributed patterns of building enterprise information systems and avoiding monolithic architectures. The emphasis is placed on the importance of such fundamental characteristics of enterprise information systems as reliability, scalability, and maintainability. The article justifies the importance of machine learning in the context of effective big data analysis and competitive gain for business, vital for both maintaining a leading position in the market and surviving in conditions of global instability and digitalization of economy. Transition from storing the current state of a enterprise information system to storing a full log and history of all changes in the event stream is proposed as an instrument of achieving linearization of the data stream for subsequent parallel computing. There is a new view that is being shaped of specialists at the intersection of engineering and analytical disciplines, who would be able to effectively develop scalable systems and algorithms for data processing and integration of its results into company business processes.
Online MPI Process Mapping for Coordinating Locality and Memory Congestion on NUMA Systems;Mulya Agung, Muhammad Alfian Amrizal, Ryusuke Egawa, Hiroyuki Takizawa;https://doi.org/10.14529/jsfi200104;communication, congestion, locality, MPI, multi-core, NUMA, process mapping;Mapping MPI processes to processor cores, called process mapping, is crucial to achieving the scalable Perfomance on multi-core processors. By analyzing the communication behavior among MPI processes, process mapping can improve the communication locality, and thus reduce the overall communication cost. However, on modern non-uniform memory access (NUMA) systems, the memory congestion problem could degrade Perfomance more severely than the locality problem because heavy congestion on shared caches and memory controllers could cause long latencies. Most of the existing work focus only on improving the locality or rely on offline profiling to analyze the communication behavior.We propose a process mapping method that dynamically performs the process mapping for adapting to communication behaviors while coordinating the locality and memory congestion. Our method works online during the execution of an MPI application. It does not require modifications to the application, previous knowledge of the communication behavior, or changes to the hardware and operating system. Experimental results show that our method can achieve Perfomance and energy efficiency close to the best static mapping method with low overhead to the application execution. In experiments with the NAS parallel benchmarks on a NUMA system, the Perfomance and total energy improvements are up to 34% (18.5% on average) and 28.9% (13.6% on average), respectively. In experiments with two GROMACS applications on a larger NUMA system, the average improvements in Perfomance and total energy consumption are 21.6% and 12.6%, respectively.
Tools for GPU Computing – Debugging and Perfomance Analysis of Heterogenous HPC Applications;Michael Knobloch, Bernd Mohr;https://doi.org/10.14529/jsfi200105;Perfomance analysis, debugging, GPU computing;General purpose GPUs are now ubiquitous in high-end supercomputing. All but one (the Japanese Fugaku system, which is based on ARM processors) of the announced (pre-)exascale systems contain vast amounts of GPUs that deliver the majority of the Perfomance of these systems. Thus, GPU programming will be a necessity for application developers using high-end HPC systems.However, programming GPUs efficiently is an even more daunting task than traditional HPC application development. This becomes even more apparent for large-scale systems containing thousands of GPUs. Orchestrating all the resources of such a system imposes a tremendous challenge to developers. Luckily a rich ecosystem of tools exist to assist developers in every development step of a GPU application at all scales.In this paper we present an overview of these tools and discuss their capabilities. We start with an overview of different GPU programming models, from low-level with CUDA over pragma-based models like OpenACC to high-level approaches like Kokkos. We discuss their respective tool interfaces as the main method for tools to obtain information on the execution of a kernel on the GPU. The main focus of this paper is on two classes of tools, debuggers and Perfomance analysis tools. Debuggers help the developer to identify problems both on the CPU and GPU side as well as in the interplay of both. Once the application runs correctly, Perfomance analysis tools can be used to pinpoint bottlenecks in the execution of the code and help to increase the overall Perfomance.
Building a Vision for Reproducibility in the Cyberinfrastructure Ecosystem: Leveraging Community Efforts;Dylan Chapp, Victoria Stodden, Michela Taufer;https://doi.org/10.14529/jsfi200106;"reproducibility, replicability, transparency, HPC, molecular
dynamics, in situ analytics";"The scientific computing community has long taken a leadership role in understanding and assessing the relationship of reproducibility to cyberinfrastructure, ensuring that computational results - such as those from simulations - are ""reproducible"", that is, the same results are obtained when one re-uses the same input data, methods, software and analysis conditions. Starting almost a decade ago, the community has regularly published and advocated for advances in this area. In this article we trace this thinking and relate it to current national efforts, including the 2019 National Academies of Science, Engineering, and Medicine report on ""Reproducibility and Replication in Science"".To this end, this work considers HPC workflows that emphasize workflows combining traditional simulations (e.g. Molecular Dynamics simulations) with in situ analytics. We leverage an analysis of such workflows to (a) contextualize the 2019 National Academies of Science, Engineering, and Medicine report's recommendations in the HPC setting and (b) envision a path forward in the tradition of community driven approaches to reproducibility and the acceleration of science and discovery. The work also articulates avenues for future research at the intersection of transparency, reproducibility, and computational infrastructure that supports scientific discovery."
Perfomance Reduction For Automatic Development of Parallel Applications  For Reconfigurable Computer Systems;Alexey I. Dordopulo, Ilya I. Levin;https://doi.org/10.14529/jsfi200201;"Perfomance reduction, hardware costs, reconfigurable computer system, parallel
applications development, information graph";In the paper, we review a suboptimal methodology of mapping of a task information graph on the architecture of a reconfigurable computer system. Using Perfomance reduction methods, we can solve computational problems which need hardware costs exceeding the available hardware resource. We proved theorems, concerning properties of sequential reductions. In our case, we have the following types of reduction such as the reduction by number of basic subgraphs, by number of computing devices, and by data width. On the base of the proved theorems and corollaries, we developed the methodology of reduction transformations of a task information graph for its automatic adaptation to the architecture of a reconfigurable computer system. We estimated the maximum number of transformations, which, according to the suggested methodology, are needed for balanced reduction of the Perfomance and hardware costs of applications for reconfigurable computer systems.
Long Distance Geographically Distributed InfiniBand Based Computing;Karol Niedzielewski, Marcin Semeniuk, Jarosław Skomiał, Jerzy Proficz, Piotr Sumioka, Bartosz Pliszka, Marek Michalewicz;https://doi.org/10.14529/jsfi200202;"HPC, distributed computing and systems, InfiniBand, federated supercomputing,
geographically distributed workflows, ADIOS, HPX, High Perfomance Parallex";Collaboration between multiple computing centres, referred as federated computing is becoming important pillar of HPC (HPC) and will be one of its key components in the future. To test technical possibilities of future collaboration using 100Gb optic fiber link (Connection was 900 km in length with 9ms RTT time) we prepared two scenarios of operation.In the first one, Interdisciplinary Centre for Mathematical and Computational Modelling (ICM) in Warsaw and Centre of Informatics - Tricity Academic Supercomputer & networK (CI-TASK) in Gdańsk prepared a long distance geographically distributed computing cluster. System consisted of 14 nodes (10 nodes at ICM facility and 4 at TASK facility) connected using InfiniBand. Our tests demonstrate that it is possible to perform computationally intensive data analysis on systems of this class without substantial drop in Perfomance for a certain type of workloads. Additionally, we show that it is feasible to use High Perfomance Parallex [1], high level abstraction libraries for distributed computing, to develop software for such geographically distributed computing resources and maintain desired efficiency.In the second scenario, we prepared distributed simulation-postprocessing-visualization workflow using ADIOS2 [2] and two programming languages (C++ and python). In this test we prove capabilities of performing different parts of analysis in seperate sites.
Potential of I/O Aware Workflows in Climate and Weather;Julian M. Kunkel, Luciana R. Pedro;https://doi.org/10.14529/jsfi200203;workflow, heterogeneous storage, data-driven, climate/weather;The efficient, convenient, and robust execution of data-driven workflows and enhanced data management are essential for productivity in scientific computing. In HPC, the concerns of storage and computing are traditionally separated and optimised independently from each other and the needs of the end-to-end user. However, in complex workflows, this is becoming problematic. These problems are particularly acute in climate and weather workflows, which as well as becoming increasingly complex and exploiting deep storage hierarchies, can involve multiple data centres.The key contributions of this paper are: 1) A sketch of a vision for an integrated data-driven approach, with a discussion of the associated challenges and implications, and 2) An architecture and roadmap consistent with this vision that would allow a seamless integration into current climate and weather workflows as it utilises versions of existing tools (ESDM, Cylc, XIOS, and DDN’s IME).The vision proposed here is built on the belief that workflows composed of data, computing, and communication-intensive tasks should drive interfaces and hardware configurations to better support the programming models. When delivered, this work will increase the opportunity for smarter scheduling of computing by considering storage in heterogeneous storage systems. We illustrate the Perfomance-impact on an example workload using a model built on measured Perfomance data using ESDM at DKRZ.
Bridging the Architecture Gap: Abstracting Perfomance-Relevant Properties of Modern Server Processors;Johannes Hofmann, Christie L. Alappat, Georg Hager, Dietmar Fey, Gerhard Wellein;https://doi.org/10.14529/jsfi200204;"microarchitecture comparison, Intel, AMD, ARM, IBM, Perfomance evaluation,
Perfomance modeling, analytic modeling, execution-cache-memory model";We propose several improvements to the execution-cache-memory (ECM) model, an analytic Perfomance model for predicting single- and multicore runtime of steady-state loops on server processors. The model is made more general by strictly differentiating between application and machine models: an application model comprises the loop code, problem sizes, and other runtime parameters, while a machine model is an abstraction of all Perfomance-relevant properties of a processor. Moreover, new first principles underlying the model’s estimates are derived from common microarchitectural features implemented by today’s server processors to make the model more architecture independent, thereby extending its applicability beyond Intel processors. We introduce a generic method for determining machine models, and present results for relevant server-processor architectures by Intel, AMD, IBM, and Marvell/Cavium. Considering this wide range of architectures, the set of features required for adequate Perfomance modeling is surprisingly small. To validate our approach, we compare Perfomance predictions to empirical data for an OpenMP-parallel preconditioned CG algorithm, which includes compute- and memory-bound kernels. Both single- and multicore analysis shows that the model exhibits average and maximum relative errors of 5 % and 10 %. Deviations from the model and insights gained are discussed in detail. 
Dawn: a High-level Domain-Specific Language Compiler Toolchain for Weather and Climate Applications;Carlos Osuna, Tobias Wicky, Fabian Thuering, Torsten Hoefler, Oliver Fuhrer;https://doi.org/10.14529/jsfi200205;GPGPU computing, DSL, weather and climate, code optimization, compiler, Perfomance portability;High-level programming languages that allow to express numerical methods and generate efficient parallel implementations are of key importance for the productivity of domain-scientists. The diversity and complexity of hardware architectures is imposing a huge challenge for large and complex models that must be ported and maintained for multiple architectures combining various parallel programming models. Several domain-specific languages (DSLs) have been developed to address the portability problem, but they usually impose a parallel model for specific numerical methods and support optimizations for limited scope operators. Dawn provides a high-level concise language for expressing numerical finite difference/volume methods using a sequential and descriptive language. The sequential statements are transformed into an efficient target-dependent parallel implementation by the Dawn compiler toolchain. We demonstrate our approach on the dynamical solver of the COSMO model, achieving Perfomance improvements and code size reduction of up to 2x and 5x, respectively.
Accounting of Receptor Flexibility in Ultra-Large Virtual Screens with VirtualFlow Using a Grey Wolf Optimization Method;Christoph Gorgulla, Konstantin Fackeldey, Gerhard Wagner, Haribabu Arthanari;https://doi.org/10.14529/jsfi200301;"ultra-large virtual screening, molecular docking, drug discovery, COVID-19,
structure-based drug design, CADD, computer aided drug design, AutoDock, grey wolf optimization, cloud computing";Structure-based virtual screening approaches have the ability to dramatically reduce the time and costs associated to the discovery of new drug candidates. Studies have shown that the true hit rate of virtual screenings improves with the scale of the screened ligand libraries. Therefore, we have recently developed an open source drug discovery platform (VirtualFlow), which is able to routinely carry out ultra-large virtual screenings. One of the primary challenges of molecular docking is the circumstance when the protein is highly dynamic or when the structure of the protein cannot be captured by a static pose. To accommodate protein dynamics, we report the extension of VirtualFlow to allow the docking of ligands using a grey wolf optimization algorithm using the docking program GWOVina, which substantially improves the quality and efficiency of flexible receptor docking compared to AutoDock Vina. We demonstrate the linear scaling behavior of VirtualFlow utilizing GWOVina up to 128 000 CPUs. The newly supported docking method will be valuable for drug discovery projects in which protein dynamics and flexibility play a significant role.
Perspectives on Supercomputing and Artificial Intelligence Applications in Drug Discovery;Jun Xu, Jiming Ye;https://doi.org/10.14529/jsfi200302;drug discovery, big data, artificial intelligence, HPC;This review starts with outlining how science and technology evaluated from last century into high throughput science and technology in modern era due to the Nobel-Prize-level inventions of combinatorial chemistry, polymerase chain reaction, and high-throughput screening. The evolution results in big data accumulated in life sciences and the fields of drug discovery. The big data demands for supercomputing in biology and medicine, although the computing complexity is still a grand challenge for sophisticated biosystems in drug design in this supercomputing era. In order to resolve the real-world issues, artificial intelligence algorithms (specifically machine learning approaches) were introduced, and have demonstrated the power in discovering structure-activity relations hidden in big biochemical data. Particularly, this review summarizes on how people modernize the conventional machine learning algorithms by combing non-numeric pattern recognition and deep learning algorithms, and successfully resolved drug design and high throughput screening issues. The review ends with the perspectives on computational opportunities and challenges in drug discovery by introducing new drug design principles and modeling the process of packing DNA with histones in micrometer scale space, a n example of how a macrocosm object gets into microcosm world.
Computational Modeling of the SARS-CoV-2 Main Protease Inhibition by the Covalent Binding of Prospective Drug Molecules;Alexander V. Nemukhin, Bella L. Grigorenko, Igor V. Polyakov, Sofya V. Lushchekina;https://doi.org/10.14529/jsfi200303;"SARS-CoV-2 main protease, QM/MM, molecular docking, molecular dynamics,
covalent inhibitor";We illustrate modern modeling tools applied in the computational design of drugs acting as covalent inhibitors of enzymes. We take the Main protease (Mpro) from the SARS-CoV-2 virus as an important present-day representative. In this work, we construct a compound capable to block Mpro, which is composed of fragments of antimalarial drugs and covalent inhibitors of cysteine proteases. To characterize the mechanism of its interaction with the enzyme, the algorithms based on force fields, including molecular mechanics (MM), molecular dynamics (MD) and molecular docking, as well as quantum-based approaches, including quantum chemistry and quantum mechanics/molecular mechanics (QM/MM) methods, should be applied. The use of supercomputers is indispensably important at least in the latter approach. Its application to enzymes assumes that energies and forces in the active sites are computed using methods of quantum chemistry, whereas the rest of protein matrix is described using conventional force fields. For the proposed compound, containing the benzoisothiazolone fragment and the substitute at the uracil ring, we show that it can form a stable covalently bound adduct with the target enzyme, and thus can be recommended for experimental trials.
Computational Characterization of the Substrate Activation in the Active Site of SARS-CoV-2 Main Protease;Maria G. Khrenova, Vladimir G. Tsirelson, Alexander V. Nemukhin;https://doi.org/10.14529/jsfi200304;"SARS-CoV-2 main protease, QM/MM MD, GPU-accelerated algorithms, substrate
activation";Molecular dynamics simulations with the QM(DFT)/MM potentials are utilized to discriminate between reactive and nonreactive complexes of the SARS-CoV-2 main protease and its substrates. Classification of frames along the molecular dynamic trajectories is utilized by analysis of the 2D maps of the Laplacian of electron density. Those are calculated in the plane formed by the carbonyl group of the substrate and a nucleophilic sulfur atom of the cysteine residue that initiates enzymatic reaction. Utilization of the GPU-based DFT code allows fast and accurate simulations with the hybrid functional PBE0 and double-zeta basis set. Exclusion of the polarization functions accelerates the calculations 2-fold, however this does not describe the substrate activation. Larger basis set with d-functions on heavy atoms and p-functions on hydrogen atoms enables to disclose equilibrium between the reactive and nonreactive species along the MD trajectory. The suggested approach can be utilized to choose covalent inhibitors that will readily interact with the catalytic residue of the selected enzyme.
In Search of Non-covalent Inhibitors of SARS-CoV-2 Main Protease: Computer Aided Drug Design Using Docking and Quantum Chemistry;Alexey V. Sulimov, Danil C. Kutov, Anna S. Taschilova, Ivan S. Ilin, Nadezhda V. Stolpovskaya, Khidmet S. Shikhaliev, Vladimir B. Sulimov;https://doi.org/10.14529/jsfi200305;"docking, global optimization, quantum docking, inhibitors, CADD, SARS–CoV–2,
COVID–19, Mpro";Two stages virtual screening of a database containing several thousand low molecular weight organic compounds is performed with the goal to find inhibitors of SARS-CoV-2 main protease. Overall near 41000 different 3D molecular structures have been generated from the initial molecules taking into account several conformers of most molecules. At the first stage the classical SOL docking program is used to determine most promising candidates to become inhibitors. SOL employs the MMFF94 force field, the genetic algorithm (GA) of the global energy optimization, takes into account the desolvation effect arising upon protein-ligand binding and the internal stress energy of the ligand. Parameters of GA are selected to perform the meticulous global optimization, and for docking of one ligand several hours on one computing core are needed on the average. The main protease model is constructed on the base of the protein structure from the Protein Data Bank complex 6W63. More than 1000 ligands structures have been selected for further postprocessing. The SOL score values of these ligands are  more negative than the threshold of –6.3 kcal/mol obtained for the native X77 ligand docking. Subsequent calculation of the protein-ligand binding enthalpy by the PM7 quantum-chemical semiempirical method with COSMO solvent model have narrowed down the number of best candidates. Finally, the diverse set of 20 most perspective candidates for the in vitro validation are selected.
Computational Approaches To Identify A Hidden Pharmacological Potential In Large Chemical Libraries;Dmitry S. Druzhilovskiy, Leonid A. Stolbov, Polina I. Savosina, Pavel V. Pogodin, Dmitry A. Filimonov, Alexander V. Veselovsky, Karen Stefanisko, Nadya I. Tarasova, Marc C. Nicklaus, Vladimir V. Poroikov;https://doi.org/10.14529/jsfi200306;drug discovery, chemical-pharmacological space, big data analysis, similarity assessment, machine learning, molecular modeling, virtual screening, HIV/AIDS, SAVI, COVID-19;To improve the discovery of more effective and less toxic pharmaceutical agents, large virtual repositories of synthesizable molecules have been generated to increase the explored chemical-pharmacological space diversity. Such libraries include billions of structural formulae of drug-like molecules associated with data on synthetic schemes, required building blocks, estimated physical-chemical parameters, etc. Clearly, such repositories are “Big Data”. Thus, to identify the most promising compounds with the required pharmacological properties (hits) among billions of available opportunities, special computational methods are necessary. We have proposed using a combined computational approach, which combines structural similarity assessment, machine learning, and molecular modeling. Our approach has been validated in a project aimed at finding new pharmaceutical agents against HIV/AIDS and associated comorbidities from the Synthetically Accessible Virtual Inventory (SAVI), a 1.75 billion compound database. Potential inhibitors of HIV-1 protease and reverse transcriptase and agonists of toll-like receptors and STING, affecting innate immunity, were computationally identified. The activity of the three synthesized compounds has been confirmed in a cell-based assay. These compounds belong to the chemical classes, in which the agonistic effect on TLR 7/8 had not been previously shown. Synthesis and biological testing of several dozens of compounds with predicted antiretroviral activity are currently taking place at the NCI/NIH. We also carried out virtual screening among one billion substances to find compounds potentially possessing anti-SARS-CoV-2 activity. The selected hits' information has been accepted by the European Initiative “JEDI Grand Challenge against COVID-19” for synthesis and further biological evaluation. The possibilities and limitations of the approach are discussed.
Effects of Using a Memory Stalled Core for Handling MPI Communication Overlapping in the SOR Solver on SX-ACE and SX-Aurora TSUBASA;Takashi Soga, Kenta Yamaguchi, Raghunandan Mathur, Osamu Watanabe, Akihiro Musa, Ryusuke Egawa, Hiroaki Kobayashi;https://doi.org/10.14529/jsfi200401;Thermal plasma flows, SOR method, MPI, OpenMP, Perfomance tuning, SXACE, SX-Aurora TSUBASA;Modern HPC (HPC) systems consist of a large number of nodes featuring multi-core processors. Many computational fluid dynamics (CFD) codes utilize a Message Passing Interface (MPI) to exploit the potential of such systems. In general, the MPI communication costs increase as the number of MPI processes increases. In this paper, we discuss Perfomance of the code in which a core is used as a dedicated communication core when the core cannot contribute to the Perfomance improvement due to memory-bandwidth limitations. By using the dedicated communication core, the communication operations are overlapped with computation operations, thus enabling highly efficient computation by exploiting the limited memory bandwidth and idle cores. The Perfomance evaluation shows that this code can hide the MPI communication times of 90% on the supercomputer SX-ACE system and 80% on the supercomputer SX-Aurora TSUBASA system, and the Perfomance of the successive over-relaxation (SOR) method is improved by 32% on SX-ACE and 20% on SX-Aurora TSUBASA.
Enhancing the in Situ Visualization of Perfomance Data in Parallel CFD Applications;Rigel F. C. Alves, Andreas Knüpfer;https://doi.org/10.14529/jsfi200402;"parallel computing, Perfomance analysis, in situ processing, computational fluid
dynamics";This paper continues the work initiated by the authors on the feasibility of using ParaView as visualization software for the analysis of parallel CFD codes’ Perfomance. Current Perfomance tools are unable to show their data on top of complex simulation geometries (e.g. an aircraft engine). In our previous paper, a plugin for the open-source Perfomance tool Score-P has been introduced, which intercepts an arbitrary number of manually selected code regions (mostly functions) and send their respective measurements – amount of executions and cumulative time spent – to ParaView (through its in situ library, Catalyst), as if they are any other flow-related variable. This paper adds to such plugin the capacity to also show communication data (messages sent between MPI ranks) on top of the CFD mesh. Testing is done again with Rolls-Royce’s in-house CFD code, Hydra. The plugin’s original feature (regions’ measurements) is here revisited, in a bigger test-case, which is also used to illustrate the new feature (communication data). The benefits and overhead of the tool are discussed.
Improving Quantum Annealing Perfomance on Embedded Problems;Michael R. Zielewski, Mulya Agung, Ryusuke Egawa, Hiroyuki Takizawa;https://doi.org/10.14529/jsfi200403;quantum annealing, quantum computer, job-shop scheduling, combinatorial optimization;Recently, many researchers are investigating quantum annealing as a solver for real-world combinatorial optimization problems. However, due to the format of problems that quantum annealing solves and the structure of the physical annealer, these problems often require additional setup prior to solving. We study how these setup steps affect Perfomance and provide insight into the interplay among them using the job-shop scheduling problem for our evaluation. We show that the empirical probability of success is highly sensitive to problem setup and that excess variables and large embeddings reduce Perfomance. We then show that certain problem instances are unable to be solved without the use of additional post-processing methods. Finally, we investigate the effect of pausing during the anneal. Our results show that pausing within a certain time window can improve the probability of success, which is consistent with other work. However, we also show that the Perfomance improvement due to pausing can be masked depending on properties of the embedding, and thus, special care must be taken for embedded problems.
Developing an Architecture-independent Graph Framework for Modern Vector Processors and NVIDIA GPUs;Ilya V. Afanasyev;https://doi.org/10.14529/jsfi200404;"vector computers, NVIDIA GPUs, graph algorithms, graph framework, VGL,
CUDA, optimisation";This paper describes the first-in-the-world attempt to develop an architectural-independent graph framework named VGL, designed for different modern architectures with high-bandwidth memory. Currently VGL supports two classes of architectures: NEC SX-Aurora TSUBASA vector processors and NVIDIA GPUs. However, VGL can be easily extended to other architectures due to its flexible software structure. VGL is designed to provide users with the possibility of selecting the most suitable architecture for solving a specific graph problem on a given input data, which, in return, allows to significantly outperform existing frameworks and libraries, developed for modern multicore CPUs and NVIDIA GPUs. Since VGL uses an identical set of computational and data abstractions for all architectures, its users can easily port graph algorithms between different target architectures without any source code modifications. Additionally, in this paper we show how graph algorithms should be implemented and optimised for NVIDIA GPU and NEC SX-Aurora TSUBASA architectures, demonstrating that both architectures have multiple similar properties and hardware features.
Update on Perfomance Analysis of Different Computational Architectures: Molecular Dynamics in Application to Protein-Protein Interactions;Vladimir A. Fedorov, Ekaterina G. Kholina, Ilya B. Kovalenko, Nikita B. Gudimchuk, Philipp S. Orekhov, Artem A. Zhmurov;https://doi.org/10.14529/jsfi200405;molecular dynamics, coarse grain, tubulin, microtubule, Ndc80;Molecular dynamics has proved itself as a powerful computer simulation method to study dynamics, conformational changes, and interactions of biological macromolecules and their complexes. In order to achieve the best Perfomance and efficiency, it is crucial to benchmark various hardware platforms for the simulations of realistic biomolecular systems with different size and timescale. Here, we compare Perfomance and scalability of a number of commercially available computing architectures using all-atom and coarse-grained molecular dynamics simulations of water and the Ndc80-microtubule protein complex in the GROMACS-2019.4 package. We report typical single-node Perfomance of various combinations of modern CPUs and GPUs, as well as multiple-node Perfomance of the “Lomonosov-2” supercomputer. These data can be used as the practical guidelines for choosing optimal hardware for molecular dynamics simulations.
Computer Design of Structure of Molecules of High-Energy Tetrazines. Calculation of Thermochemical Properties;Vadim M. Volokhov, Elena S. Amosova, Alexander V. Volokhov, Tatiana S. Zyubina, David B. Lempert, Leonid S. Yanovskiy, Ilya D. Fateev;https://doi.org/10.14529/jsfi200406;HPC, enthalpy of formation, quantum-chemical calculations, high-enthalpy compounds, IR spectra of gaseous molecules, combined CBS-4M method, combined G4 method;The article presents high-Perfomance calculations, using quantum chemical ab initio methods, of thermochemical characteristics of high-energy compounds: C2N6O4, C2N6O5, C2N6O6, C2H2N6O4, C3HN7O6, C3HN7O4F2, C4N10O12, C3HN6O4F, C4N10O8F4, C4N8O8F2. The IR absorption spectra, structural parameters and atomic displacements for the most intense vibrations, as well as the enthalpies of formation are provided in the article. The calculations were performed at the B3LYP/6-311+G(2d,p) level and using the combined methods CBS-4M and G4 within the Gaussian 09 application package (Linda paralellization). It is shown that the enthalpy of formation depends on the molecule structure.
Forecastability Measures that Describe the Complexity of a Site for Deep Learning Wind Predictions;Jaume Manero, Javier Béjar;https://doi.org/10.14529/jsfi210102;"wind forecasting, time series, wind time series, deep learning, CNN, convolutional
networks, forecastability";The application of deep learning to wind time series for multi-step prediction obtains good results at short horizons. The accuracy of a wind forecast is highly dependent on the specific structure of wind in the specific location, as many local features influence wind behaviour. The characterization of the complexity of a site for wind prediction is defined as forecastability or predictability and can be obtained from the inner structure of the meteorological time series observations from a site. We analyze the time series structure searching for properties that have a high correlation with the prediction result, properties that can create measures that have the potential to describe the forecastability of a site. The best measures will show a high correlation with the accuracy of the predictions. In this work, we analyze wind time series from 126,692 wind locations in the US, where we apply several deep learning methods first, and then we verify several forecastability descriptors with the accuracy deep learning results. We require High-Perfomance Computing (HPC) resources for this task as the deep learning algorithms have sensible resource requirements and are applied to a large set of data. The measures defined and explored in this work are based on several techniques that decompose or transform the wind time-series. By combining several of these measures, we can obtain better predictors of the site complexity, which will allow us to evaluate the future error of a prediction on this site. Forecastability measures can contribute to a wind site multi-dimensional description, becoming a valuable tool for wind resource analysts and wind forecasters.
Size & Shape Matters: The Need of HPC Benchmarks of High Resolution Image Training for Deep Learning;Ferran Parés Pont, Pedro Megias, Dario Garcia-Gasulla, Marta Garcia-Gasulla, Eduard Ayguadé, Jesús Labarta;https://doi.org/10.14529/jsfi210103;deep learning, convolutional neural networks, high-resolution images, variableshape images, HPC benchmarks;One of the purposes of HPC benchmarks is to identify limitations and bottlenecks in hardware. This functionality is particularly influential when assessing Perfomance on emerging tasks, the nature and requirements of which may not yet be fully understood. In this setting, a proper benchmark can steer the design of next generation hardware by properly identifying said requirements, and quicken the deployment of novel solutions. With the increasing popularity of deep learning workloads, benchmarks for this family of tasks have been gaining popularity. Particularly for image based tasks, which rely on the most well established family of deep learning models: Convolutional Neural Networks. Significantly, most benchmarks for CNN use low-resolution and fixed-shape (LR&FS) images. While this sort of inputs have been very successful for certain purposes, they are insufficient for some domains of special interest (e.g., medical image diagnosis or autonomous driving) where one requires higher resolutions and variable-shape (HR&VS) images to avoid loss of information and deformation. As of today, it is still unclear how does image resolution and shape variability affect the nature of the problem from a computational perspective. In this paper we assess the differences between training with LR&FS and HR&VS, as means to justify the importance of building benchmarks specific for the latter. Our results on three different HPC clusters show significant variations in time, resources and memory management, highlighting the differences between LR&FS and HR&VS image deep learning.
Computational Resource Consumption in Convolutional Neural Network Training – A Focus on Memory;Luis A. Torres, Carlos J. Barrios, Yves Denneulin;https://doi.org/10.14529/jsfi210104;HPC, deep learning, profiling, Perfomance characterization, memory consumption;Deep neural networks (DNNs) have grown in popularity in recent years thanks to the increase in computing power and the size and relevance of data sets. This has made it possible to build more complex models and include more areas of research and application. At the same time, the amount of data generated during the training process of these models puts great pressure on the capacity and bandwidth of the memory subsystem and, as a direct consequence, has become one of the biggest bottlenecks for the scalability of neural networks. Therefore, the optimizing of the workloads produced by DNNs in the memory subsystem requires a detailed understanding of access to the memory and the interactions between the processor, accelerator devices, and the system memory hierarchy. However, contrary to what would be expected, most DNN profilers work at a high level, so they only perform an analysis of the model and individual layers of the network leaving aside the complex interactions between all the hardware components involved in the training. This article shows the characterization performed using a convolutional neural network implemented in the two most popular frameworks: TensorFlow and Pytorch. Likewise, the behavior of the component interactions is discussed by varying the batch size for two sets of synthetic data and showing the results obtained by the profiler created for the study. Moreover, the results obtained when evaluating the AlexNet version on TensorFlow and its similarity in behavior when using a basic CNN are included.
The MareNostrum Experimental Exascale Platform (MEEP);Alexander Fell, Daniel J. Mazure, Teresa C. Garcia, Borja Perez, Xavier Teruel, Pete Wilson, John D. Davis;https://doi.org/10.14529/jsfi210105;HPC, HPC, accelerator, software stack, open source hardware;Nascent Open Source Instruction Set Architectures such as OpenPOWER or RISC-V, allow software/hardware co-designers to fully utilize the underlying hardware, modify it or extend it based on their needs. In this paper, we introduce the vision of the MareNostrum Experimental Exascale Platform (MEEP), an Open Source platform enabling software and hardware stack experimentation  targeting the HPC (HPC) ecosystem. MEEP is built with state-of-the-art FPGAs that support PCIe and High Bandwidth Memory (HBM), making it ideal to emulate chiplet-based HPC accelerators such as ACME, at the chip, package, and/or system level. MEEP provides an FPGA Shell containing standardized interfaces (I/O and memory), enabling an emulated accelerator to communicate with the hardware of the FPGA and ensures quick integration. The first demonstration of MEEP is mapping a new accelerator, the Accelerated Compute and Memory Engine (ACME), on to this digital laboratory. This enables exploration of this novel disaggregated architecture, which separates the computation from the memory operations, optimizing the accelerator for both dense (compute-bound) as well as sparse (memory-bandwidth bound) workloads. Dense workloads focus on the computational capabilities of the engine, while dedicated processors for memory accesses optimize non-unit stride and/or random memory accesses required by sparse workloads. MEEP is an open source digital laboratory that can provide a future environment for full-stack co-design and pre-silicon exploration.  MEEP invites software developers and hardware engineers to build the application, compiler, libraries and the hardware to solve future challenges in the HPC, AI, ML, and DL domains.
Micro-Workflows Data Stream Processing Model for Industrial Internet of Things;Ameer B. A. Alaasam, Gleb I. Radchenko, Andrey N. Tchernykh;https://doi.org/10.14529/jsfi210106;stream processing, fog computing, cloud computing, scientific workflow, microworkflow, IoT;The fog computing paradigm has become prominent in stream processing for IoT systems where cloud computing struggles from high latency challenges. It enables the deployment of computational resources between the edge and cloud layers and helps to resolve constraints, primarily due to the need to react in real-time to state changes, improve the locality of data storage, and overcome external communication channels’ limitations. There is an urgent need for tools and platforms to model, implement, manage, and monitor complex fog computing workflows. Traditional scientific workflow management systems (SWMSs) provide modularity and flexibility to design, execute, and monitor complex computational workflows used in smart industry applications. However, they are mainly focused on batch execution of jobs consisting of tightly coupled tasks. Integrating data streams into SWMSs of IoT systems is challenging. We proposed a microworkflow model to redesign the monolith architecture of workflow systems into a set of smaller and independent workflows that support stream processing. Micro-workflow is an independent data stream processing service that can be deployed on different layers of the fog computing environment. To validate the feasibility and practicability of the micro-workflow refactoring, we provide intensive experimental analysis evaluating the interval between sensor messages, the time interval required to create a message, between sending sensor message and receiving the message in SWMS, including data serialization, network latency, etc. We show that the proposed decoupling support of the independence of implementation, execution, development, maintenance, and cross-platform deployment, where each micro-workflow becomes a standalone computational unit, is a suitable mechanism for IoT stream processing.
Scalability prediction for fundamental Perfomance factors;Claudia Rosas, Judit Giménez, Jesús Labarta;https://doi.org/10.14529/jsfi140201;parallel efficiency, curve-fitting, exascale computing, analysis and prediction;"Inferring the expected Perfomance for parallel applications is getting harder than ever; applications need to be modeled for restricted or nonexistent systems and Perfomance analysts are required to identify and extrapolate their behavior using only the available resources. Prediction models can be based on detailed knowledge of the application algorithms or on blindly trying to extrapolate measurements from existing architectures and codes. This paper describes the work done to define an intermediate methodology where the combination of (a) the essential knowledge about fundamental factors in parallel codes, and (b) detailed analysis of the application behavior at low core counts on current platforms, guides the modeling efforts to estimate behavior at very large core counts. Our methodology integrates the use of several components like instrumentation package, visualization tools, simulators, analytical models and very high level information from the application running on systems in production to build a Perfomance model."
Predicting the Energy and Power Consumption of Strong and Weak Scaling HPC Applications;Hayk Shoukourian, Torsten Wilde, Axel Auweter, Arndt Bode;https://doi.org/10.14529/jsfi140202;"adaptive prediction, energy consumption, power consumption, energy capping,
power capping, AEPCP model, energy measurement, node scaling, EtS prediction, HPC";Keeping energy costs in budget and operating within available capacities of power distribution and cooling systems is becoming an important requirement for HPC (HPC) data centers. It is even more important when considering the estimated power requirements for Exascale computing. Power and energy capping are two of emerging techniques aimed towards controlling and efficient budgeting of power and energy consumption within the data center. Implementation of both techniques requires a knowledge of, potentially unknown, power and energy consumption data of the given parallel HPC applications for different numbers of compute servers (nodes).This paper introduces an Adaptive Energy and Power Consumption Prediction (AEPCP) model capable of predicting the power and energy consumption of parallel HPC applications for different number of compute nodes. The suggested model is application specific and describes the behavior of power and energy with respect to the number of utilized compute nodes, taking as an input the available history power/energy data of an application. It provides a generic solution that can be used for each application but it produces an application specific result. The AEPCP model allows for ahead of time power and energy consumption prediction and adapts with each additional execution of the application improving the associated prediction accuracy. The model does not require any application code instrumentation and does not introduce any application Perfomance degradation. Thus it is a high level application energy and power consumption prediction model. The validity and the applicability of the suggested AEPCP model is shown in this paper through the empirical results achieved using two application-benchmarks on the SuperMUC HPC system (the 10th fastest supercomputer in the world, according to Top500 November 2013 rankings) deployed at Leibniz Supercomputing Centre.
Data Compression for the Exascale Computing Era - Survey;Seung Woo Son, Zhengzhang Chen, William Hendrix, Ankit Agrawal, Wei-keng Liao, Alok Choudhary;https://doi.org/10.14529/jsfi140205;"Fault tolerance, checkpoint/restart, lossless/lossy compression, error bound, data
clustering";While periodic checkpointing has been an important mechanism for tolerating faults in HPC (HPC) systems, it is cost-prohibitive as the HPC system approaches exascale. Applying compression techniques is one common way to mitigate such burdens by reducing the data size, but they are often found to be less effective for scientific datasets. Traditional lossless compression techniques that look for repeated patterns are ineffective for scientific data in which high-precision data is used and hence common patterns are rare to find. In this paper, we present a comparison of several lossless and lossy data compression algorithms and discuss their methodology under the exascale environment. As data volume increases, we discover an increasing trend of new domain-driven algorithms that exploit the inherent characteristics exhibited in many scientific dataset, such as relatively small changes in data values from one simulation iteration to the next or among neighboring data. In particular, significant data reduction has been observed in lossy compression. This paper also discusses how the errors introduced by lossy compressions are controlled and the tradeoffs with the compression ratio.
Extreme Big Data (EBD): Next Generation Big Data Infrastructure Technologies Towards Yottabyte/Year;Satoshi Matsuoka, Hitoshi Sato, Osamu Tatebe, Michihiro Koibuchi, Ikki Fujiwara, Shuji Suzuki, Masanori Kakuta, Takashi Ishida, Yutaka Akiyama, Toyotaro Suzumura, Koji Ueno, Hiroki Kanezashi, Takemasa Miyoshi;https://doi.org/10.14529/jsfi140206;"Big Data, Supercomputing, Extreme Computing and Big Data Convergence, Data
Intensive Computing, Non-Volatile Memory";"Our claim is that so-called ``Big Data'' will evolve into a new era with proliferation of data from multiple sources such as massive numbers of sensors whose resolution is increasing exponentially, high-resolution simulations generating huge data results, as well as evolution of social infrastructures that allow for ``opening up of data silos'', i.e., data sources being abundant across the world instead of being confined within an institution, much as how scientific data are being handled in the modern era as a common asset openly accessible within and across disciplines. Such a situation would create the need for not only petabytes to zetabytes of capacity and beyond, but also for extreme scale computing power. Our new project, sponsored under the Japanese JST-CREST program is called ``Extreme Big Data"", and aims to achieve the {\it convergence of extreme supercomputing and big data} in order to cope with such explosion of data. The project consists of six teams, three of which deals with defining future EBD convergent SW/HW architecture and system, and the other three the EBD co-design applications that represent different facets of big data, in metagenomics, social simulation, and climate simulation with real-time data assimilation. Although the project is still early in its lifetime, started in Oct. 2013, we have already achieved several notable results, including becoming world #1 on the Green Graph 500, a benchmark to measure the power efficiency of graph processing that appear in typical big data scenarios."
Scalable parallel Perfomance measurement and analysis tools - state-of-the-art and future challenges;Bernd Mohr;https://doi.org/10.14529/jsfi140207;parallel programming, Perfomance tools, extreme scale computing;Current large-scale HPC systems consist of complex configurations with a huge number of potentially heterogeneous components. As the systems get larger, their behavior becomes more and more dynamic and unpredictable because of hard- and software re-configurations due to faultrecovery and power usage optimizations. Deep software hierarchies of large, complex system software and middleware components are required to operate such systems. Therefore, porting, adapting and tuning applications to today's complex systems is a complicated and time-consumingtask. Sophisticated integrated Perfomance measurement, analysis, and optimization capabilities are required to efficiently utilize such systems. This article will summarize the state-of-the-art of scalable and portable parallel Perfomance tools and the challenges these tools are facing on future extreme-scale and big data systems.
Accelerating Seismic Redatuming Using Tile Low-Rank Approximations on NEC SX-Aurora TSUBASA;Yuxi Hong, Hatem Ltaief, Matteo Ravasi, Laurent Gatineau, David Keyes;https://doi.org/10.14529/jsfi210201;"seismic redatuming, tile low-rank approximations, matrix-vector multiplication,
load balancing, high bandwidth memory, NEC SX-Aurora TSUBASA";With the aim of imaging subsurface discontinuities, seismic data recorded at the surface of the Earth must be numerically re-positioned inside the subsurface where reflections have originated, a process referred to as redatuming. The recently developed Marchenko method is able to handle full-wavefield data including multiple arrivals. A downside of this approach is that a multi-dimensional convolution operator must be repeatedly evaluated to solve an expensive inverse problem. As such an operator applies multiple dense matrix-vector multiplications (MVM), we identify and leverage the data sparsity structure for each frequency matrix and propose to accelerate the MVM step using tile low-rank (TLR) matrix approximations. We study the TLR impact on time-to-solution for the MVM using different accuracy thresholds whilst at the same time assessing the quality of the resulting subsurface seismic wavefields and show that TLR leads to a minimal degradation in terms of signal-to-noise ratio on a 3D synthetic dataset. We mitigate the load imbalance overhead and provide Perfomance evaluation on two distributed-memory systems. Our MPI+OpenMP TLR-MVM implementation reaches up to 3X Perfomance speedup against the dense MVM counterpart from NEC scientific library on 128 NEC SX-Aurora TSUBASA cards. Thanks to the second generation of high bandwidth memory technology, it further attains up to 67X Perfomance speedup compared to the dense MVM from Intel MKL when running on 128 dual-socket 20-core Intel Cascade Lake nodes with DDR4 memory. This corresponds to 110 TB/s of aggregated sustained bandwidth for our TLR-MVM implementation, without suffering deterioration in the quality of the reconstructed seismic wavefields.
Porting and Optimizing Molecular Docking onto the SX-Aurora TSUBASA Vector Computer;Leonardo Solis-Vasquez, Erich Focht, Andreas Koch;https://doi.org/10.14529/jsfi210202;application porting, Perfomance optimization, molecular docking, AutoDock, vector computing, SX-Aurora;In computer-aided drug design, the rapid identification of drugs is critical for combating diseases. A key method in this field is molecular docking, which aims to predict the interactions between two molecules. Molecular docking involves long simulations running compute-intensive algorithms, and thus, can profit a lot from hardware-based acceleration. In this work, we investigate the Perfomance efficiency of the SX-Aurora TSUBASA vector computer for such simulations. Specifically, we present our methodology for porting and optimizing AutoDock, a widely-used molecular docking program. Using a number of platform-specific code optimizations, we achieved executions on the SX-Aurora TSUBASA that are in average 3.6× faster than on modern 128-core CPU servers, and up to a certain extent, competitive to V100 and A100 GPUs. To the best of our knowledge, this is the first molecular docking implementation for the SX-Aurora TSUBASA.
First Experience of Accelerating a Field-Induced Chiral Transition Simulation Using the SX-Aurora TSUBASA;Shinji Yoshida, Arata Endo, Hirono Kaneyasu, Susumu Date;https://doi.org/10.14529/jsfi210203;SX-Aurora TSUBASA, OS Offload, VH Call, VEO, vectorization ratio;An analysis method based on the Ginzburg-Landau equation for the superconductivity is applied to the field-induced chiral transition simulation (FICT). However, the FICT is time consuming because it takes approximately 10 hours on a single SX-ACE vector processor. Moreover, the FICT must be repeatedly performed with parameters changed to understand the mechanism of the phenomenon. The newly emerged SX-Aurora TSUBASA, the successor of the SX-ACE processor, is expected to provide much higher Perfomance to the programs executed on the SX-ACE as is. However, the SX-Aurora TSUBASA processor has changed its architecture of compute nodes and gives users three different execution models, which leads to users’ concerns and questions in terms of how three execution models should be selectively used. In this paper, we report the first experience of using the SX-Aurora TSUBASA processor for the FICT. Specifically, we have developed three implementations of the FICT corresponding to the three execution models suggested by the SX-Aurora TSUBASA. For acceleration of the FICT, improvement of the vectorization ratio in the program execution and the efficient transfer of data to the general purpose processor as the vector host from the vector processor as the vector engine is explored. The evaluation in this paper shows how acceleration of the FICT is achieved as well as how much effort of users is required.
Evaluating the Perfomance of OpenMP Offloading on the NEC SX-Aurora TSUBASA Vector Engine;Tim Cramer, Boris Kosmynin, Simon Moll, Manoel Römmer, Erich Focht, Matthias S. Müller;https://doi.org/10.14529/jsfi210204;HPC, OpenMP, offloading, reverse offloading, vector computing, Perfomance;The NEC SX-Aurora TSUBASA vector engine (VE) follows the tradition of long vector processors for HPC (HPC). The technology combines the vector computing capabilities with the popularity of standard x86 architecture by integrating it as an accelerator. To decrease the burden of code porting for different accelerator types, the OpenMP specification is designed to be single parallel programming model for all of them. Besides the availability of compiler and runtime implementations, the functionality as well as the Perfomance is important for the usability and acceptance of this paradigm. In this work, we present LLVM-based solutions for OpenMP target device offloading from the host to the vector engine and vice versa (reverse offloading). Therefore, we use our source-to-source transformation tool sotoc as well as the native LLVM-VE code path. We assess the functionality and present the first Perfomance numbers of real-world HPC kernels. We discuss the advantages and disadvantage of the different approaches and show that our implementation is competitive to other GPU OpenMP runtime implementations. Our work gives scientific programmers new opportunities and flexibilities for the development of scalable OpenMP offloading applications for SX-Aurora TSUBASA.
Perfomance and Power Analysis of a Vector Computing System;Kazuhiko Komatsu, Akito Onodera, Erich Focht, Soya Fujimoto, Yoko Isobe, Shintaro Momose, Masayuki Sato, Hiroaki Kobayashi;https://doi.org/10.14529/jsfi210205;"SX-Aurora TSUBASA, optimization, vector computing, power efficiency, Himeno
benchmark, HPCG";The Perfomance of recent computing systems has drastically improved due to the increase in the number of cores. However, this approach is reaching the limitation due to the power constraints of facilities. Instead, this paper focuses on a vector processing with long vector length that has a potential to realize high Perfomance and high power efficiency. This paper discusses the potential through the optimization of two benchmarks, the Himeno and HPCG benchmarks, for the latest vector computing system SX-Aurora TSUBASA. The architecture of SX-Aurora TSUBASA owes the high efficiency to making good of its long vector length. Considering these characteristics, various levels of optimizations required for a large-scale vector computing system are examined such as vectorization, loop unrolling, use of cache, domain decomposition, process mapping, and problem size tuning. The evaluation and analysis suggest that the optimizations improve the sustained Perfomance, power efficiency, and scalability of both benchmarks. Therefore, it is clarified that the SX-Aurora TSUBASA architecture can achieve higher power efficiency due to its high sustained memory bandwidth paired with the long vector computing.
Distributed Graph Algorithms for Multiple Vector Engines of NEC SX-Aurora TSUBASA Systems;Ilya V. Afanasyev, Vadim V. Voevodin, Kazuhiko Komatsu, Hiroaki Kobayashi;https://doi.org/10.14529/jsfi210206;vector computers, graph algorithms, graph framework, VGL, optimisation;This paper describes the world-first attempt to develop distributed graph algorithm implementations, aimed for modern NEC SX-Aurora TSUBASA vector systems. Such systems are equipped with up to eight powerful vector engines, which are capable to significantly accelerate graph processsing and simultaneously increase the scale of processed input graphs. This paper describes distributed implementations of three widely-used graph algorithms: Page Rank (PR), Bellman-Ford Single Source Shortest Paths (further referred as SSSP) and Hyperlink-Induced Topic Search (HITS), evaluating their Perfomance and scalability on Aurora 8 system. In this paper we describe graph partitioning strategies, communication strategies, programming models and single-VE optimizations used in these implementations. The developed implementations achieve 40, 6.6 and 1.3 GTEPS Perfomance on PR, SSSP and HITS algorithm on 8 vector engines, at the same time achieving up to 1.5x, 2x and 2.5x acceleration on 2, 4 and 8 vector engines of Aurora 8 systems. Finally, this paper describes an approach to incorporate distributed graph processing support into our previously developed Vector Graph Library (VGL) framework – a novel framework for graph analytics on NEC SX-Aurora TSUBASA architecture.
Optimizing Load Balance in a Parallel CFD Code for a Large-scale Turbine Simulation on a Vector Supercomputer;Osamu Watanabe, Kazuhiko Komatsu, Masayuki Sato, Hiroaki Kobayashi;https://doi.org/10.14529/jsfi210207;turbine simulation code, MPI, OpenMP, hybrid parallelization, vector supercomputer, load balance;A turbine for power generation is one of the essential infrastructures in our society. A turbine's failure causes severe social and economic impacts on our everyday life. Therefore, it is necessary to foresee such failures in advance. However, it is not easy to expect these failures from a real turbine. Hence, it is required to simulate various events occurring in the turbine by numerical simulations of the turbine. A multiphysics CFD code, ‘‘Numerical Turbine,’' has been developed on vector supercomputer systems for large-scale simulations of unsteady wet steam flows inside a turbine. To solve this problem, the Numerical Turbine code is a block structure code using MPI parallelization, and the calculation space consists of grid blocks of different sizes. Therefore, load imbalance occurs when executing the code in MPI parallelization. This paper creates an estimation model that finds the calculation time from each grid block's calculation amount and calculation Perfomance. It proposes an OpenMP parallelization method for the load balance of MPI applications. This proposed method reduces the load imbalance by considering the vector Perfomance according to the calculation amount based on the model. Moreover, this proposed method recognizes the need to reduce the load imbalance without pre-execution. The Perfomance evaluation shows that the proposed method improves the load balance from 24.4 % to 9.3 %.
Evaluating Perfomance of Mixed Precision Linear Solvers with Iterative Refinement;Boris I. Krasnopolsky, Alexey V. Medvedev;https://doi.org/10.14529/jsfi210301;systems of linear algebraic equations, elliptic equations, algebraic multigrid methods, iterative refinement, mixed precision calculations;The solution of systems of linear algebraic equations is among the time-consuming problems when performing the numerical simulations. One of the possible ways of improving the corresponding solver Perfomance is the use of reduced precision calculations, which, however, may affect the accuracy of the obtained solution. The current paper analyzes the potential of using the mixed precision iterative refinement procedure to solve the systems of equations occurring as a result of the discretization of elliptic differential equations. The paper compares several inner solver stopping criteria and proposes the one allowing to eliminate the residual deviation and minimize the number of extra iterations. The presented numerical calculation results demonstrate the efficiency of the adopted algorithm and show about the decrease in the solution time by a factor of 1.5 for the turbulent flow simulations when using the iterative refinement procedure to solve the corresponding pressure Poisson equation.
Fog Computing State of the Art: Concept and Classification of Platforms to Support Distributed Computing Systems;Alexandra A. Kirsanova, Gleb I. Radchenko, Andrey N. Tchernykh;https://doi.org/10.14529/jsfi210302;"big data processing, fog computing, scheduling, cloud computing, edge computing,
Internet of Things";As the Internet of Things (IoT) becomes a part of our daily life, there is a rapid growth in the connected devices. A well-established approach based on cloud computing technologies cannot provide the necessary quality of service in such an environment, particularly in terms of reducing data latency. Today, fog computing technology is seen as a novel approach for processing large amounts of critical and time-sensitive data. This article reviews cloud computing technology and analyzes the prerequisites for the evolution of this approach and the emergence of the concept of fog computing. As part of an overview of the critical features of fog computing, we analyze the frequent confusion of the concepts of fog and edge computing. We provide an overview of fog computing technologies: virtualization, containerization, orchestration, scalability, parallel computing environments, as well as systematic analysis of the most popular platforms that support fog computing. As a result of the analysis, we offer two approaches to classification of the fog computing platforms: by the principle of openness/closure of components and by the three-level classification based on the provided platform functionality (Deploy-, Platform- and Ecosystem as a Service).
VaLiPro: Linear Programming Validator for Cluster Computing Systems;Leonid B. Sokolinsky, Irina M. Sokolinskaya;https://doi.org/10.14529/jsfi210303;linear programming, solution validator, VaLiPro, parallel algorithm, cluster computing system, BSF-skeleton;The article presents and evaluates a scalable algorithm for validating solutions to linear programming problems on cluster computing systems. The main idea of the method is to generate a regular set of points (validation set) on a small-radius hypersphere centered at the solution point submitted to validation. The objective function is computed at each point of the validation that belongs to the feasible region. If all the values are less than or equal to the value of the objective function at the point that is to be validated, then this point is the correct solution. The parallel implementation of the VaLiPro algorithm is written in C++ through the parallel BSF-skeleton, which encapsulates all aspects related to the MPI-based parallelization of the program. We provide the results of large-scale computational experiments on a cluster computing system to study the scalability of the VaLiPro algorithm.
A Review of Supercomputer Perfomance Monitoring Systems;Konstantin S. Stefanov, Sucheta Pawar, Ashish Ranjan, Sanjay Wandhekar, Vladimir V. Voevodin;https://doi.org/10.14529/jsfi210304;monitoring, supercomputers, Perfomance monitoring, review;HPC is now one of the emerging fields in computer science and its applications. Top HPC facilities, supercomputers, offer great opportunities in modeling diverse processes thus allowing to create more and greater products without full-scale experiments. Current supercomputers and applications for them are very complex and thus are hard to use efficiently. Perfomance monitoring systems are the tools that help to understand the efficiency of supercomputing applications and overall supercomputer functioning. These systems collect data on what happens on a supercomputer (Perfomance data, Perfomance metrics) and present them in a way allowing to make conclusions about Perfomance issues in programs running on the supercomputer. In this paper we give an overview of existing Perfomance monitoring systems designed for or used on supercomputers. We give a comparison of Perfomance monitoring systems found in literature, describe problems emerging in monitoring large scale HPC systems, and outline our vision on future direction of HPC monitoring systems development.
Administration, Monitoring and Analysis of Supercomputers in Russia: a Survey of 10 HPC Centers;Vadim V. Voevodin, Roman A. Chulkevich, Pavel S. Kostenetskiy, Vyacheslav I. Kozyrev, Anton K. Maliutin, Dmitry A. Nikitenko, Sergey G. Rykovanov, Artemiy B. Shamsutdinov, Yurii N. Shkandybin, Sergey A. Zhumatiy;https://doi.org/10.14529/jsfi210305;"supercomputer, HPC, administration, survey, monitoring,
Perfomance";Supercomputer technologies are in demand for solving many important and computationallyintensive tasks in various fields of science and technology. Therefore, it is not surprising that there are several dozen supercomputer centers only in Russia. However, the goals of creating such centers, as well as the range of tasks solved in them, can vary greatly, therefore the structure of supercomputers and the policies for their usage can significantly differ. This leads to the fact that many supercomputer centers live an isolated life – the administrators of such centers tend to solve administration-related tasks on their own, despite the fact that solutions for many similar tasks have already been developed and applied in other centers. This can happen due to different reasons, but in any case, this situation could and should be improved. To do this, it is worth establishing a closer connection between supercomputer centers, which will allow more actively exchanging experience or jointly developing desired system software. In order to understand the current situation in this area, a survey was conducted of representatives among 10 large supercomputer centers in Russia, and its results are presented in this paper. Two relevant topics about using monitoring data in practice and real-life examples of supercomputer functioning improvement are also discussed here in more detail. Their vision on these topics is provided by the system administrators of HSE University, Skoltech and Moscow State University.
Efficient Implementation of Liquid Crystal Simulation Software on Modern HPC Platforms;Ilya V. Afanasyev, Dmitry I. Lichmanov, Vladimir Yu. Rudyak, Vadim V. Voevodin;https://doi.org/10.14529/jsfi210306;NVIDIA GPU, NEC SX-Aurora TSUBASA, liquid crystals, HPC, co-design, Perfomance optimization, Monte Carlo, cubic lattice;In this paper we demonstrate the process of efficient porting a software package for Markov chain Monte Carlo (MCMC) simulations on a finite cubic lattice on multiple modern architectures: Pascal, Volta and Turing NVIDIA GPUs, NEC SX-Aurora TSUBASA vector engines and Intel Xeon Gold processors. In the studied software, MCMC methodology is used for simulations of liquid crystal structures, but it can be as well employed in a wide range of problems of mathematical physics and numerical methods. The main goals of this work are to determine the best software optimization strategy for this class of algorithms and to examine the speed and the efficiency of such simulations on modern HPC platforms. We evaluate the effects of various optimizations, such as using more suitable memory access patterns, multitasking for efficient utilization of massive parallelism on the target architectures, improved cache hit-rates, parallel workload balancing, etc. We perform a detailed Perfomance analysis for each target platform using software tools such as nvprof, Ftrace and VTune. On this basis, we evaluate and compare the efficiency of the developed computational kernels on different platforms and subsequently rank these platforms by their Perfomance. The results show that NVIDIA GPU and NEC SX-Aurora TSUBASA platforms, although at first glance seem very different, require similar optimization approaches in many cases due to similarities in data processing principles.
Technology for Supercomputer Simulation of Turbulent Flows in the Good New Days of Exascale Computing;Andrey V. Gorobets, Alexey P. Duben;https://doi.org/10.14529/jsfi210401;"computational fluid dynamics, turbulent flows, scale-resolving simulation, hybrid
RANS-LES approach, CPU+GPU, MPI+OpenMP+OpenCL";A technology for scale-resolving simulations of turbulent flows in the problems of aerodynamics and aeroacoustics is presented. It is based on the higher accuracy numerical schemes on unstructured mixed-element meshes and latest non-zonal hybrid approaches combining Reynoldsaveraged Navier – Stokes (RANS) and Large eddy simulation (LES) methods for turbulence modeling. It targets a wide range of HPC (HPC) systems, from a compute server or small cluster to an exascale supercomputer. The advantages of the key components of the technology are summarized. These key components are a hybrid RANS-LES turbulence modeling method, a numerical scheme for discretization in space, a parallel algorithm, and a portable software implementation for modern hybrid systems with extra massive parallelism. Examples of our simulations are given and parallel Perfomance on various HPC systems is presented.
Improving the Computational Efficiency of the Global SL-AV Numerical Weather Prediction Model;Mikhail A. Tolstykh, Rostislav Yu. Fadeev, Vladimir V. Shashkin, Gordey S. Goyman;https://doi.org/10.14529/jsfi210402;"numerical weather prediction, global atmosphere model, computational efficiency,
I/O optimization";The recent works on improving the efficiency of the Russian SL-AV global numerical weather prediction model both for medium- and long-range forecasts are described. The algorithmic improvements of SL-AV dynamical core, implementation of parallel I/O and several code optimizations are presented. We investigate the impact of single precision computations in some parts of the code on present climate simulations. As a result of efforts described in this article, we are now able to compute a 24-hour forecast for the model version having about 10 km horizontal resolution and 104 vertical levels in 13 min using 2916 processor cores of Cray XC40 system. This timing allows multiple experiments for tuning this new model and fits the requirements for operational weather forecast. The single long-range forecast with low-resolution SL-AV version now takes just 89 minutes instead of 111. We have also verified that the partial utilization of single precision computations produces approximately the same model climate as the previous version with fully double precision computations.
The Influence of Autumn Eurasian Snow Cover on the Atmospheric Dynamics Anomalies during the Next Winter in INMCM5 Model Data;Maria A. Tarasevich, Evgeny M. Volodin;https://doi.org/10.14529/jsfi210403;"climate model, seasonal hindcasts, North Atlantic Oscillation, Eurasian snow
cover, teleconnection";The influence of autumn Eurasian snow cover on the atmospheric dynamics anomalies during the following winter is studied based on the INM RAS climate model data. The North Atlantic Oscillation is the leading pattern that causes the weather and climate variability in the Northern hemisphere. We evaluate the up-to-date model version (INMCM5) ability of the autumn Eurasian snow – winter NAO teleconnection simulation on different timescales. Maximum covariance analysis (MCA) is used to find winter atmospheric signals that are significantly correlated with autumn snow cover anomalies. Using MCA we conclude that Autumn Eurasian snow – winter NAO teleconnection is present in INMCM5 experiments on pre-industrial and present-day climate simulation. However, this method fails to show this phenomenon in experiments on a seasonal timescale. We conduct additional experiments on a seasonal timescale to assess the sensitivity of North Atlantic Oscillation index predictability to initial snow cover perturbations. These experiments demonstrate the absence of direct autumn Eurasian snow impact on the NAO index.
Representation of Spatial Data Processing Pipelines Using Relational Database;Igor G. Okladnikov;https://doi.org/10.14529/jsfi210404;spatial data, information systems, databases, workflow, directed multigraph, processing pipeline, climate research;A methodology for representation of spatial data processing pipelines using relational database within the framework of the computing backend of the online information-analytical system “Climate” (http://climate.scert.ru) is proposed. Each pipeline is represented by a sequence of instructions for the computing backend describing how to run data processing modules and pass datasets between them (from the output of one module to the input of another one), including raw data and final computational results obtained in graphical or binary formats. Using relational database for storing descriptions of processing pipelines used in the “Climate” system provides flexibility and efficiency while adding and developing spatial data processing modules. It also provides computing pipelines scaling for further implementation for multiprocessor systems.
Direct Numerical Simulation of Stratified Turbulent Flows and Passive Tracer Transport on HPC Systems: Comparison of CPU Architectures;Evgeny V. Mortikov, Andrey V. Debolskiy;https://doi.org/10.14529/jsfi210405;turbulence, direct numerical simulation, ARM, supercomputing;In this paper we assess the influence of CPU architectures commonly used in HPC systems on the efficiency of the implementation of algorithms used for direct numerical simulation (DNS) of turbulent flows. We consider a stably stratified turbulent plane Couette flow as a benchmark problem supplemented with the additional transport of passive substances. The comparison includes the Intel Xeon, AMD Rome x86 CPU architecture processors and the Huawei Kunpeng ARM CPU processor. We discuss the role of memory-oriented optimizations on the efficiency of tracer transport implementation on each platform.
Scalability as a Key Property of Mapping Computational Tasks to Supercomputer Architecture;Alexander S. Antonov;https://doi.org/10.14529/jsfi210406;scalability, supercomputer, AlgoWiki, parallel structure, problems, methods, algorithms, implementations, computing platforms;When solving complex computational problems on modern supercomputers, an increasingly important role is played by the scalability property, which characterizes the ability of applications to adapt to various degrees of parallelism of computing systems.
High-Perfomance Shallow Water Model for Use on Massively Parallel and Heterogeneous Computing Systems;Andrey V. Chaplygin, Anatoly V. Gusev, Nikolay A. Diansky;https://doi.org/10.14529/jsfi210407;"shallow water, supercomputer modeling, heterogeneous computing systems, MPI,
OpenMP, CUDA";This paper presents the shallow water model, formulated from the ocean general circulation sigma model INMOM (Institute of Numerical Mathematics Ocean Model). The shallow water model is based on software architecture, which separates the physics-related code from parallel implementation features, thereby simplifying the model’s support and development. As an improvement of the two-dimensional domain decomposition method, we present the blocked-based decomposition proposing load-balanced and cache-friendly calculations on CPUs. We propose various hybrid parallel programming patterns in the shallow water model for effective calculation on massively parallel and heterogeneous computing systems and evaluate their scaling Perfomances on the Lomonosov-2 supercomputer. We demonstrate that Perfomance per a single grid point on GPUs dramatically decreases for small grid sizes starting from 219 points per node, while Perfomance on CPUs scales up to 217 well. Although, calculations on GPUs outperform calculations on CPUs by a factor of 4.7 at 30 nodes using 60 GPUs and 360 CPU cores at 6100 x 4460 grid size. We demonstrate that overlapping kernel execution with data transfers on GPUs increases Perfomance by 28%. Furthermore, we demonstrate the advantage of using the load-balancing method in the Azov Sea model on CPUs and GPUs.
PLUMED Plugin Integration into High Perfomance Pmemd Program for Enhanced Molecular Dynamics Simulations;Viktor V. Drobot, Evgeny M. Kirilin, Kirill E. Kopylov, Vytas K. Švedas;https://doi.org/10.14529/jsfi210408;"high Perfomance pmemd program, enhanced molecular dynamics simulations,
PLUMED plugin integration, metadynamics, CUDA, GPU";Metadynamics as an enhanced sampling procedure of molecular dynamics simulations is an effective tool to simulate complex molecular motions, conformations and reactivity, including enzyme plasticity and catalysis. The classic non-enhanced molecular simulation tools have reached unprecedently high Perfomance utilizing GPU units, however their implementation for enhanced sampling are still on demand. The widespread AMBER (molecular dynamics package) + PLUMED (metadynamics plugin) still does not take advantage of GPU computing or the CPU utilization optimization included in the AMBER pmemd program. In this work we have developed PLUMED binding to pmemd program resolving Perfomance issues within hybrid molecular dynamics/metadynamics runs. Preliminary checks and test results of the model system have validated this implementation.
Turbulent Length Scale for Multilayer RANS Model of Urban Canopy and Its Evaluation Based on Large-Eddy Simulations;Andrey V. Glazunov, Andrey V. Debolskiy, Evgeny V. Mortikov;https://doi.org/10.14529/jsfi210409;"atmospheric boundary layer, numerical simulation of turbulence, urban canopy,
scalar turbulent transport";Large-Eddy Simulation (LES) numerical experiments of neutrally-stratified turbulent flow over an urban-type surface and passive scalar transport by this flow are performed. A simple parameterization of the turbulent length scale containing only one empirical constant is proposed. Multilayer Reynolds-Averaged Navier-Stokes (RANS) model of turbulent flow and turbulent scalar diffusion is constructed. The results of the RANS model are compared with the LES experiments. It is shown that the proposed approach allows predicting the average flow velocity and the scalar concentration inside and above the urban canopy.
4D Technology of Variational Data Assimilation for Sea Dynamics Problems;Victor P. Shutyaev, Valery I. Agoshkov, Vladimir B. Zalesny, Eugene I. Parmuzin, Natalia B. Zakharova;https://doi.org/10.14529/jsfi220101;"sea dynamics modeling, variational data assimilation, observations, sea surface
temperature";The technology aimed at HPC is presented for modeling the sea dynamics problems based on 4D variational data assimilation technique developed at the Marchuk Institute of Numerical Mathematics, Russian Academy of Sciences (INM RAS). The technology is based on the multicomponent splitting method for the mathematical model of sea dynamics and the minimization of cost functionals related to the observation data by solving an optimality system that involves the adjoint equations with observation data and observation error covariances. Efficient algorithms for solving the variational data assimilation problems are presented based on modern iterative processes with a special choice of iterative parameters. The technology is illustrated for the Baltic Sea dynamics model with variational data assimilation to restore the initial states and the heat fluxes on the sea surface.
A Supercomputer-Based Modeling System for Short-Term Prediction of Urban Surface Air Quality;Alexander V. Starchenko, Evgeniy A. Danilkin, Sergei A. Prokhanov, Lubov I. Kizhner, Elena A. Shelmina;https://doi.org/10.14529/jsfi220102;"parallel computations, numerical weather prediction, mesoscale models, urban air
quality, MPI";This paper proposes a mathematical model and an effective supercomputer-based numerical method for short-term prediction of extreme meteorological conditions and atmospheric air quality over limited stretches of land encompassing large population centers. The mathematical model includes a pollutant transport model with a reduced chemical mechanism and a non-hydrostatic mesoscale meteorological model with a modern moisture microphysics parametrization scheme. The numerical method relies on the use of the finite volume method and semi-implicit difference schemes of the second order of approximation, which are solved using the TDMA method with a linear dependence of the number of arithmetic operations on the size of the grid. This property of the numerical method ensures high efficiency when parallelized: not less than 70% when using up to 256 computing cores with a horizontal grid size of 0.5–1.0 km. Development of parallel programs was carried out using the Message Passing Interface parallel programming technology, two-dimensional decomposition of the grid area along horizontal (west to east and south to north) directions, and introduction of additional fictitious grid nodes along the perimeter of the decomposition subdomains.
River Routing in the INM RAS-MSU Land Surface Model: Numerical Scheme and Parallel Implementation on Hybrid Supercomputers;Victor M. Stepanenko;https://doi.org/10.14529/jsfi220103;land surface model, soil, river network, MPI, OpenMP;The land surface model (LSM) is a necessary compartment of any numerical weather forecast system or the Earth system model. This paper presents a new version of the INM RAS-MSU land surface model where the river hydrodynamic and thermodynamic scheme is embedded into the parallel execution framework using MPI and OpenMP. Numerical experiments have been performed for the East European domain with resolution 0.5°× 0.5°. The soil model parallel efficiency at 1–144 MPI cores was 0.52–0.79 and limited by the presence of ocean area, and by imbalance of computational load between soil columns. The acceleration of the river model at MPI level was defined by the size of the largest river basin in the domain. At the OpenMP level, the potential for acceleration of large river basin simulation is shown to be close to number of threads used, based on fractal properties of the river networks. This acceleration was hindered in our numerical experiments by the reduced river orders at the coarse land surface model resolution, so that the optimal speedup for the Volga river basin was 2.5–3 times attained at 4–6 threads. This Perfomance is projected to improve with refinement of the LSM spatial resolution.
Machine Learning Approaches to Extreme Weather Events Forecast in Urban Areas: Challenges and Initial Results;Fabio Porto, Mariza Ferro, Eduardo Ogasawara, Thiago  Moeda, Claudio Daniel Tenorio de Barros, Anderson Chaves Silva, Rocio Zorrilla, Rafael  Silva Pereira, Rafaela  Nascimento Castro, João Victor Silva, Rebecca Salles, Augusto José Fonseca, Juliana Hermsdorff, Marcelo Magalhães, Vitor Sá, Antônio Adolfo Simões, Carlos Cardoso, Eduardo  Bezerra;https://doi.org/10.14529/jsfi220104;machine learning, rainfall forecast, extreme events;Weather forecast services in urban areas face an increasingly hard task of alerting the population on extreme weather events. The hardness of the problem is due to the dynamics of the phenomenon, which challenges numerical weather prediction models and opens an opportunity for Machine Learning (ML) based models that may learn complex mappings between input-output from data. In this paper, we present an ongoing research project which aims at building ML predictive models for extreme precipitation forecast in urban areas, in particular in the Rio de Janeiro City. We present the techniques that we have been developing to improve rainfall prediction and extreme rainfall forecast, along with some initial experimental results. Finally, we discuss some challenges that remain to be tackled in this project.
Data Assimilation by Neural Network for Ocean Circulation: Parallel Implementation;Haroldo F. Campos Velho, Helaine C. M. Furtado, Sabrina B. M. Sambatti, Carla Barros Osthoff Ferreira de Barros, Maria E. S. Welter, Roberto P. Souto, Diego  Carvalho, Douglas O. Cardoso;https://doi.org/10.14529/jsfi220105;data assimilation, artificial neural network, shallow water equations, parallel processing;Data assimilation (DA) is an essential issue for operational prediction centers, where a computer code is applied to simulate physical phenomena by solving differential equations. The procedure to determine the best initial condition combining data from observation and previous forecasting (background) is carried out by a data assimilation method. The Kalman filter (KF) is a technique for data assimilation, but it is computationally expensive. An approach to reduce the computational effort for DA is to emulate the KF by a neural network. The multi-layer perceptron neural network (MLP-NN) is employed to emulate the Kalman in a 2D ocean circulation model, and algorithmic complexity to KF and NN is presented. A shallow-water system models the ocean dynamics. Synthetic measurements are used for evaluating the MLP-NN for the data assimilation process. Here, a parallel version for the DA procedure by the neural network is described and tested, showing the Perfomance improvement for a parallel version of the NN-DA.
Multistage Iterative Method to Tackle Inverse Problems of Wave Tomography;Alexander V. Goncharsky, Sergey Y. Romanov, Sergey Y. Seryozhnikov;https://doi.org/10.14529/jsfi220106;"ultrasound tomography, coefficient inverse problem, gradient method, numerical
simulation";This paper is concerned with developing the methods for solving inverse problems of lowfrequency ultrasound tomography under scalar wave models using supercomputer technologies. Unlike X-ray tomography, the inverse problem considered is posed as a problem of minimizing a non-convex residual functional. The multistage iterative method (MSM) is proposed as a method for obtaining an approximate solution to the inverse problem. Convergence of the method to the exact solution is achieved via the use of low-frequency sounding signals at the initial stages of the iterative method. The method is illustrated on model problems focused on ultrasound tomographic diagnostics of soft tissues in medicine. Finite-difference time-domain method is used to solve the wave equation, which accounts for most of the computational complexity of the method. The multistage method reduces the computation time, since the initial stages use low-resolution finite difference grids. The effectiveness of the MSM method is investigated on GPU and SIMD-capable CPU computing platforms. Numerical simulations showed that modern processors equipped with AVX-512 FPUs are capable of solving small-scale problems of wave tomography. For large-scale tasks, GPUs equipped with fast on-board memory are preferred. The numerical algorithm is data-parallel and well-suited for GPU architecture. The proposed method can be used in medical imaging and nondestructive testing applications.
Computational Characterization of N-acetylaspartylglutamate Synthetase: From the Protein Primary Sequence to Plausible Catalytic Mechanism;Igor V. Polyakov, Artem E. Kniga, Alexander V. Nemukhin;https://doi.org/10.14529/jsfi220201;"molecular dynamics, quantum mechanics/molecular mechanics, QM/MM MD,
GPU-accelerated algorithms, N-acetylaspartylglutamate synthetase, enzyme-substrate complexes,
reaction intermediates";The methods of supercomputer molecular modeling are applied to characterize structure and dynamics of one of the key human brain enzymes, N-acetylaspartylglutamate synthetase. The three-dimensional all-atom models of the enzyme with the reactants in the active site are constructed in several steps, starting from pilot protein structure in the apo-form obtained with the AlphaFold2 from the protein primary sequence. Deposition of reactant molecules into the protein cavity, construction of the reaction intermediate and relaxation of the complex are carried out with the help of large-scale classical molecular dynamics calculations. On the top of the construct, molecular dynamics simulations with the quantum mechanics/molecular mechanics interaction potentials are performed for the most promising conformations of the model system. Analysis of the latter allows us to propose plausible catalytic mechanisms of chemical reactions in the enzyme active site. The applied computational strategy opens the way towards ab initio enzymology using modern supercomputer simulations.
Predicting the Activity of Boronate Inhibitors Against Metallo-β-lactamase Enzymes;Elena O. Levina, Maria G. Khrenova, Vladimir G. Tsirelson;https://doi.org/10.14529/jsfi220202;"metallo- β-lactamase, boronate inhibitors, MD, QM/MM MD, quantum theory of
atoms in molecules (QTAIM), GPU-accelerated algorithms";Potency of boronate inhibitors against metallo-β-lactamases (MβLs) has been found to be dependent on the electrophilicity of the boron atom. It forms a covalent bond with the oxygen atom of the catalytic OH− ion in the active site of the enzyme. The ability of the boronate inhibitor to influence the protein conformation also affects the binding potency. Molecular dynamics (MD) simulations of cyclic and non-cyclic boronate complexes with NDM-1 MβL show their higher impact on the inhibitor efficiency compared with the electrophilicity of the boron atom. Therefore, we focus on the hardware impact on the computational speedup of the GPU-accelerated MD. Using this data, we propose a comprehensive protocol for in silico prediction of the activity of boronate molecules against MβL enzymes, which includes MD simulations, combined quantum mechanics / molecular mechanics (QM/MM) computations and molecular dynamics simulations with the QM/MM potentials (QM/MM MD).
Predicting Binding Free Energies for DPS Protein-DNA Complexes and Crystals Using Molecular Dynamics;Eduard V. Tereshkin, Ksenia B. Tereshkina, Yurii F. Krupyanskii;https://doi.org/10.14529/jsfi220203;"molecular dynamics, slow-growth thermodynamic integration method, DPS protein,
DNA stabilization, DNA-DPS binding free energy";The interaction between deoxyribonucleic acid (DNA) and deoxyribonucleic acid-binding protein from starved cells (DPS) in bacterial cells leads to intracellular crystallization of the genetic material of bacteria, which contributes to the survival of bacteria under stress factors, including antibacterial agents. Molecular modeling can help explain the molecular mechanisms of DNA binding to this protein. In this paper, we report a supercomputer simulation of the molecular dynamics of several types DNA-DPS complexes and crystals ranging from DPS+DNA dimer to DNA in periodic crystal channels of Escherichia coli DPS protein using a coarse-grained Martini force field. By modeling DNA of 24 base pairs, comparable in size to the diameter of the DPS protein, we use the slow-growth thermodynamic integration method to find binding protein-DNA free energy and discuss the contribution of ions and the length of trajectories sufficient for this type of simulations. The results obtained are important for further research in the field of simulation of biological DNA-protein crystals and the study of the molecular mechanisms of DNA interaction with the DPS protein.
Computational Modeling of the Interaction of Molecular Oxygen with the Flavin-dependent Enzyme RutA;Igor V. Polyakov, Tatiana M. Domratcheva, Anna M. Kulakova, Alexander V. Nemukhin, Bella L. Grigorenko;https://doi.org/10.14529/jsfi220204;computational modeling, molecular dynamics, quantum mechanics/molecular mechanics, protein-oxygen interaction, flavin-dependent enzymes;Supercomputer molecular modeling methods are applied to characterize structure and dynamics of the flavin-dependent enzyme RutA in the complex with molecular oxygen. Following construction of a model protein system, molecular dynamics (MD) simulations were carried out using either classical force field interaction potentials or the quantum mechanics/molecular mechanics (QM/MM) potentials. Several oxygen-binding pockets in the protein cavities were located in these simulations. The QM/MM-based MD calculations rely on the interface between the quantum chemistry package TeraChem and the MD package NAMD. The results show a stable localization of the oxygen molecule in the enzyme active site. Static QM/MM calculations carried out with two different packages, NWChem and TURBOMOLE, allowed us to establish the structure of the RutA-O2 complex. Biochemical perspectives of the hallmark reaction of incorporating oxygen into organic compounds emerged from these simulations are formulated.
Analysis of Ion Atmosphere Around Nucleosomes Using Supercomputer MD Simulations;Nikita A. Kosarim, Grigoriy A. Armeev, Mikhail P. Kirpichnikov, Alexey K. Shaytan;https://doi.org/10.14529/jsfi220205;"molecular modeling, molecular dynamics simulations, nucleosomes, protein-DNA
interactions, monovalent cations, sodium, potassium";The nucleosome is the basic unit of eukaryotic DNA compaction. It consists of about 147 base pairs wrapped around an octamer of histone proteins. Nucleosomal dynamics provides the availability of packaged DNA for various factors that carry out the vital processes associated with chromatin. It is not completely known how the structure and dynamics of the nucleosome depends on the ionic environment. The current researches do not give an unambiguous answer and often contradict each other. In this paper, we demonstrate supercomputer molecular dynamics simulations of nucleosome models surrounded by monovalent sodium and potassium cations. Analyzing the trajectories, we have shown the details of the distribution of sodium and potassium ions around the linker DNA, nucleosomal DNA at the sites of nucleosomal opening, and histone residues involved in the process of nucleosomal breathing. We have demonstrated the mobility of DNA linkers and the process of nucleosomal unwrapping in various ionic environments, and also assessed the probable mechanisms of the dependence of nucleosome unwrapping on the type of ions in the system. Our study is intended to emphasize the importance of understanding the role of the ionic environment in the functioning of chromatin.
Molecular Modeling of Penicillin Acylase Binding with a Penicillin Nucleus by HPC: Can Enzyme or its Mutants Possess β-lactamase Activity?;Evgeny M. Kirilin, Anna A. Bochkova, Nikolay V. Panin, Igor V. Pochinok, Vytas Švedas;https://doi.org/10.14529/jsfi220206;moonlighting protein, penicillin acylase engineering, β-lactam antibiotics resistance, β-lactamase design, metadynamics;HPC has been used for molecular modeling of penicillin acylase interaction with a penicillin nucleus 6-aminopenicillanic acid (6-APA) to assess whether the wild-type enzyme or its mutants could possess β-lactamase activity. Applying parallel hybrid GPU/CPU computing technologies for metadynamics calculations with the PLUMED library in conjunction with AMBER software suite it has been shown that trace amounts of wild-type penicillin acylase6-APA complexes leading to a β-lactamase reaction can be formed. Higher β-lactamase activity can be observed in enzyme mutants by introducing charged residue in the substrate binding pocket and its proper positioning with respect to a catalytic nucleophile, including stabilization of the tetrahedral intermediate in the oxyanion hole. Thus, it has been shown that the certain mutations facilitate the orientation of the substrate required for the manifestation of β-lactamase activity in the penicillin acylase active center.
Search for Ligands Complementary to the 430-cavity of Influenza Virus Neuraminidase by Virtual Screening;Dmitry K. Nilov, Michaela Schmidtke, Vadim A. Makarov, Vytas K. Švedas;https://doi.org/10.14529/jsfi220207;influenza, neuraminidase, 430-cavity, inhibitor, anthrapyrazole;An anthrapyrazole derivative STK663786 has been identified as a selective ligand of the socalled 430-cavity of influenza virus neuraminidase at virtual screening of a library of low-molecularweight compounds. It is able to form favorable contacts with hydrophobic residues as well as cationπ interaction and hydrogen bonds with the polar Arg371 residue. The experimentally determined EC₅₀ values have been found to be 19 and 30 µM for viruses H1N1 and H3N2, respectively. Complementarity of STK663786 to the 430-cavity adjacent to the sialic acid binding subsite in the active center of neuraminidase makes this compound a valuable structural fragment at construction of bifunctional inhibitors of the enzyme.
Exascale Machines Require New Programming Paradigms and Runtimes;Georges Da Costa, Thomas Fahringer, Juan Antonio Rico Gallego, Ivan Grasso, Atanas Hristov, Helen D. Karatza, Alexey Lastovetsky, Fabrizio Marozzo, Dana Petcu, Georgios L. Stavrinides, Domenico Talia, Paolo Trunfio, Hrachya Astsatryan;https://doi.org/10.14529/jsfi150201;programming models, ultrascale, runtimes, extreme scale;Extreme scale parallel computing systems will have tens of thousands of optionally accelerator-equiped nodes with hundreds of cores each, as well as deep memory hierarchies and complex interconnect topologies. Such Exascale systems will provide hardware parallelism at multiple levels and will be energy constrained. Their extreme scale and the rapidly deteriorating reliablity of their hardware components means that Exascale systems will exhibit low mean-time-between-failure values. Furthermore, existing programming models already require heroic programming and optimisation efforts to achieve high efficiency on current supercomputers. Invariably, these efforts are platform-specific and non-portable. In this paper we will explore the shortcomings of existing programming models and runtime systems for large scale computing systems. We then propose and discuss important features of programming paradigms and runtime system to deal with large scale computing systems with a special focus on data-intensive applications and resilience.Finally, we also discuss code sustainability issues and propose several software metrics that are of paramount importance for code development for large scale computing systems.
Acceleration of MPI mechanisms for sustainable HPC applications;Jesus Carretero, Javier Garcia-Blas, David E. Singh, Florin Isaila, Alexey Lastovetsky, Thomas Fahringer, Radu Prodan, Peter Zangerl, Christi Symeonidou, Afshin Fassihi, Horacio Pérez-Sánchez;https://doi.org/10.14529/jsfi150202;"MPI, MPI sustainability, programming models, resilience, data management, MPI
applications";Ultrascale computing systems are meant to reach a growth of two or three orders of magnitude of today computing systems. However, to achieve the Perfomances required, we will need to design and implement more sustainable solutionsfor ultra-scale computing systems, understanding sustainability in a holistic manner to address challenges as economy-of-scale, agile elastic scalability, heterogeneity, programmability, fault resilience, energy efficiency, and scalable storage. Some of those solutions could be provided into MPI, but other should be devised as higher level concepts, less generalists, but adapted to applicative domains, possibly as programming patterns or or libraries. In this paper, we show some proposals to extend MPI trying to cover major domains that are relevant towards sustainability: MPI programming optimizations and programming models, resilience, data management, and their usage from applications.
Resilience within Ultrascale Computing System: Challenges and Opportunities from Nesus Project;Pascal Bouvry, Rudolf Mayer, Jakub Muszyński, Dana Petcu, Andreas Rauber, Gianluca Tempesti, Tuan Trinh, Sébastien Varrette;https://doi.org/10.14529/jsfi150203;HPC, fault tolerance, algorithm-based fault tolerance, extreme data, evolutionary algorithm, ultrascale computing system;Ultrascale computing is a new computing paradigm that comes naturally from the necessity of computing systems that should be able to handle massive data in possibly very large scale  distributed systems, enabling new forms of applications that can serve a very large amount of  users and in a timely manner that we have never experienced before. However, besides the benefits,  ultrascale computing systems do not come without challenges. One of the challenges is the resilience  of ultrascale computing systems. Although resilience is already an established field in system  science and many methodologies and approaches are available to deal with it, the unprecedented  scales of computing, of the massive data to be managed, new network technologies, and drastically  new forms of massive scale applications bring new challenges that need to be addressed. This paper  reviews the challenges and approaches of resilience in ultrascale computing systems from multiple  perspectives involving and addressing the resilience aspects of hardware-software co-design for  ultrascale systems, resilience against (security) attacks, new approaches and methodologies to  resilience in ultrascale systems, applications and case studies.
Energy Measurement Tools for Ultrascale Computing: A Survey;Francisco Almeida, Javier Arteaga, Vicente Blanco, Alberto Cabrera;https://doi.org/10.14529/jsfi150204;"energy measurement, power measurement, data acquisition tools, infrastructure
management, ultrascale computing";With energy efficiency one of the main challenges on the way towards ultrascale systems, there is great need for access to high-quality energy consumption data. Such data would enable researchers and designers to pinpoint energy inefficiencies at all levels of the computing stack, from whole nodes down to critical regions of code. However, measurement capabilities are often missing, and significantly differ between platforms where they exist. A standard is yet  to be established. To that end, this paper attempts an extensive survey of energy measurement tools currently available at both the hardware and software level, comparing their features with respect to energy monitoring.
Energy-efficient Algorithms for Ultrascale Systems;Jesus Carretero, Salvatore Distefano, Dana Petcu, Daniel Pop, Thomas Rauber, Gudula Rünger, David E. Singh;https://doi.org/10.14529/jsfi150205;energy-awareness, energy-efficient algorithms, ultrascale computing;The chances to reach Exascale or Ultrascale Computing are strongly connected with the problem of the energy consumption for processing applications. For physical as well as economical reasons, the energy consumption has to be reduced significantly to make Ultrascale Computing possible. The research efforts towards energy-saving mechanisms of the hardware has already led to energy-aware hardware systems available today. However, hardware mechanisms can only obtain an energy reduction if software can exploit them such that energy-efficient computing actually results. In the software area, there also exists a multitude of research approaches towards energy saving. These research approaches and results are often isolated either on the system software level or the application organization level, reflecting the expertise of the corresponding research group. The challenge of reducing the energy consumption dramatically to make Ultrascale Computing possible are so ambitions that a concerted action combining all these software levels and research efforts seems reasonable. In this article, we demonstrate the current research efforts and results related to energy in the diverse areas of software. Moreover, we conclude with open problems and questions concerning energy-related techniques with an emphasis on the application algorithmic side.
Energy Efficiency for Ultrascale Systems: Challenges and Trends from Nesus Project;Michel Bagein, Jorge Barbosa, Vicente Blanco, Ivona Brandic, Samuel Cremer, Sebastien Fremal, Helen Karatza, Laurent Lefevre, Toni Mastelic, Ariel Oleksiak, Anne-Cecile Orgerie, Georgios L. Stavrinides, Sebastien Varrette;https://doi.org/10.14529/jsfi150206;"energy and power measurement, data acquisition tools, energy modeling, scheduling,
applications, heterogeneous infrastructures, ultrascale computing";"Energy consumption is one of the main limiting factors for designing and deploying ultrascale systems. Therefore, this paper presents challenges and trends associated with energy efficiency for ultrascale systems based on current activities of the working group on ""Energy Efficiency"" in the European COST Action Nesus IC1305. The analysis contains major areas that are related to studies of energy efficiency in ultrascale systems: heterogeneous and low power hardware architectures, power monitoring at large scale, modeling and simulation of ultrascale systems, energy-aware scheduling and resource management, and energy-efficient application design."
Research Problems and Opportunities in Memory Systems;Onur Mutlu, Lavanya Subramanian;https://doi.org/10.14529/jsfi140302;memory systems, scaling, DRAM, flash, non-volatile memory, QoS, reliability;The memory system is a fundamental Perfomance and energy bottleneckin almost all computing systems. Recent system design, application,and technology trends that require more capacity, bandwidth,efficiency, and predictability out of the memory system make it aneven more important system bottleneck. At the same time, DRAMtechnology is experiencing difficult {\em technology scaling}challenges that make the maintenance and enhancement of its capacity,energy-efficiency, and reliability significantly more costly withconventional techniques.In this article, after describing the demands and challenges faced bythe memory system, we examine some promising research and designdirections to overcome challenges posed by memoryscaling. Specifically, we describe three major {\em new} researchchallenges and solution directions: 1) enabling new DRAMarchitectures, functions, interfaces, and better integration of theDRAM and the rest of the system (an approach we call {\em system-DRAM                          co-design}), 2) designing a memory system that employs emergingnon-volatile memory technologies and takes advantage of multipledifferent technologies (i.e., {\em hybrid memory systems}), 3)providing predictable Perfomance and QoS to applications sharing thememory system (i.e., {\em QoS-aware memory systems}). We also brieflydescribe our ongoing related work in combating scaling challenges ofNAND flash memory. 
Early evaluation of direct large-scale InfiniBand networks with adaptive routing;Alexander N. Daryin, Anton A. Korzh;https://doi.org/10.14529/jsfi140303;adaptive routing, InfiniBand, high-radix topology, network simulation;We assess the problem of choosing optimal direct topology for InfiniBand networks in terms of Perfomance. Newest topologies like Dragonfly, Flattened butterfly and Slim Fly are considered, as well as standard Tori and Hypercubes.We consider some reasonable extensions to InfiniBand hardware which could be implemented by vendors easily and may allow reasonable routing algorithms for such topologies. A number of routing algorithms are proposed and compared for various traffic patterns. Mapping algorithms for Dragonfly and Flattened Butterfly are proposed. Based on this research it has been decided to use Flattened Butterfly topology for system #22 in November 2014 Top 500 list.
Heterogeneous parallel computing: from clusters of workstations to hierarchical hybrid platforms;Alexey Lastovetsky;https://doi.org/10.14529/jsfi140304;parallel computing, heterogeneous computing, data partitioning;The paper overviews the state of the art in design and implementation of data parallelscientic applications on heterogeneous platforms. It covers both traditional approaches originallydesigned for clusters of heterogeneous workstations and the most recent methods developed in thecontext of modern multicore and multi-accelerator heterogeneous platforms.
Co-design of Parallel Numerical Methods for Plasma Physics and Astrophysics;Boris M. Glinskiy, Igor M. Kulikov, Alexey V. Snytnikov, Alexey A. Romanenko, Igor G. Chernykh, Vitaly A. Vshivkov;https://doi.org/10.14529/jsfi140305;"Co-design, hybrid supercomputers, Particle-In-Cell method, Godunov method,
GPU";Physically meaningful simulations in plasma physics and astrophysics need powerful hybrid supercomputers equipped with computation accelerators. The development of parallel numerical codes for such supercomputers is a complex scientific problem. In order to solve it the concept of co-design is employed. The co-design is defined as considering the architecture of the supercomputer at all stages of the development of the code. The use of co-design is shown by the example of two physical problems: the interaction of an electron beam with plasma and the collision of galaxies. The resulting speedup and efficiency are shown.
AlgoWiki: an Open Encyclopedia of Parallel Algorithmic Features;Vladimir V. Voevodin, Alexander S. Antonov, Jack Dongarra;https://doi.org/10.14529/jsfi150101;algorithm structure, resource of parallelism, parallel computing, efficiency, Perfomance, supercomputers, scalability, data locality, encyclopedia of algorithmic features;The main goal of this project is to formalize the mapping of algorithms onto the architecture of parallel computing systems. The basic idea is that features of algorithms are independent of any computing system. A detailed description of a given algorithm with a special emphasis on its parallel properties is made once, and after that it can be used repeatedly for various implementations of the algorithm on different computing platforms. Machine-dependent, part of this work is devoted to describing features of algorithms implementation for different parallel architectures. The proposed description of algorithms includes many non-trivial features such as: parallel algorithm complexity, resource of parallelism and its properties, features of the informational graph, computational cost of algorithms, data locality analysis as well as analysis of scalability potential, and many others. Descriptions of algorithms form the basis of AlgoWiki, which allows for collaboration with the computing community in order to produce different implementations and achieve improvement. Project website: http://algowiki-project.org/en/
Applications for ultrascale computing;Milan Mihajlovic, Lars Ailo Bongo, Raimondas Ciegis, Neki Frasheri, Dragi Kimovski, Peter Kropf, Svetozar Margenov, Maya Neytcheva, Thomas Rauber, Gudula Runger, Roman Trobec, Roel Wuyts, Roman Wyrzykowski, Jing Gong;https://doi.org/10.14529/jsfi150102;sustainable ultrascale systems, impact factors on applications, multiscale and multiphysics applications, computational modelling;Studies of complex physical and engineering systems, represented by multi-scale and multi-physics computer simulations have an increasing demand for computing power, especially when the simulations of realistic problems are considered. This demand is driven by the increasing size and complexity of the studied systems or the time constraints. Ultrascale computing systems offer a possible solution to this problem. Future ultrascale systems will be large-scale complex computing systems combining technologies from HPC, distributed systems, big data, and cloud computing. Thus, the challenge of developing and programming complex algorithms on these systems is twofold. Firstly, the complex algorithms have to be either developed from scratch, or redesigned in order to yield high Perfomance, while retaining correct functional behaviour. Secondly, ultrascale computing systems impose a number of non-functional cross-cutting concerns, such as fault tolerance or energy consumption, which can significantly impact the deployment of applications on large complex systems. This article discusses the state-of-the-art of programming for current and future large scale systems with an emphasis on complex applications. We derive a number of programming and execution support requirements by studying several computing applications that the authors are currently developing and discuss their potential and necessary upgrades for ultrascale execution.
Dense Matrix Computations on NUMA Architectures with Distance-Aware Work Stealing;Rabab Al-Omairy, Guillermo Miranda, Hatem Ltaief, Rosa M. Badia, Xavier Martorell, Jesus Labarta, David Keyes;https://doi.org/10.14529/jsfi150103;"Dense Matrix Computations, Dynamic Runtime Systems, Software Productivity,
Non-Uniform Memory Access, Data Locality, Work Stealing, HPC";We employ the dynamic runtime system OmpSs to decrease the overhead of data motion in the now ubiquitous non-uniform memory access (NUMA) high concurrency environment of multicore processors. The dense numerical linear algebra algorithms of Cholesky factorization and symmetric matrix inversion are employed as representative benchmarks. Work stealing occurs within an innovative NUMA-aware scheduling policy to reduce data movement between NUMA nodes. The overall approach achieves separation of concerns by abstracting the complexity of the hardware from the end users so that high productivity can be achieved. Perfomance results on a large NUMA system outperform the state-of-the-art existing implementations up to a two fold speedup for the Cholesky factorization, as well as the symmetric matrix inversion, while the OmpSs-enabled code maintains strong similarity to its original sequential version.
Neo-hetergeneous Programming and Parallelized Optimization of a Human Genome Re-sequencing Analysis Software Pipeline on TH-2 Supercomputer;Xiangke Liao, Shaoliang Peng, Yutong Lu, Yingbo Cui, Chengkun Wu, Heng Wang, Jiajun Wen;https://doi.org/10.14529/jsfi150104;"biological big data; parallelized optimization; TH-2; sequence alignment; SNP detection; whole genome re-sequencing";The growing velocity of biological big data is way beyond Moore's Law of compute power growth. The amount of genomic data has been explosively accumulating, which calls for an enormous amount of computing power, while current computation methods cannot scale out with the data explosion. In this paper, we try to utilize huge computing resources to solve thebig dataproblems of genome processing on TH-2 supercomputer. TH-2supercomputer adopts neo-heterogeneous architecture and owns 16,000 compute nodes: 32000 Intel Xeon CPUs + 48000 Xeon Phi MICs. The heterogeneity, scalability, and parallel efficiency pose great challenges forthe deployment of the genomeanalysis software pipeline on TH-2. Runtime profiling shows that SOAP3-dp and SOAPsnp are the most time-consuming parts (up to 70% of total runtime) in the whole pipeline, which need parallelized optimization deeply and large-scale deployment. To address this issue, we first designa series of new parallel algorithms for SOAP3-dp and SOAPsnp, respectively, to eliminatethe spatial-temporal redundancy. Then we propose a CPU/MIC collaboratedparallel computing method in one node to fully fill the CPU/MIC time slots. We also propose a series ofscalable parallel algorithms and large scaleprogramming methods to reduce the amount of communications between different nodes. Moreover, we deploy and evaluate our works on the TH-2 supercomputer in different scales. At the most large scale, the whole process takes 8.37 hours using 8192 nodes to finish the analysis of a 300TB dataset of whole genome sequences from 2,000 human beings, which can take as long as 8 months on a commodity server. The speedup is about 700x.
Data Exploration at the Exascale;Hank Childs;https://doi.org/10.14529/jsfi150301;scientific visualization, HPC, Lagrangian flow analysis;"In situ processing - i.e., coupling visualization routines to a simulation code to generate images in real-time - is predicted to be the dominant form for visualization on upcoming supercomputers. Unfortunately, traditional in situ techniques are largely incongruent with exploratory visualization, which is an important activity to enable understanding of simulation data. In re- sponse, a new paradigm is emerging: data is transformed and massively reduced in situ and then the resulting form is explored post hoc. The fundamental tension in this approach is between the extent of the data reduction and the loss in integrity in the resulting data. However, new oppor- tunities, in terms of increased access to data, may blunt this tension and allow for both sufficient data reduction and also more accurate analysis. With this paper, we describe the trends behind ""data exploration at the exascale"" and also summarize some recent results that confirmed that this new paradigm can produce superior results compared to the traditional one. "
InfiniCloud: Leveraging the Global InfiniCortex Fabric and OpenStack Cloud for Borderless HPC of Genomic Data;Kenneth Hon Kim Ban, Jakub Chrzeszczyk, Andrew Howard, Dongyang Li, Tin Wee Tan;https://doi.org/10.14529/jsfi150302;Genomics, Cloud-Computing, InfiniBand, Trans-continental, Virtualization, SRIOV, OpenStack, HPC;At the Supercomputing Frontiers Conference in Singapore in 2015, A*CRC (Singapore) and NCI (Canberra, Australia) presented InfiniCloud, a geographically distributed, high Perfomance InfiniBand HPC Cloud which aims to enable borderless processing of genomic data as part of the InfiniCortex project. This paper provides a high-level technical overview of the architecture of InfiniCloud and how it can be used for high Perfomance computation of genomic data in geographically distant sites by encapsulation of workflows/applications in Virtual Machines (VM) coupled with on-the-fly configuration of clusters and high speed transfer of data via long range InfiniBand. 
Perfomance Assessment of InfiniBand HPC Cloud Instances on Intel Haswell and Intel Sandy Bridge Architectures;Jonathan Low, Jakub Chrzeszczyk, Andrew Howard, Andrzej Chrzeszczyk;https://doi.org/10.14529/jsfi150303;"Cloud-Computing, InfiniBand, Trans-continental, Benchmarking, Virtualization,
SRIOV, BeeGFS, OpenStack, HPC";"This paper aims to establish a Perfomance baseline of a HPC installation of OpenStack. We created InfiniCloud - a distributed High Perfomance Cloud hosted on remote nodes of InfiniCortex. InfiniCloud compute nodes use high Perfomance Intel (R) Haswell and Sandy Bridge CPUs, SSD storage and 64-256GB RAM. All computational resources are connected by high Perfomance IB interconnects and are capable of trans-continental IB communication using Obsidian Longbow range extenders.We benchmark the Perfomance of our test-beds using micro-benchmarks for TCP bandwidth, IB bandwidth and latency, file creation Perfomance, MPI collectives and Linpack. This paper compares different CPU generations across virtual and bare-metal environments.The results show modest improvements in TCP and IB bandwidth and latency on Haswell; Perfomance being largely dependent on the IB hardware. Virtual overheads were minimal and near-native Perfomance is possible for sufficiently large messages. From the Linpack testing, users can expect more than twice the Perfomance in their applications on Haswell-provisioned VMs. On Haswell hardware, native and virtual Perfomance differences is still significant for MPI collective operations. Finally, our parallel filesystem testing revealed virtual Perfomance coming close to native only for non-sync/fsync file operations."
The L-CSC cluster: Optimizing power efficiency to become the greenest supercomputer in the world in the Green500 list of November 2014;David Rohr, Gvozden Neskovic, Volker Lindenstruth;https://doi.org/10.14529/jsfi150304;L-CSC, HPL, Linpack, Green500, GPU, Energy Efficiency, HPC, LQCD;The L-CSC (Lattice Computer for Scientific Computing) is a general purpose compute cluster built with commodity hardware installed at GSI. Its main operational purpose is Lattice QCD (LQCD) calculations for physics simulations. Quantum Chromo Dynamics (QCD) is the physical theory describing the strong force, one of the four known fundamental interactions in the universe. L-CSC leverages a multi-GPU design accommodating the huge demand of LQCD for memory bandwidth. In recent years, heterogeneous clusters with accelerators such as GPUs have become more and more powerful while supercomputers in general have shown enormous increases in power consumption making electricity costs and cooling a significant factor in the total cost of ownership. Using mainly GPUs for processing, L-CSC is very power-efficient, and its architecture was optimized to provide the greatest possible power efficiency. This paper presents the cluster design as well as optimizations to improve the power efficiency. It examines the power measurements performed for the Green500 list of the most power-efficient supercomputers in the world which led to the number 1 position as the greenest supercomputer in November 2014.
An Autonomic Perfomance Environment for Exascale;Kevin A. Huck, Allan Porterfield, Nick Chaimov, Hartmut Kaiser, Allen D. Malony, Thomas Sterling, Rob Fowler;https://doi.org/10.14529/jsfi150305;ParalleX, HPX, exascale, Perfomance measurement, adaptive runtimes;"Exascale systems will require  new approaches to Perfomance observation, analysis, and runtime decision-making to optimize for Perfomance and efficiency. The standard ""first-person"" model, in which multiple operating system processes and threads observe themselves and record first-person Perfomance profiles or traces for offline analysis, is not adequate to observe and capture interactions at shared resources in highly concurrent, dynamic systems. Further, it does not support mechanisms for runtime adaptation. Our approach, called APEX (Autonomic Perfomance Environment for eXascale), provides mechanisms for sharing information among the layers of the software stack, including hardware, operating and runtime systems, and application code, both new and legacy. The Perfomance measurement components share information  across layers, merging first-person data sets with information collected by  third-person tools observing shared hardware and software states at  node- and global-levels. Critically, APEX provides a policy engine designed to guide runtime adaptation mechanisms to make algorithmic changes, re-allocate resources, or change scheduling rules when appropriate conditions occur."
Visualization for Exascale: Portable Perfomance is Critical;Kenneth Moreland, Matthew Larsen, Hank Childs;https://doi.org/10.14529/jsfi150306;scientific visualization, exascale, Perfomance portability, data parallel primitives;Researchers face a daunting task to provide scientific visualization capabilities for exascale computing. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Multiple vendors create such accelerator processors, each with significantly different features and Perfomance characteristics. To address these visualization needs across multiple platforms, we are embracing the use of data parallel primitives that encapsulate highly efficient parallel algorithms that can be used as building blocks for conglomerate visualization algorithms. We can achieve Perfomance portability by optimizing this small set of data parallel primitives whose tuning conveys to the conglomerates.
A Case for Embedded FPGA-based SoCs in Energy-Efficient Acceleration of Graph Problems;Nachiket Kapre, Pradeep Moorthy;https://doi.org/10.14529/jsfi150307;energy efficiency, sparse graphs, embedded SoCs, FPGAs;Sparse graph problems are notoriously hard to accelerate on conventional platforms due to irregular memory access patterns resulting in underutilization of memory bandwidth. These bottlenecks on traditional x86-based systems mean that sparse graph problems scale very poorly, both in terms of Perfomance and power efficiency. A cluster of embedded SoCs (systems-on-chip) with closely-coupled FPGA accelerators can support distributed memory accesses with better matched low-power processing. We first conduct preliminary experiments across a range of COTS (commercial off-the-shelf) embedded SoCs to establish promise for energy-efficiency acceleration of sparse problems. We select the Xilinx Zynq SoC with FPGA accelerators to construct a prototype 32-node Beowulf cluster. We develop specialized MPI routines and memory DMA offload engines to support irregular communication efficiently. In this setup, we use the ARM processor as a data marshaller for local DMA traffic as well as remote MPI traffic while the FPGA may be used as a programmable accelerator. Across a set of benchmark graphs, we show that 32-node embedded SoC cluster can exceed the energy efficiency of an Intel E5-2407 by as much as 1.7× at a total graph processing capacity of 91–95 MTEPS for graphs as large as 32 million nodes and edges. 
Live Programming in Scientific Simulation;Ben Swift, Andrew Sorensen, Henry Gardner, Peter Davis, Viktor K. Decyk;https://doi.org/10.14529/jsfi150401;live programming, particle-in-cell, JIT-compilation;We demonstrate that a live-programming environment can be used to harness and add run-time interactivity to scientific simulation codes. Through a set of examples using a Particle-In-Cell (PIC) simulation framework we show how the real-time, human-in-the-loop interactivity of live programming can be incorporated into a traditional, “offline”, development workflow. We discuss how live programming tools and techniques can be productively integrated into the existing HPC landscape to increase productivity and enhance exploration and discovery.
Creating interconnect topologies by algorithmic edge removal: MOD and SMOD graphs;Marek T. Michalewicz, Łukasz P. Orłowski, Yuefan Deng;https://doi.org/10.14529/jsfi150402;"supercomputer interconnects, big data, exascale computing, graph theory, topology
of graphs, classes of graphs, graph generation";We introduce a method of constructing classes of graphs by algorithmic removal of entire groups of edges. Our approach to creating new classes of graphs is to focus entirely on the structure and properties of the adjacency matrix. At an initialisation step of the algorithm we start with a complete (fully connected) graph. In Part I we present MOD and arrested MOD graphs resulting from removal of square blocks of edges at each iteration and substitution of removed blocks with a diagonal matrix with one extra pivotal element along the main diagonal. The MOD graphs possess unique and useful properties. All important graph measures are easily expressed in analytical form and are presented in the paper. Several important properties of MOD graphs compare very favourably with graphs representing common interconnect topologies: hypercube, 3D and 5D tori, TOFU and dragony. This lead us to consider MOD and arrested MOD graphs as interesting candidats for eective supercomputer interconnects.In Part II, at each iterative step we successively remove triangular shapes from adjacency matrix. This iterative process leads to the nal matrix which has two Sierpinski gaskets aligned along the main diagonal. It will be shown below, that this new class of graphs is not a Sierpinski graph, since it is the adjacency matrix which has a structure of a Sierpinski gasket, and not a graph described by this matrix. We call this new class of graphs Sierpinski-Michalewicz-Or lowski-Deng (SMOD) graphs. The most remarkable property of the SMOD class of graphs, is that irrespective of the graph order, the diameter is constant and equals 2. The size of the graph, or the total number of edges, is about 10% of the size of a complete graph of the same order. We analyse important graph theoretic characte-ristics related to the topology such as diameter as a function of graph order, size, mean path length, ratio of the graph size to the size of a complete graph of the same order, and some spectral properties.Keywords: supercomputer interconnects, big data, exascale computing, graph theory,topology of graphs, classes of graphs, graph generation.
Multi-Scale Supercomputing of Large Molecular Aggregates: A Case Study of the Light-Harvesting Photosynthetic Center;Alexander V. Nemukhin, Igor V. Polyakov, Alexander I. Moskovsky;https://doi.org/10.14529/jsfi150403;quantum chemistry, fragmentation, multi-scale approaches, parallel algorithms;Numerical solution of the quantum mechanical Schrödinger equation is required to model electronic excitations in the light-harvesting photosynthetic complexes composed of up to millions of atoms. We demonstrate that the modern supercomputers can be used to treat electronic structure calculations in such large molecular aggregates if proper multi-scale massive-parallel approaches are applied. We show that the three-level parallelization scheme based on the novel numerical algorithms assuming fragmentation of a light-harvesting complex allows us to reduce considerably the high scaling of ab initio quantum chemistry methods. More specifically we applied the time-dependent density functional theory based upon the fragment molecular orbital presentation (FMO-TDDFT) implemented at the modern supercomputers to obtain a realistic estimate of the electronic excitation in the complex. The application shows a good overall scaling.  
Parallel software platform INMOST: a framework for numerical modeling;Alexander A. Danilov, Kirill M. Terekhov, Igor N. Konshin, Yuri V. Vassilevski;https://doi.org/10.14529/jsfi150404;distributed mesh, polyhedral mesh, parallel framework, numerical modeling;The INMOST mathematical modeling toolkit helps a user to formulate and solve a problem of partial differential equations on general meshes in parallel. The current work covers: data structure description for efficient distributed unstructured mesh representation, interrelation of mesh elements with maximal flexibility of supported types of the mesh, treatment of ghost cells and distribution of mesh data for parallel execution, flexible templates for the implementation of numerical schemes, convenient framework for parallel linear systems assembly and solution. We also present aspects of the implementation and a simple example of application of INMOST to the solution of anisotropic diffusion problem. On this example we demonstrate the application of INMOST for all the stages of numerical modeling: construction of the distributed mesh, assignment of the problem data to the elements, problem discretization on local domain, solution of linear system in parallel. INMOST is a newly developed, flexible and efficient numerical analysis framework that provides scientists the infrastructure for designing highly scalable high Perfomance applications for mathematical modeling.
Parallel Programming Models for Dense Linear Algebra on Heterogeneous Systems;Jack Dongarra, M. Abalenkovs, A. Abdelfattah, M. Gates, A. Haidar, J. Kurzak, P. Luszczek, S. Tomov, I. Yamazaki, A. YarKhan;https://doi.org/10.14529/jsfi150405;Programming models, runtime, HPC, GPU, multicore, dense linear algebra;We present a review of the current best practices in parallel programming models for dense linear algebra (DLA) on heterogeneous architectures. We consider multicore CPUs, stand alone manycore coprocessors, GPUs, and combinations of these. Of interest is the evolution of the programming models for DLA libraries { in particular, the evolution from the popular LAPACK and ScaLAPACK libraries to their modernized counterparts PLASMA (for multicore CPUs) and MAGMA (for heterogeneous architectures), as well as other programming models and libraries.Besides providing insights into the programming techniques of the libraries considered, we outline our view of the current strengths and weaknesses of their programming models { especially in regards to hardware trends and ease of programming high-Perfomance numerical software that current applications need { in order to motivate work and future directions for the next generation of parallel programming models for high-Perfomance linear algebra libraries on heterogeneous systems.
NR-MPI: A Non-stop and Fault Resilient MPI Supporting Programmer Defined Data Backup and Restore for E-scale Super Computing Systems;Suo Guang;https://doi.org/10.14529/jsfi160101;Message passing interface, fault tolerant MPI, NR-MPI, Application-level Checkpoint/Restart;Fault resilience has became a major issue for HPC systems, particularly, in the perspective of future E-scale systems, which will consist of millions of CPU cores and other components. MPI-level fault tolerant constructs, such as ULFM, are being proposed to support software level fault tolerance. However, there are few systematic evaluations by application programmers using benchmarks or pseudo applications. This paper proposes NR-MPI, a \emph{N}on-stop and Fault \emph{R}esilient \emph{MPI}, supporting programmer defined data backup and restore. To help programmers write fault tolerant programs, NR-MPI provides a set of friendly programming interfaces and a state transition diagram for data backup and restore. This paper focuses on design, implementation and evaluation of NR-MPI. Specifically,this paper puts emphases on failure detection in MPI library, friendly programming interface extending for NR-MPI and examples of fault tolerant programs based NR-MPI. Furthermore, to support failure recovery of applications, NR-MPI implements data backup interfaces based on double in-memory checkpoint/restart. We conduct experiments with both NPB benchmarks and Sweep3D on TH supercomputer in NSCC-TJ. Experimental results show that NR-MPI based fault tolerant programs can recover from failures online without restarting, and the overhead is small even for applications with tens of thousands of cores.
Reconfigurable computer systems: from the first FPGAs towards liquid cooling systems;Ilya I. Levin, Alexey I. Dordopulo, Alexander M. Fedorov, Igor A. Kalyaev;https://doi.org/10.14529/jsfi160102;FPGA, reconfigurable computer systems, immersion liquid cooling system;The paper covers the history of development of design technologies of reconfigurable computer systems based on FPGAs of various families. Five generations of reconfigurable computer systems with high placement density, designed on the base of various FPGA families, from Xilinx Virtex-E to modern Virtex UltraScale, are described. The last achievements in the domain of design of energetic effective reconfigurable computer systems with high real Perfomance are presented. One of such achievements is the developed liquid cooling system for Virtex UltraScale FPGAs. It provides independent circulation of the cooling liquid in the 3U computational module with the 19’’ height for cooling of 96-128 FPGA chips that in total generate 9.6-12.8 kWatt of heat. The distinctive features of the designed immersion liquid cooling system are high cooling efficiency with power reserve for the designed perspective FPGA families, resistance to leaks and their consequences, and compatibility with traditional water cooling systems based on industrial chillers.
Supercomputer technologies in tomographic imaging applications;Alexander V. Goncharsky, Sergey Y. Romanov, Sergey Y. Seryozhnikov;https://doi.org/10.14529/jsfi160103;supercomputer, GPU cluster, wave-tomography imaging, coefficient inverse problems, scalar wave equation, breast cancer;Currently, tomographic imaging is widely used in medical and industrial non-destructive testing applications. X-ray tomography is the prevalent imaging technology. Modern medical X-ray CT scanners provide up to 1 mm spatial resolution. The disadvantage of X-ray tomography is that it cannot be used for regular medical examinations. Early breast cancer diagnosis is one of the most pressing issues in modern healthcare. Ultrasound tomography devices are being developed in USA, Germany and Russia to address this problem. One of the main challenges in ultrasound tomographic imaging is the development of efficient algorithms for solving inverse problems of wave tomography, which are nonlinear three-dimensional coefficient inverse problems for a hyperbolic differential equation. Solving such computationally-expensive problems requires the use of supercomputers.
Server Level Liquid Cooling: Do Higher System Temperatures Improve Energy Efficiency?;Alexander A. Moskovsky, Egor A. Druzhinin, Alexey B. Shmelev, Vladimir V. Mironov, Andrey Semin;https://doi.org/10.14529/jsfi160104;hot liquid cooling, cold plates, energy efficiency;Liquid cooling is now a mainstream approach to boost energy efficiency for HPC systems. Higher coolant temperature is usually considered as an advantage, since it allows heat reuse/recuperation and simplifies datacenter infrastructure by eliminating the need of chiller machine. However, the use of hot coolant imposes high requirements for cooling equipment. A promising approach is to utilize coldplates with channel structure and liquid circulation for heat removal from semiconductor components. We have designed a coldplate with low heat-resistance that ensures effective cooling with only 2030° temperature difference between coolant and electronic parts of a server. Under stress-test conditions the coolant temperature was up to 65 °C while server operation was undisturbed. We also studied power efficiency (expressed in floating point operations per watt) dependence on the coolant temperature (19-65 °C) on theindividualserverlevel (based on Intel Grantley platform with dual Intel Xeon E5-2697v3 processors). иThe power Perfomance ratio shows moderate (≈10%) efficiency drop from 19 to 65°C due to increase of leak age current in chipset components and reduction of processor frequency resulted into proportional reduction of DGEMM benchmark Perfomance. It must be taken into account by datacenter designers, that the amount of recuperated energy from 65 °C should be at least≈10% to justify the choice of high temperature coolant solution.
Data Compression for Climate Data;Michael Kuhn, Julian Kunkel, Thomas Ludwig;https://doi.org/10.14529/jsfi160105;data compression, storage system, climate data, cost efficiency;The different rates of increase for computational power and storage capabilities of supercomputers turn data storage into a technical and economical problem. Because storage capabilities are lagging behind, investments and operational costs for storage systems have increased to keep up with the supercomputers' I/O requirements. One promising approach is to reduce the amount of data that is stored. In this paper, we take a look at the impact of compression on Perfomance and costs of high Perfomance systems. To this end, we analyze the applicability of compression on all layers of the I/O stack, that is, main memory, network and storage. Based on the Mistral system of the German Climate Computing Center (Deutsches Klimarechenzentrum, DKRZ), we illustrate potential Perfomance improvements and cost savings. Making use of compression on a large scale can decrease investments and operational costs by 50% without negatively impacting Perfomance. Additionally, we present ongoing work for supporting enhanced adaptive compression in the parallel distributed file system Lustre and application-specific compression.
