import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, normalize


def vectorizeApi(list_of_docs, model):
    features = []
    for tokens in list_of_docs:
        zero_vector = np.zeros(model.vector_size)
        vectors = []
        for token in tokens:
            if token in model:
                try:
                    vectors.append(model[token])
                except KeyError:
                    continue
        if vectors:
            vectors = np.asarray(vectors)
            avg_vec = vectors.mean(axis=0)
            features.append(avg_vec)
        else:
            features.append(zero_vector)
    vectorized_docs = pcaModule(features)
    return vectorized_docs


def pcaModule(X):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_normalized = normalize(X_scaled)
    X_normalized = pd.DataFrame(X_normalized)
    pca = PCA(n_components=2)
    X_principal = pca.fit_transform(X_normalized)
    X_principal = pd.DataFrame(X_principal)
    X_principal.columns = ['X', 'Y']
    return X_principal


# def csvReader():
#     with open("optData.csv", encoding='utf-8') as csvfile:
#         filereader = csv.reader(csvfile, delimiter=';')
#         vectors = []
#         for row in filereader:
#             vectors.append(row[3])
#     return vectors
#
#
# def csvWriter(data):
#     with open('vecData.csv', 'w', encoding="utf-8", newline='') as file:
#         writer = csv.writer(file, delimiter=";")
#         writer.writerows(data)
#
#
# def main():
#     vectors = csvReader()
