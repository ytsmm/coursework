{
  "articles": [
    {
      "doi": "https://doi.org/10.14529/jsfi160201",
      "title": "A Dynamic Congestion Management System for InfiniBand Networks",
      "authors": "Fabrice Mizero, Malathi Veeraraghavan, Qian Liu, Robert D. Russell, John M. Dennis",
      "keywords": "A Dynamic Congestion Management System for InfiniBand NetworksInfiniBand, Congestion control, Link-by-link flow control, Cascading rate reductions, Dynamic parameter settingWhile the InfiniBand link-by-link flow control helps avoid packet loss, it unfortunately causes the effects of congestion to spread through a network. Flows whose paths do not even pass through congested ports could suffer from reduced throughput. We propose a Dynamic Congestion Management System (DCMS) to address this problem. Without per-flow information, the DCMS leverages performance counters of switch ports to detect onset of congestion, and determines whether-or-not victim flows are present. The DCMS then takes actions to cause an aggressive reduction in the sending rates of congestion-causing (contributor) flows if victim flows are present. On the other hand, in the absence of victim flows, the DCMS allows the contributor flows to maintain high sending rates and finish as quickly as possible.Our results show that dynamic congestion management can enable a network to serve both contributor flows and victim flows effectively. The DCMS solution operates within the constraints of the InfiniBand Standard.",
      "axisX": "-0.2867736753192729",
      "axisY": "-0.4559230073526782",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160202",
      "title": "Many-Core Approaches to Combinatorial Problems: case of the Langford Problem",
      "authors": "Micha\u00ebl Krajecki, Julien Loiseau, Fran\u00e7ois Alin, Christophe Jaillet",
      "keywords": "Many-Core Approaches to Combinatorial Problems: case of the Langford ProblemCombinatorial problems, parallel algorithm, GPU accelerators, CUDA, Langford\nproblemAs observed from the last TOP500 list - November 2015 -, GPUs-accelerated clusters emerge as clear evidence. But exploiting such architectures for combinatorial problem resolution remains a challenge. In this context, this paper focuses on the resolution of an academic combinatorial problem, known as Langford pairing problem, which can be solved using several approaches. We first focus on a general solving scheme based on CSP (Constraint Satisfaction Problem) formalism and backtrack called the Miller algorithm. This method enables us to compute instances up to L(2,21) using both CPU and GPU computational power with load balancing.As dedicated algorithms may still have better computation efficiency we took advantage of the Godfrey algebraic method to solve the Langford problem and implemented it using our multiGPU approach. This allowed us to recompute the last open instances, L(2, 27) and L(2, 28), respectively in less than 2 days and 23 days using best-effort computation on the ROMEO supercomputer with up to 500,000 GPU cores.",
      "axisX": "-0.05659873010108921",
      "axisY": "-0.1352527575731025",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160203",
      "title": "A Radical Approach to Computation with Real Numbers",
      "authors": "John L. Gustafson",
      "keywords": "A Radical Approach to Computation with Real NumbersFloating point, Unum computing, Computer arithmetic, Energy efficiency, Valid\narithmeticIf we are willing to give up compatibility with IEEE 754 floats and design a number format with goals appropriate for 2016, we can achieve several goals simultaneously: Extremely high energy efficiency and information-per-bit, no penalty for decimal operations instead of binary, rigorous bounds on answers without the overly pessimistic bounds produced by interval methods, and unprecedented high speed up to some precision. This approach extends the ideas of unum arithmetic introduced two years ago by breaking completely from the IEEE float-type format, resulting in fixed bit size values, fixed execution time, no exception values or \u201cgradual underflow\u201d issues, no wasted bit patterns, and no redundant representations (like \u201cnegative zero\u201d). As an example of the power of this format, a difficult 12-dimensional nonlinear robotic kinematics problem that has defied solvers to date is quickly solvable with absolute bounds. Also unlike interval methods, it becomes possible to operate on arbitrary disconnected subsets of the real number line with the same speed as operating on a simple bound.",
      "axisX": "0.047433295032869756",
      "axisY": "-0.3633121129454406",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160205",
      "title": "Making Large-Scale Systems Observable - Another Inescapable Step Towards Exascale",
      "authors": "Dmitry A. Nikitenko, Sergey A. Zhumatiy, Pavel A. Shvets",
      "keywords": "Making Large-Scale Systems Observable - Another Inescapable Step Towards Exascalescalable monitoring visualization, situational screen, supercomputer state visualization, joint monitoring sources, supercomputer dashboard, HPC instrument control boardThe effective mastering of extremely parallel HPC system is impossible without deep understanding of all internal processes and behavior of the whole diversity of the components: computing processors and nodes, memory usage, interconnect, storage, whole software stack, cooling and so forth in detail. There are numerous visualization tools that provide information on certain components and system as a whole, but most of them have severe issues that limit appliance in real life, thus becoming inacceptable for the future system scales.Predefined monitoring systems and data sources, lack of dynamic on-the-fly reconfiguration, inflexible visualization and screening options are among the most popular issues.The proposed approach to monitoring data processing resolves the majority of known problems, providing a scalable and flexible solution based on any available monitoring systems and other data sources. The approach implementation is successfully used in every-day practice of the largest in Russia supercomputer center of Moscow State University.",
      "axisX": "-0.48225418043527213",
      "axisY": "0.17126102685116873",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160206",
      "title": "Application of CUDA technology to calculation of ground states of few-body nuclei by Feynman's continual integrals method",
      "authors": "Mikhail A. Naumenko, Vyacheslav V. Samarin",
      "keywords": "Application of CUDA technology to calculation of ground states of few-body nuclei by Feynman's continual integrals methodNVIDIA CUDA, Feynman\u2019s continual integrals method, few-body nucleiThe possibility of application of modern parallel computing solutions to speed up the calculations of ground states of few-body nuclei by Feynman's continual integrals method has been investigated. These calculations may sometimes require large computational time, particularly in the case of systems with many degrees of freedom. This paper presents the results of application of general-purpose computing on graphics processing units (GPGPU). The energy and the square modulus of the wave function of the ground states of several few-body nuclei have been calculated using NVIDIA CUDA technology. The results show that the use of GPGPU significantly increases the speed of calculations.",
      "axisX": "0.03529765594194227",
      "axisY": "-0.23001766647300617",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160301",
      "title": "Hybrid CPU + Xeon Phi implementation of the Particle-in-Cell method for plasma simulation",
      "authors": "Iosif B. Meyerov, Sergey I. Bastrakov, Igor A. Surmin, Alexey V. Bashinov, Evgeny S. Efimenko, Artem V. Korzhimanov, Alexander A. Muraviev, Arkady A. Gonoskov",
      "keywords": "Hybrid CPU + Xeon Phi implementation of the Particle-in-Cell method for plasma simulationhybrid computing, Xeon Phi, Particle-in-CellThis paper presents experimental results of Particle-in-Cell plasma simulation on a hybrid system with CPUs and Intel Xeon Phi coprocessors. We consider simulation of two relevant laser-driven particle acceleration regimes using the Particle-in-Cell code PICADOR. On a node of a cluster with 2 CPUs and 2 Xeon Phi coprocessors the hybrid CPU + Xeon Phi configuration allows to fully utilize the computational resources of the node. It outperforms both CPU-only and Xeon Phi-only configurations with the speedups between 1.36 x and 1.68 x.",
      "axisX": "0.4094511619731937",
      "axisY": "0.6100658480794067",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160302",
      "title": "Easy Access to HPC Resources through the Application GUI",
      "authors": "Matthijs van Waveren, Ahmed Seif El Nawasany, Nasr Hassanein, David Moon, Niall O'Byrnes, Alain Clo, Karthikeyan Murugan, Antonio Arena",
      "keywords": "Easy Access to HPC Resources through the Application GUIKAUST, Remote Job Submission, Middleware, MATLAB, VASP, MedeA, ADFThe computing environment at the King Abdullah University of Science and Technology (KAUST) is growing in size and complexity. KAUST hosts the tenth fastest supercomputer in the world (Shaheen II) and several HPC clusters. Researchers can be inhibited by the complexity, as they need to learn new languages and execute many tasks in order to access the HPC clusters and the supercomputer. In order to simplify the access, we have developed an interface between the applications and the clusters and supercomputer that automates the transfer of input data and job submission and also the retrieval of results to the researcher\u2019s local workstation. The innovation is that the user now submits his jobs from within the application GUI on his workstation, and does not have to directly log into the clusters or supercomputer anymore. This article details the solution and its benefits to the researchers.",
      "axisX": "-0.3418977234798478",
      "axisY": "0.4445035166266096",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160303",
      "title": "Predicting I/O Performance in HPC Using Artificial Neural Networks",
      "authors": "Jan Fabian Schmid, Julian M. Kunkel",
      "keywords": "Predicting I/O Performance in HPC Using Artificial Neural Networksfile system, performance, modeling I/O, artificial neural networksThe prediction of file access times is an important part for the modeling of supercomputer's storage systems. These models can be used to develop analysis tools which support the users to integrate efficient I/O behavior.In this paper, we analyze and predict the access times of a Lustre file system from the client perspective. Therefore, we measure file access times in various test series and developed different models for predicting access times.\u00a0 The evaluation shows that in models utilizing artificial neural networks the average prediciton error is about 30% smaller than in linear models. A phenomenon in the distribution of file access times is of particular interest: File accesses with identical parameters show several typical access times.The typical access times usually differ by orders of magnitude and can be explained with a different processing of the file accesses in the storage system - an alternative I/O path. We investigate a method to automatically determine the alternative I/O path and quantify the significance of knowledge about the internal processing. It is shown that the prediction error is improved significantly with this approach.",
      "axisX": "-0.15300837176018042",
      "axisY": "-0.08492759850942652",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160304",
      "title": "Analyzing Data Properties using Statistical Sampling \u2013 Illustrated on Scientific File Formats",
      "authors": "Julian Martin Kunkel",
      "keywords": "Analyzing Data Properties using Statistical Sampling \u2013 Illustrated on Scientific File FormatsScientific Data, Compression, Analyzing Data PropertiesUnderstanding the characteristics of data stored in data centers helps computer scientists in identifying the most suitable storage infrastructure to deal with these workloads. For example, knowing the relevance of file formats allows optimizing the relevant formats but also helps in a procurement to define benchmarks that cover these formats. Existing studies that investigate performance improvements and techniques for data reduction such as deduplication and compression operate on a subset of data. Some of those studies claim the selected data is representative and scale their result to the scale of the data center. One hurdle of running novel schemes on the complete data is the vast amount of data stored and, thus, the resources required to analyze the complete data set. Even if this would be feasible, the costs for running many of those experiments must be justified.This paper investigates stochastic sampling methods to compute and analyze quantities of interest on file numbers but also on the occupied storage space. It will be demonstrated that on our production system, scanning 1% of files and data volume is sufficient to deduct conclusions. This speeds up the analysis process and reduces costs of such studies significantly.",
      "axisX": "-0.35531235309226056",
      "axisY": "-0.3442153218866474",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160305",
      "title": "Spectral Domain Decomposition Using Local Fourier Basis: Application to Ultrasound Simulation on a Cluster of GPUs",
      "authors": "Jiri Jaros, Filip Vaverka, Bradley E. Treeby",
      "keywords": "Spectral Domain Decomposition Using Local Fourier Basis: Application to Ultrasound Simulation on a Cluster of GPUsdomain decomposition, ultrasound simulation, spectral methods, GPU, FFT, local\nFourier basisThe simulation of ultrasound wave propagation through biological tissue has a wide range of practical applications. However, large grid sizes are generally needed to capture the phenomena of interest. Here, a novel approach to reduce the computational complexity is presented. The model uses an accelerated k-space pseudospectral method which enables more than one hundred GPUs to be exploited to solve problems with more than 3*10^9 grid points. The classic communication bottleneck of Fourier spectral methods, all-to-all global data exchange, is overcome by the application of domain decomposition using local Fourier basis. Compared to global domain decomposition, for a grid size of 1536 x 1024 x 2048, this reduces the simulation time by a factor of 7.5 and the simulation cost by a factor of 3.8.",
      "axisX": "0.285572547339693",
      "axisY": "-0.2627926735905032",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160306",
      "title": "HCA aware Parallel Communication Library: A feasibility study for offloading MPI requirements",
      "authors": "Kedar Kulkarni, Shreeya Badhe, Geetanjali Gadre",
      "keywords": "HCA aware Parallel Communication Library: A feasibility study for offloading MPI requirementsMPI, HCA, Communication Pattern Offloading, High Performance Networks,\nCommunication LibraryMessage Passing Interface (MPI) is a standardized message passing system, independent of underlying network, and the most widely used parallel programming paradigm. The communication library should make full use of the Host Channel Adapter (HCA) characteristics to maximize performance of the HPC cluster. The communication library may not able to take full advantage of the underlying network adapter, if the library is made generalized. This can have a significant impact on the performance.Our primary goal is to develop a network dependent message passing library called a Parallel Communication Library (PCL) that will exploit C-DAC's proprietary PARAMNet HCA features for efficient message transfer. Using PCL, we intend to observe the feasibility of the network and performance enhancement for additional features. The objective is to carry out different trials by implementing additional features and analyze the implications which would give us more insight towards suitability of transport offload/onload mechanism. This experimentation would give us feedbacks for designing the next generation architecture.",
      "axisX": "-0.4391910429445331",
      "axisY": "0.16183952607617863",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160307",
      "title": "Parallel Processing Model for Cholesky Decomposition Algorithm in AlgoWiki Project",
      "authors": "Alexander S. Antonov, Alexey V. Frolov, Hiroaki Kobayashi, Igor N. Konshin, Alexey M. Teplov, Vadim V. Voevodin, Vladimir V. Voevodin",
      "keywords": "Parallel Processing Model for Cholesky Decomposition Algorithm in AlgoWiki ProjectAlgoWiki, algorithm properties, Cholesky decomposition, memory access locality,\ndynamic characteristics, scalabilityThe comprehensive analysis of algorithmic properties of well-known. Cholesky decomposition was performed on the basis of multifold AlgoWiki technologies. There was performed a detailed analysis of information graph, data structure, memory access profile, computation locality, scalability and other algorithm properties, that allow us to demonstrate a lot of unevident properties split up. into machine-independent and machine-dependent subsets. A comprehension of the parallel algorithm structure provide us with the possibility to efficiently implement the algorithm at hardware platform specified.",
      "axisX": "0.25460541273407433",
      "axisY": "0.37572538158437246",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160401",
      "title": "In Situ Exploration of Particle Simulations with CPU Ray Tracing",
      "authors": "Will Usher, Ingo Wald, Aaron Knoll, Michael Papka, Valerio Pascucci",
      "keywords": "In Situ Exploration of Particle Simulations with CPU Ray Tracingin situ rendering, parallel systems, point-based data, CPU and GPU clustersWe present a system for interactive in situ visualization of large particle simulations, suitable for general CPU-based HPC architectures. As simulations grow in scale, in situ methods are needed to alleviate IO bottlenecks and visualize data at full spatio-temporal resolution. We use a lightweight loosely-coupled layer serving distributed data from the simulation to a data-parallel renderer running in separate processes. Leveraging the OSPRay ray tracing framework for visualization and balanced P-k-d trees, we can render simulation data in real-time, as they arrive, with negligible memory overhead. This flexible solution allows users to perform exploratory in situ visualization on the same computational resources as the simulation code, on dedicated visualization clusters or remote workstations, via a standalone rendering client that can be connected or disconnected as needed.\u00a0 We evaluate this system on simulations with up to 227M particles in the LAMMPS and Uintah computational frameworks, and show that our approach provides many of the advantages of tightly-coupled systems, with the flexibility to render on a wide variety of remote and coprocessing resources.",
      "axisX": "0.171776434365887",
      "axisY": "0.6887420187961663",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160402",
      "title": "In Situ Visualization and Production of Extract Databases",
      "authors": "Brad Joseph Whitlock, Earl P. N. Duque",
      "keywords": "In Situ Visualization and Production of Extract DatabasesIn Situ, High Performance Computing, Visualization, Extract Database, SENSEI,\nLibsim, Workflow\u00a0Simulations running at high concurrency on HPC systems generate large volumes of data that are impractical to write to disk due to time and storage constraints. Applications often adapt by saving data infrequently, resulting in datasets with poor temporal resolution. This can make datasets difficult to interpret during post hoc visualization and analysis, or worse, it can lead to lost science. In Situ visualization and analysis can enable efficient production of small data products such as rendered images or surface extracts that consist of polygonal geometry plus fields. These data products are far smaller than their source data and can be processed much more economically in a traditional post hoc workflow using far fewer computational resources. We used the SENSEI and Libsim in situ infrastructures to implement rendering workflow and surface data extraction workflows in the AVF-LESLIE combustion code. These workflows were then demonstrated at high levels of concurrency and showed significant data reductions and limited impact on the simulation runtime. ",
      "axisX": "-0.09163309661539125",
      "axisY": "0.10555690981779363",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160403",
      "title": "In situ, steerable, hardware-independent and data-structure agnostic visualization with ISAAC",
      "authors": "Alexander Matthes, Axel Huebl, Ren\u00e9 Widera, Sebastian Grottel, Stefan Gumhold, Michael Bussmann",
      "keywords": "In situ, steerable, hardware-independent and data-structure agnostic visualization with ISAACHPC, in situ, visualization, live rendering, petascale, particle-in-cell,\nC++11, CUDA, Alpaka, FOSSThe computation power of supercomputers grows faster than the bandwidth of their storage and network. Especially applications using hardware accelerators like Nvidia GPUs cannot save enough data to be analyzed in a later step. There is a high risk of loosing important scientific information. We introduce the in situ template library ISAAC which enables arbitrary applications like scientific simulations to live visualize their data without the need of deep copy operations or data transformation using the very same compute node and hardware accelerator the data is already residing on. Arbitrary meta data can be added to the renderings and user defined steering commands can be asynchronously sent back to the running application. Using a aggregating server, ISAAC streams the interactive visualization video and enables user to access their applications from everywhere.",
      "axisX": "0.015074683071935189",
      "axisY": "0.5475388446485834",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160404",
      "title": "Preparing for In Situ Processing on Upcoming Leading-edge Supercomputers",
      "authors": "James Kress, Randy Michael Churchill, Scott Klasky, Mark Kim, Hank Childs, David Pugmire",
      "keywords": "Preparing for In Situ Processing on Upcoming Leading-edge SupercomputersScientific Visualization, In Situ Methods, Data Staging Methods, Data Reductions, High Performance ComputingHigh performance computing applications are producing increasingly large amounts of data\u00a0and placing enormous stress on current capabilities for traditional\u00a0post-hoc visualization techniques. Because of the growing compute\u00a0and I/O imbalance, data reductions, including in situ visualization,\u00a0are required. These reduced data are used for analysis and\u00a0visualization in a variety of different ways. Many of he\u00a0visualization and analysis requirements are known a priori, but when\u00a0they are not, scientists are dependent on the reduced data to\u00a0accurately represent the simulation in post hoc analysis.\u00a0The contributions of this paper is a description of the directions we are\u00a0pursuing to assist a large scale fusion simulation code succeed on\u00a0the next generation of supercomputers. These directions include\u00a0the role of in situ processing for performing data reductions, as well\u00a0as the tradeoffs between data size and data integrity within the context\u00a0of complex operations in a typical scientific workflow.",
      "axisX": "-0.3691631957093635",
      "axisY": "-0.13591438955175686",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160405",
      "title": "Analysis of CPU Usage Data Properties and their possible impact on Performance Monitoring",
      "authors": "Konstantin S. Stefanov, Alexey A. Gradskov",
      "keywords": "Analysis of CPU Usage Data Properties and their possible impact on Performance Monitoringperformance monitoring, sensor properties, sampling rate, CPU usage, CPU load\nlevelCPU usage data (CPU user, system, iowait etc. load levels) are often the basic data used for performance monitoring. The source of these data is\u00a0 the operating system. In this paper we analyze some properties of CPU usage data provided by Linux kernel. We examine kernel source code and provide test results to find which level of accuracy and precision one may expect when using CPU load level data.",
      "axisX": "-0.08110289289152564",
      "axisY": "0.06147039262128787",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160406",
      "title": "Parallel algorithm for 3D modeling of monochromatic acoustic field by the method of integral equations",
      "authors": "Mikhail S. Malovichko, Nikolay E. Khokhlov, Nikolay B. Yavich, Michael S. Zhdanov",
      "keywords": "Parallel algorithm for 3D modeling of monochromatic acoustic field by the method of integral equationsintegral equations, acoustics, seismics, MPI, OpenMPWe present a parallel algorithm for solution of the three-dimensional Helmholtz equation in the frequency domain by the method of volume integral equations. The algorithm is applied to seismic forward modeling. The method of integral equations reduces the size of the problem by dividing the geologic model into the anomalous and background parts, but leads to a dense system matrix. A tolerable memory consumption and numerical complexity were achieved by applying an iterative solver, accompanied by an effective matrix-vector multiplication operation, based on the fast Fourier transform. We used OpenMP to speed up the matrix-vector multiplication, while MPI was used to speed up the equation system solver, and also for parallelizing across multiple sources. Practical examples and efficiency tests are presented.",
      "axisX": "0.7597162202426101",
      "axisY": "-0.0665313158606902",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170101",
      "title": "Design and Implementation of the PULSAR Programming System for Large Scale Computing",
      "authors": "Jakub Kurzak, Piotr Luszczek, Ichitaro Yamazaki, Yves Robert, Jack Dongarra",
      "keywords": "Design and Implementation of the PULSAR Programming System for Large Scale Computingruntime scheduling, dataflow scheduling, distributed computing, massively parallel\ncomputing, multicore processors, hardware accelerators, virtualization, systolic arraysThe objective of the PULSAR project was to design a programming model suitable\u00a0for large scale machines with complex memory hierarchies, and to deliver a prototype\u00a0implementation of a runtime system supporting that model.\u00a0PULSAR tackled the challenge by proposing a programming model based on systolic\u00a0processing and virtualization.\u00a0The PULSAR programming model is quite simple, with point-to-point channels as the\u00a0main communication abstraction.\u00a0The runtime implementation is very lightweight and fully distributed,\u00a0and provides multithreading, message-passing and multi-GPU offload capabilities.\u00a0Performance evaluation shows good scalability up to one thousand nodes\u00a0with one thousand GPU accelerators.",
      "axisX": "0.1610969600389075",
      "axisY": "0.8392823014745753",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170102",
      "title": "Workflows for Science: a Challenge when Facing the Convergence of HPC and Big Data",
      "authors": "Rosa M Badia, Eduard Ayguade, Jesus Labarta",
      "keywords": "Workflows for Science: a Challenge when Facing the Convergence of HPC and Big Dataworkflows, scientific applications, Big DataWorkflows have been used traditionally as a mean to describe and implement the\u00a0computing usually parametric studies and explorations searching for the best solution\u00a0 that\u00a0 scientific researchers want to perform.\u00a0A workflow is not only the computing application, but a way of documenting a\u00a0process. \u00a0Science workflows may be of very different nature depending on the area of research, matching the actual experiment that the scientist want to perform.\u00a0Workflow Management Systems are environments that offer the researchers\u00a0tools to define, publish, execute and document their workflows.\u00a0In some cases, the science workflows are used to generate data; in other cases\u00a0are used to analyse existing data; only in a few cases, workflows are used both to generate and analyse\u00a0 data. The design of experiments is in some cases generated blindly, without a clear idea of which points are relevant to be computed/simulated, ending up with huge amount of computation that is performed following a brute-force strategy.\u00a0However, the evolution of systems and the large amount of data\u00a0generated by the applications require an in-situ analysis of the data, thus requiring new solutions to develop workflows that includes both the simulation/computational part and the analytic part. What is more, the fact that both components, computation and analytics, can be run together\u00a0 will enable the possibility of defining more dynamic workflows, with new computations being decided by the analytics in a more efficient way.The first part of the paper will review current approaches that a set of scientific communities follows in the development of their workflows. Due to the election of several scientific communities and use cases using a specific Workflow Management System, this survey maybe incomplete with regard a complete revision of the literature about workflows, but we expect that the reader appreaciates the effort performed in trying to see the scientific communities needs and requirements.\u00a0The second part of the paper will propose a new software architecture to develop a new\u00a0 family of end-to-end workflows that enables the management of\u00a0 dynamic workflows composed of simulations, analytics and visualization, including inputs/outputs from streams.",
      "axisX": "-0.582460443609747",
      "axisY": "-0.1888062094874615",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170103",
      "title": "A Survey: Runtime Software Systems for High Performance Computing",
      "authors": "Thomas Sterling, Matthew Anderson, Maciej Brodowicz",
      "keywords": "A Survey: Runtime Software Systems for High Performance Computingruntime system, parallel computing, scalability, survey, High Performance ComputingHPC system design and operation are challenged by the critical requirements for signicant advances in eciency, scalability, user productivity, and performance portability, even at the end of Moore's Law with approaching nano-scale semiconductor technology. Conventional practices employ distributed memory message passing programming interfaces, sometimes combining second level thread-based intra shared memory node interfaces such as OpenMP or with means of controlling heterogeneous components such as OpenCL for GPUs. While these methods include some modest runtime control, they are principally course grained and statically scheduled. Yet, performance for many real-world applications yield eciencies of less than 10% although some benchmarks may achieve 80% eciency or better (e.g., HPL). To address these challenges, strategies employing runtime software systems are being pursued to exploit information about the status of the application and the system hardware operation throughout the execution for purposes of introspection to guide the task scheduling and resource management in support of dynamic adaptive control. Runtime systems provide adaptive means to reduce the eects of starvation, latency, overhead, and contention. While each is unique in its details, many share common properties such as multi-tasking either preemptive or non-preemptive, message-driven computation such as active messages, sophisticated ne-grain synchronization such as dataow and futures contructs, global name or address spaces, and control policies for optimizing task scheduling in part to address the uncertainty of asynchrony. This survey will identify key parameters and properties of modern and sometimes experimental runtime systems actively employed today and provide a detailed description, summary, and comparison within a shared space of dimensions. It is not the intent of this paper to determine which is better or worse but rather to provide sucient detail to permit the reader to select among them according to individual need.\u00a0",
      "axisX": "-0.659706556677338",
      "axisY": "0.031034520344558243",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170104",
      "title": "xSDK Foundations: Toward an Extreme-scale Scientific Software Development Kit",
      "authors": "Roscoe Bartlett, Irina Demeshko, Todd Gamblin, Glenn Hammond, Michael Allen Heroux, Jeffrey Johnson, Alicia Klinvex, Xiaoye Li, Lois Curfman McInnes, J. David Moulton, Daniel Osei-Kuffuor, Jason Sarich, Barry Smith, James Willenbring, Ulrike Meier Yang",
      "keywords": "xSDK Foundations: Toward an Extreme-scale Scientific Software Development KitxSDK, Extreme-scale scientific software development kit, numerical libraries, software interoperability, sustainabilityExtreme-scale computational science increasingly demands multiscale and multiphysics formulations. Combining software developed by independent groups is imperative: no single team has resources for all predictive science and decision support capabilities. Scientific libraries provide high-quality, reusable software components for constructing applications with improved robustness and portability.\u00a0 However, without coordination, many libraries cannot be easily composed.\u00a0 Namespace collisions, inconsistent arguments, lack of third-party software versioning, and additional difficulties make composition costly.The Extreme-scale Scientific Software Development Kit (xSDK) defines community policies to improve code quality and compatibility across independently developed packages (hypre, PETSc, SuperLU, Trilinos, and Alquimia) and provides a foundation for addressing broader issues in software interoperability, performance portability, and sustainability.\u00a0 The xSDK provides turnkey installation of member software and seamless combination of aggregate capabilities, and it marks first steps toward extreme-scale scientific software ecosystems from which future applications can be composed rapidly with assured quality and scalability.",
      "axisX": "-0.5463646441286657",
      "axisY": "0.279067369921116",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170105",
      "title": "Performance Portability of HPC Discovery Science Software: Fusion Energy Turbulence Simulations at Extreme Scale",
      "authors": "William Tang, Bei Wang, Stephane Ethier, Zhihong Lin",
      "keywords": "Performance Portability of HPC Discovery Science Software: Fusion Energy Turbulence Simulations at Extreme ScaleTurbulence\nSimulations, Particle-In-Cell, Portability, HPCAs HPC R&D moves forward on a variety of \u201cpath to exascale\u201d architectures today, an associated objective is to demonstrate performance portability of discovery-science-capable software.\u00a0 Important application domains, such as Magnetic Fusion Energy (MFE), have improved modelling of increasingly complex physical systems -- especially with respect to reducing \u201ctime-to-solution\u201d as well as\u00a0 \u201cenergy to solution.\u201d\u00a0 The emergence of new insights on confinement scaling in MFE systems has been aided significantly by efficient software capable of harnessing powerful supercomputers to carry out simulations with unprecedented resolution and temporal duration to address increasing problem sizes.\u00a0 Specifically, highly scalable particle-in-cell (PIC) programing methodology is used in this paper to demonstrate how modern\u00a0scientific applications can achieve efficient architecture-dependent optimizations of performance scaling and code portability for path-to-exascale platforms.",
      "axisX": "-0.3292581673266861",
      "axisY": "0.054094914966143874",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170201",
      "title": "Using High Performance Computing to Create and Freely Distribute the South Asian Genomic Database, Necessary for Precision Medicine in this Population",
      "authors": "Asmi H. Shah, Jonathan D. Picker, Saumya S. Jamuar",
      "keywords": "Using High Performance Computing to Create and Freely Distribute the South Asian Genomic Database, Necessary for Precision Medicine in this PopulationPrecision Medicine, ggcINDIA, Beacon Network, South Asian Genome, Genome\nData SharingPrecision medicine is an emerging approach for disease treatment and prevention that takes into account individual variability in genes, environment, and lifestyle for each person\u201d. Efforts to implement precision medicine have gained traction in recent years due to significantly increased understanding of the role of genetic variations in human disease over the past decade. However, delivery of precision medicine requires robust population specific reference genome datasets for full appreciation of existing natural variation. The majority of publicly available genomic databases are primarily derived from Caucasian populations and do not fully address the diversity of Asian populations. In an effort to address this problem, we have aggregated and built a genomic database, ggcINDIA, specifically for South Asian populations. In collaboration with Global Alliance for Genomics and Health (GA4GH), we have made this database publicly available to the community through the GA4GH's Beacon project. ggcINDIA represents the first Beacon for South Asian populations. As more data is generated and aggregated, the ggcINDIA beacon will provide the precise genomic data that is critical to the delivery of precision medicine within South Asia.",
      "axisX": "-0.5729209123299398",
      "axisY": "-0.37931760139461196",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170202",
      "title": "An Application of GPU Acceleration in CFD Simulation for Insect Flight",
      "authors": "Yang Yao, Khoon-Seng Yeo",
      "keywords": "An Application of GPU Acceleration in CFD Simulation for Insect Flightinsect flight, free flight, general-purpose computing on graphics processing units\n(GPGPU), computational fluid dynamics (CFD), flapping-wing aerodynamicsThe mobility and maneuverability of winged insects have been attracting attention, but the knowledge on the behavior of free-flying insects is still far from complete. This paper presents a computational study on the aerodynamics and kinematics of a free-flying model fruit-fly. An existing integrative computational fluid dynamics (CFD) framework was further developed using CUDA technology and adapted for the free flight simulation on heterogenous clusters.The application of general-purpose computing on graphics processing units (GPGPU) significantly accelerated the insect flight simulation and made it less computational expensive to find out the steady state of the flight using CFD approach.A variety of free flight scenarios has been simulated using the present numerical approach, including hovering, fast rectilinear flight, and complex maneuvers. The vortical flow surrounding the model fly in steady flight was visualized and analyzed. The present results showed good consistency with previous studies.",
      "axisX": "0.10609051308605563",
      "axisY": "0.03662769114374323",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170203",
      "title": "Simultac Fonton: A Fine-Grain Architecture for Extreme Performance beyond Moore's Law",
      "authors": "Maciej Brodowicz, Thomas Sterling",
      "keywords": "Simultac Fonton: A Fine-Grain Architecture for Extreme Performance beyond Moore's LawHigh-Performance Computing, parallel processing, exascale, non-von Neumann architecture, cellular architectureWith nano-scale technology and Moore's Law end, architecture advance serves as the principal means of achieving enhanced efficiency and scalability into the exascale era. Ironically, the field that has demonstrated the greatest leaps of technology in the history of humankind, has retained its roots in its earliest strategy, the von Neumann architecture model which has imposed tradeoffs no longer valid for today's semiconductor technologies, although they were suitable through the 1980s. Essentially all commercial computers, including HPC, have been and are von Neumann derivatives. The bottlenecks imposed by this heritage are the emphasis on ALU/FPU utilization, single instruction issue and sequential consistency, and the separation of memory and processing logic (\"von Neumann bottleneck\"). Here the authors explore the possibility and implications of one class of non von Neumann architecture based on cellular structures, asynchronous multi-tasking, distributed shared memory, and message-driven computation. \"Continuum Computer Architecture\" is introduced as a genus of ultra-fine-grained architectures where complexity of operation is an emergent behavior of simplicity of design combined with highly replicated elements. An exemplar species of CCA, \"Simultac\" is considered comprising billions of simple elements, \"fontons\", of merged properties of data storage and movement combined with logical transformations. Employing the ParalleX execution model and a variation of the HPX+ runtime system software, the Simultac may provide the path to cost effective data analytics and machine learning as well as dynamic adaptive simulations in the trans-exaOPS performance regime.",
      "axisX": "-0.18029006833614683",
      "axisY": "0.11217333885622849",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170204",
      "title": "The Simultaneous Transmit And Receive (STAR) Message Protocol",
      "authors": "Earle Jennings",
      "keywords": "The Simultaneous Transmit And Receive (STAR) Message ProtocolMPI, optical communications, exascale, HPC, security, router, memory wall, fault\nresilienceThe STAR protocol is introduced, which solves three problems with MPI, a well known secur- ity problem, and three exascale communication problems. Optical implementations are developed compatible with 100 Gbit/sec Ethernet. Automatic fault resilience mechanisms are discussed, which improve HPC quality of service, and meet the exascale reliability and resilience challenges. The bandwidth problem for exascale computers interfacing with data centers is solved. The STAR protocol is combined with the SiMulPro core architecture (discussed in another article of this issue). The combination enables data centers, handheld computers, networked sensors, and super- computers, to be invulnerable to memory fault injection of viruses and rootkits. ",
      "axisX": "-0.10671874211096664",
      "axisY": "0.537775619257088",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170205",
      "title": "Core Module Optimizing PDE Sparse Matrix Models With HPCG Example",
      "authors": "Earle Jennings",
      "keywords": "Core Module Optimizing PDE Sparse Matrix Models With HPCG ExampleHPCG, superscalar, sparse matrix, partial differential equation, memory wall, HPC\u00a0This paper introduces a fundamentally new computer architecture for supercomputers. The core module is application compatible with an existing superscalar microprocessor, with minimized energy use, and is optimized for local sparse matrix operations. Optimized sparse matrix manip- ulation is discussed by analyzing the High Performance Conjugate Gradient (HPCG) benchmark speci...cation. This analysis shows how the DRAM memory wall is removed for this benchmark, and for sparse matrix models of partial di\u00a4erential equations (PDEs) for a wide cross section of applications. By giving the programmer improved control over the con...guration of the super- computer, the potential for communication problems is minimized. Application compatibility is achieved while removing the superscalar instruction interpreter and multi-thread controller from the existing microprocessor\u2019s hardware. These are transformed into compile-time utilities. The instruction cache is removed through an innovation in VLIW instruction processing. The data caches are unnecessary and are turned o\u00a4 in order to optimally implement sparse matrix models. ",
      "axisX": "0.28405864285694044",
      "axisY": "0.6973914145852035",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170206",
      "title": "Beating Floating Point at its Own Game: Posit Arithmetic",
      "authors": "John L. Gustafson, Isaac T. Yonemoto",
      "keywords": "Beating Floating Point at its Own Game: Posit Arithmeticcomputer arithmetic, energy-efficient computing, floating point, posits, LINPACK,\nlinear algebra, neural networks, unum computing, valid arithmeticA new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers (floats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and \u201cNot-a-Number\u201d (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality. A comprehensive series of benchmarks compares floats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than \u201capproximate computing\u201d methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than floats of the same size; in some cases, a 32-bit posit may safely replace a 64-bit float. In other words, posits beat floats at their own game.\u00a0",
      "axisX": "0.14220750699631637",
      "axisY": "0.007846872625624267",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170301",
      "title": "Resilience Design Patterns: A Structured Approach to Resilience at Extreme Scale",
      "authors": "Saurabh Hukerikar, Christian Engelmann",
      "keywords": "Resilience Design Patterns: A Structured Approach to Resilience at Extreme Scalehigh-performance computing, resilience, fault tolerance, design patternsReliability is a serious concern for future extreme-scale high-performance computing (HPC) systems. Projections based on the current generation of HPC systems and technology roadmaps suggest the prevalence of very high fault rates in future systems. The errors resulting from these faults will propagate and generate various kinds of failures, which may result in outcomes ranging from result corruptions to catastrophic application crashes. Therefore, the resilience challenge for extreme-scale HPC systems requires management of various hardware and software technologies that are capable of handling a broad set of fault models at accelerated fault rates. Also, due to practical limits on power consumption in HPC systems future systems are likely to embrace innovative architectures, increasing the levels of hardware and software complexities. As a result, the techniques that seek to improve resilience must navigate the complex trade-off space between resilience and the overheads to power consumption and performance. While the HPC community has developed various resilience solutions, application-level techniques as well as system-based solutions, the solution space of HPC resilience techniques remains fragmented. There are no formal methods and metrics to investigate and evaluate resilience holistically in HPC systems that consider impact scope, handling coverage, and performance & power efficiency across the system stack. Additionally, few of the current approaches are portable to newer architectures and software environments that will be deployed on future systems.In this paper, we develop a structured approach to the management of HPC resilience using the concept of resilience-based design patterns. A design pattern is a general repeatable solution to a commonly occurring problem. We identify the commonly occurring problems and solutions used to deal with faults, errors and failures in HPC systems. Each established solution is described in the form of a pattern that addresses concrete problems in the design of resilient systems. The complete catalog of resilience design patterns provides designers with reusable design elements. We also define a framework that enhances a designer's understanding of the important constraints and opportunities for the design patterns to be implemented and deployed at various layers of the system stack. This design framework may be used to establish mechanisms and interfaces to coordinate flexible fault management across hardware and software components. The framework also supports optimization of the cost-benefit trade-offs among performance, resilience, and power consumption. The overall goal of this work is to enable a systematic methodology for the design and evaluation of resilience technologies in extreme-scale HPC systems that keep scientific applications running to a correct solution in a timely and cost-efficient manner despite frequent faults, errors, and failures of various types.",
      "axisX": "-0.5738349879186191",
      "axisY": "-0.2742869645158511",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170302",
      "title": "Performance Evaluation of Runtime Data Exploration Framework based on In-Situ Particle Based Volume Rendering",
      "authors": "Takuma Kawamura, Tomoyuki Noda, Yasuhiro Idomura",
      "keywords": "Performance Evaluation of Runtime Data Exploration Framework based on In-Situ Particle Based Volume Renderingin-situ visualization, volume rendering, runtime steering, strong scaling, performance evaluationWe examine the performance of the in-situ data exploration framework based on the in-situ Particle Based Volume Rendering (In-Situ PBVR) on the latest many-core platform. In-Situ PBVR converts extreme scale volume data into small rendering primitive particle data via parallel Monte-Carlo sampling without costly visibility ordering. This feature avoids severe bottlenecks such as limited memory size per node and significant performance gap between computation and inter-node communication. In addition, remote in-situ data exploration is enabled by asynchronous file-based control sequences, which transfer the small particle data to client PCs, generate view-independent volume rendering images on client PCs, and change visualization parameters at runtime.In-Situ PBVR shows excellent strong scaling with low memory usage up to ~100k cores on the Oakforest-PACS, which consists of 8,208 Intel Xeon Phi7250 (Knights Landing) processors. This performance is compatible with the remote in-situ data exploration capability.",
      "axisX": "0.2044137841325735",
      "axisY": "0.45515920191214104",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170303",
      "title": "Development and Integration of an In-Situ Framework for Flow Visualization of Large-Scale, Unsteady Phenomena in ICON",
      "authors": "Michael Vetter, Stephan Olbrich",
      "keywords": "Development and Integration of an In-Situ Framework for Flow Visualization of Large-Scale, Unsteady Phenomena in ICONDSVR, ICON, in-situ visualization, visualization frameworkWith large-scale simulation models on massively parallel supercomputers generating increasingly large data sets, in-situ visualization is a promising way to avoid bottlenecks. Enabling in-situ visualization in a simulation model asks for special attention to the interface between a parallel simulation model and the data analysis part of the visualization, and to presentation and interaction scenarios. Modifications to scientific workflows would potentially result in a paradigm shift, which affects compute and data intensive applications generally. We present our approach for enabling in-situ visualization within the highly parallelized climate model ICON using the DSVR visualization framework. We focus on the requirements for generalized grid and data structures, and for universal, scalable algorithms for volume and flow visualization of time series. In-situ pathline extraction as a technique for the visualization of unsteady flows has been integrated in the climate simulation model ICON and verified in first studies.",
      "axisX": "-0.05162645235688045",
      "axisY": "0.10249196597435106",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170304",
      "title": "In Situ Visualization for 3D Agent-Based Vocal Fold Inflammation and Repair Simulation",
      "authors": "Nuttiiya Seekhao, Joseph JaJa, Luc Mongeau, Nicole Y.K. Li-Jessen",
      "keywords": "In Situ Visualization for 3D Agent-Based Vocal Fold Inflammation and Repair Simulationin situ, visualization, vocal fold, systems biology simulation, agent based modeling,\ntissue inflammation and repair, computational steeringA fast and insightful visualization is essential in modeling biological system behaviors and understanding underlying inter-cellular mechanisms. High fidelity models produce billions of data points per time step, making in situ visualization techniques extremely desirable as they mitigate I/O bottlenecks and provide computational steering capability. In this work, we present a novel high-performance scheme to couple in situ visualization with the simulation of the vocal fold inflammation and repair using little to no extra cost in execution time or computing resources. The visualization component is first optimized with an adaptive sampling scheme to accelerate the rendering process while maintaining the precision of the displayed visual results. Our software employs VirtualGL to perform visualization in situ. The scheme overlaps visualization and simulation, resulting in the optimal utilization of computing resources. This results in an in situ system biology simulation suite capable of remote simulation of 17 million biological cells and 1.2 billion chemical data points, remote visualization of the results, and delivery of visualized frames with aggregated statistics to remote clients in real-time.",
      "axisX": "0.03536434754479602",
      "axisY": "0.1917790725478063",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170305",
      "title": "Seismic Processing Performance Analysis on Different Hardware Environment",
      "authors": "Ekaterina Olegovna Tyutlyaeva, Sergey Konyukhov, Igor Odintsov, Alexander Moskovsky",
      "keywords": "Seismic Processing Performance Analysis on Different Hardware EnvironmentPerformance analysis, architecture comparison, seismic processing profiling, powerusage analysis, application behavior analysisIn this research we have used computational-intensive software that implements 2D and 3D seismic migrations to study mini-application behavior for a set of the computational architectures. In addition to three architecture type comparative analysis, two CPU generation comparisons have been done.The dynamic behavior of chosen mini-applications was studied using BSC performance analysis tools to identify their common features. In summary, we observe the best performance of mini-applications on Intel Xeon E5-2698 CPU generation 4.\u00a0 Intel Xeon Phi 7250 peculiar architectural characteristics requires careful source code optimizations to help the compiler to effectively vectorize time-consuming loops and to improve the cache locality in order to achieve higher performance level. Elbrus-4S CPU is theoretically suitable for such kind of applications, but currently observed performance is an order of magnitude less than on Xeon E5 family, we believe that the frequency and RAM bandwidth increasing, as well as source code optimization work could improve it\u2019s performance.",
      "axisX": "-0.02355022975999057",
      "axisY": "0.2676538880213858",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170306",
      "title": "Exploring Scheduling Effects on Task Performance with TaskInsight",
      "authors": "Germ\u00e1n Ceballos, Andra Hugo, Erik Hagersten, David Black-Schaffer",
      "keywords": "Exploring Scheduling Effects on Task Performance with TaskInsighttask-based scheduling, data reuse, data locality, cache modelThe complex memory hierarchies of nowadays machines make it very difficult to estimate the execution time of the tasks as depending on where the data is placed in memory, tasks of the same type may end up having different performance. Multiple scheduling heuristics have managed to improve performance by taking into account memory-related properties such as data locality and cache sharing. However, we may see tasks in certain applications or phases of applications that take little or no advantage of these optimizations. Without understanding when such optimizations are effective, we may trigger unnecessary overhead at runtime level.In previous work, we introduced TaskInsight, a technique to characterize how the memory behavior of the application is affected by different task schedulers through the analysis of data reuse across tasks. We now use this tool to dynamically trace the scheduling decisions of multithreaded applications through their execution and analyze how memory reuse can provide information on when and why locality-aware optimizations are effective and impact performance.We demonstrate how we can detect particular scheduling decisions that produced a variation in performance, and the underlying reasons when applying TaskInsight to several of the Montblanc benchmarks. This flexible insight is key both for the programmer and runtime to allow assigning the optimal scheduling policy to certain executions or phases.",
      "axisX": "-0.31945305172581273",
      "axisY": "-0.2898941218088498",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170307",
      "title": "From Processing-in-Memory to Processing-in-Storage",
      "authors": "Roman Kaplan, Leonid Yavits, Ran Ginosar",
      "keywords": "From Processing-in-Memory to Processing-in-StorageContent Addressable Memory, Associative Processing, In-Storage Processing, MemristorsNear-data in-memory processing research has been gaining momentum in recent years. Typical processing-in-memory architecture places a single or several processing elements next to a volatile memory, enabling processing without transferring data to the host CPU. The increased bandwidth to and from volatile memory leads to performance gain. However processing-in-memory does not alleviate von Neumann bottleneck for big data problems, where datasets are too large to fit in main memory.   We present a novel processing-in-storage system based on Resistive Content Addressable Memory (ReCAM). It functions simultaneously as a mass storage and as a massively parallel associative processor. ReCAM processing-in-storage resolves the bandwidth wall by keeping computation inside the storage arrays, without transferring it up the memory hierarchy.   We show that ReCAM based processing-in-storage architecture may outperform existing processing-in-memory and accelerator based designs. ReCAM processing-in-storage implementation of Smith-Waterman DNA sequence alignment reaches a speedup of almost five over a GPU cluster. An implementation of in-storage inline data deduplication is presented and shown to achieve orders of magnitude higher throughput than traditional CPU and DRAM based systems.",
      "axisX": "0.05901375245749157",
      "axisY": "-0.04507893767978684",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170401",
      "title": "Towards A Data Centric System Architecture: SHARP",
      "authors": "Richard Graham, Gil Bloch, Devendar Bureddy, Gilad Shainer, Brian Smith",
      "keywords": "Towards A Data Centric System Architecture: SHARPdata centric architecture, SHARP, collectives, MPIIncreased system size and a greater reliance on utilizing system parallelism to achieve computational needs, requires innovative system architectures to meet the simulation challenges. The SHARP technology is a step towards a data-centric architecture, where data is manipulated throughout the system. This paper introduces a new SHARP optimization, and studies aspects that impact application performance in a data-centric environment. The use of UD-Multicast to distribute aggregation results is introduced, reducing the latency of an eight-byte MPI Allreduce() across 128 nodes by 16%. Use of reduction trees that avoid the inter-socket bus further improves the eight-byte MPI Allreduce() latency across 128 nodes, with 28 processes per node, by 18%. The distribution of latency across processes in the communicator is studied, as is the capacity of the system to process concurrent aggregation operations.",
      "axisX": "-0.38790521263002925",
      "axisY": "0.03332177290055742",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170402",
      "title": "Towards Decoupling the Selection of Compression Algorithms from Quality Constraints \u2013 An Investigation of Lossy Compression Efficiency",
      "authors": "Julian Martin Kunkel, Anastasiia Novikova, Eugen Betke",
      "keywords": "Towards Decoupling the Selection of Compression Algorithms from Quality Constraints \u2013 An Investigation of Lossy Compression Efficiencydata reduction, compression, lossy, climate dataData intense scientific domains use data compression to reduce the storage space needed. Lossless data compression preserves information accurately but lossy data compression can achieve much higher compression rates depending on the tolerable error margins. There are many ways of defining precision and to exploit this knowledge, therefore, the field of lossy compression is subject to active research. From the perspective of a scientist, the qualitative definition about the implied loss of data precision should only matter.With the Scientific Compression Library (SCIL), we are developing a meta-compressor that allows users to define various quantities for acceptable error and expected performance behavior. The library then picks a suitable chain of algorithms yielding the user\u2019s requirements, the ongoing work is a preliminary stage for the design of an adaptive selector. This approach is a crucial step towards a scientifically safe use of much-needed lossy data compression, because it disentangles the tasks of determining scientific characteristics of tolerable noise, from the task of determining an optimal compression strategy. Future algorithms can be used without changing application code.In this paper, we evaluate various lossy compression algorithms for compressing different scientific datasets (Isabel, ECHAM6), and focus on the analysis of synthetically created data that serves as blueprint for many observed datasets. We also briefly describe the available quantitiesof SCIL to define data precision and introduce two efficient compression algorithms for individualdata points. This shows that the best algorithm depends on user settings and data properties.",
      "axisX": "-0.31589778778998784",
      "axisY": "-0.4857173486201815",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170403",
      "title": "Adaptive Load Balancing Dashboard in Dynamic Distributed Systems",
      "authors": "Seyedeh Leili Mirtaheri, Seyed Arman Fatemi, Lucio Grandinetti",
      "keywords": "Adaptive Load Balancing Dashboard in Dynamic Distributed SystemsLoad Balancing, Resource Management, Dynamic Variables, Communication Delay, Distributed SystemConsidering the dynamic nature of new generation scientific problems, load balancing is a necessity to manage the load in an efficient manner. Load balancing systems are use to optimize the resource consumption, maximize the throughput, minimize response time, and to prevent overload in resources. In current research, we consider operational distributed systems with dynamic variables caused by different nature of the applications and heterogeneity of the various levels in the system. Conducted studies indicate that many different factors should be considered to select the load balancing algorithm, including the processing power, load transfer and communication delay of nodes. In this work, We aim to design a dashboard that is capable to merge the load balancing algorithms in different environments. We design an adaptive system infrastructure with the ability to adjust various factors in the run time of a load balancing algorithm. We propose a task and a resource allocation mechanism and further introduce a mathematical model of load balancing process in the system. We calculate a normalized hardware score that determines the maturity of system according to the environmental conditions of the load balancing process. Evaluation results confirm that the proposed method performs well and reduces the probability of system failure.",
      "axisX": "-0.301807769957745",
      "axisY": "-0.19909090082711153",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170404",
      "title": "Additivity: A Selection Criterion for Performance Events for Reliable Energy Predictive Modeling",
      "authors": "Arsalan Shahid, Muhammad Fahad, Ravi Reddy, Alexey Lastovetsky",
      "keywords": "Additivity: A Selection Criterion for Performance Events for Reliable Energy Predictive Modelingperformance events, PMC, energy predictive models, Likwid, PAPIPerformance events or performance monitoring counters (PMCs) are now the dominant predictor variables for modeling energy consumption. Modern hardware processors provide a large set of PMCs. Determination of the best subset of PMCs for energy predictive modeling is a non-trivial task given the fact that all the PMCs can not be determined using a single application run. Several techniques have been devised to address this challenge. While some techniques are based on a statistical methodology, some use expert advice to pick a subset (that may not necessarily be obtained in one application run) that, in experts' opinion, are significant contributors to energy consumption. However, the existing techniques have not considered a fundamental property of predictor variables that should have been applied in the first place to remove PMCs unfit for modeling energy. We address this oversight in this paper. We propose a novel selection criterion for PMCs called additivity, which can be used to determine the subset of PMCs that can potentially be used for reliable energy predictive modeling. It is based on the experimental observation that the energy consumption of a serial execution of two applications is the sum of energy consumptions observed for the individual execution of each application. A linear predictive energy model is consistent if and only if its predictor variables are additive in the sense that the vector of predictor variables for a serial execution of two applications is the sum of vectors for the individual execution of each application. The criterion, therefore, is based on a simple and intuitive rule that the value of a PMC for a serial execution of two applications is equal to the sum of its values obtained for the individual execution of each application. The PMC is branded as non-additive on a platform if there exists an application for which the calculated value differs significantly from the value observed for the application execution on the platform. The use of non-additive PMCs in a model renders it inconsistent. We study the additivity of PMCs offered by the popular state-of-the-art tools, Likwid and PAPI, by employing a detailed experimental methodology on a modern Intel Haswell multicore server CPU. We show that many PMCs in Likwid and PAPI that are widely used in models as key predictor variables are non-additive. This brings into question the reliability and the reported prediction accuracy of these models.",
      "axisX": "-0.17687424085509304",
      "axisY": "-0.09177156303247967",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi170405",
      "title": "Cloud Service for Solution of Promising Problems of Nanotechnology",
      "authors": "Marina A. Kornilina, Viktoriia O. Podryga, Sergey V. Polyakov, Dmitry V. Puzyrkov, Mikhail V. Yakoboskiy",
      "keywords": "Cloud Service for Solution of Promising Problems of Nanotechnologycloud service, virtualization, supercomputer modeling in nanotechnology problems,\nvisualizationThe paper presents the problem of creating a cloud service designed to solve promising nanotechnology problems on supercomputer systems. The motivation for creating such a service was the need to integrate ideas, knowledge and computing technologies related to this applied problem, as well as the need to involve specialists in solving problems of this class. The intermediate result of the work is a prototype of the cloud environment, implemented as a KIAM Multilogin service and an application software accessible from users virtual machines. The first applications of the service were the software packages GIMM_NANO and Flow_and_Particles, designed to solve the actual problems of nanoelectronics, laser nanotechnology, multiscale problems of applied gas dynamics. The implementation of the service took into account such aspects as support for parallel computations on the park of remote supercomputers, improving the efficiency of parallelization, very large data sets processing, visualization of supercomputer modeling results. With the help of the implemented service, it was possible to optimize the process of solving the applied problems associated with calculating the parameters of gas-dynamic flows in the microchannels of industrial spraying systems. In particular, it was possible to carry out the series of studies devoted to the analysis of gas-dynamic processes at the gas-metal boundary. In these studies it was shown that in the presence of microcapillaries in a technical system, it is necessary to use direct modeling of gas dynamic processes on the basis of the first principles in the Knudsen layers, for example, using molecular dynamics methods.",
      "axisX": "-0.27161111770494567",
      "axisY": "0.30876494292045104",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180101",
      "title": "AlgoWiki Project as an Extension of the Top500 Methodology",
      "authors": "Alexander S Antonov, Jack Dongarra, Vladimir Voevodin",
      "keywords": "AlgoWiki Project as an Extension of the Top500 MethodologyAlgoWiki, parallel structure, algorithm\u2019s properties, Top500 methodology, problems,\nmethods, algorithms, implementations, computing platformsThe AlgoWiki project is dedicated to describing the parallel structure and key features of various algorithms. The descriptions are intended to provide complete information about algorithm's properties, which are needed to adequately assess their implementation efficiency for any computing platform. This work sets out the key areas for further development of the project which were recently developed based on working with the AlgoWiki encyclopedia. We are suggesting an approach to extend the Top500 methodology, which is commonly used to compare various computing platforms.",
      "axisX": "-0.459568266258808",
      "axisY": "0.1811301205461419",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180102",
      "title": "Record-and-Replay Techniques for HPC Systems: A Survey",
      "authors": "Dylan Chapp, Kento Sato, Dong H Ahn, Michela Taufer",
      "keywords": "Record-and-Replay Techniques for HPC Systems: A Surveyreproducibility, nondeterminism, fault-tolerance, exascale, message-passing, shared\nmemory, proxy application, HPC benchmarksRecord-and-replay techniques provide the ability to record executions of\u00a0nondeterministic applications and re-execute them identically. These techniques\u00a0find use in the contexts of debugging, reproducibility, and fault-tolerance,\u00a0especially in the presence of nondeterministic factors such as message races.\u00a0Record-and-replay techniques are highly diverse in terms of the fidelity\u00a0of replay they provide, the assumptions they make about the recorded\u00a0application, the programming models they target, and the runtime overheads they\u00a0impose. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0In the high performance computing (HPC) environment, all the above factors must\u00a0be considered in concert, thus presenting additional implementation challenges.\u00a0In this manuscript, we survey record-and-replay techniques in terms of the\u00a0programming models they target and the workloads on which they were evaluated,\u00a0providing a categorization of these techniques benefiting application developers\u00a0and researchers targeting exascale challenges.\u00a0This manuscript answers three questions through this survey:\u00a0What are the gaps in the\u00a0existing space of record-and-replay techniques? What is the roadmap to\u00a0widespread use of record-and-replay on production-scale HPC workloads? And, what are the critical open problems that must be addressed to make\u00a0record-and-replay viable at exascale? \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0Keywords:\u00a0Reproducibility, nondeterminism, fault-tolerance, exascale, message-passing, shared memory, proxy application, HPC benchmarks",
      "axisX": "-0.5201916506510385",
      "axisY": "-0.0030967048596627204",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180103",
      "title": "Survey of Storage Systems for High-Performance Computing",
      "authors": "Jakob L\u00fcttgau, Michael Kuhn, Kira Duwe, Yevhen Alforov, Eugen Betke, Julian Kunkel, Thomas Ludwig",
      "keywords": "Survey of Storage Systems for High-Performance Computingstorage hierarchy, file system, storage system, I/O interface, data formatIn current supercomputers, storage is typically provided by parallel distributed file systems for hot data and tape archives for cold data. These file systems are often compatible with local file systems due to their use of the POSIX interface and semantics, which eases development and debugging because applications can easily run both on workstations and supercomputers. There is a wide variety of file systems to choose from, each tuned for different use cases and implementing different optimizations. However, the overall application performance is often held back by I/O bottlenecks due to insufficient performance of file systems or I/O libraries for highly parallel workloads. Performance problems are dealt with using novel storage hardware technologies as well as alternative I/O semantics and interfaces. These approaches have to be integrated into the storage stack seamlessly to make them convenient to use. Upcoming storage systems abandon the traditional POSIX interface and semantics in favor of alternative concepts such as object and key-value storage; moreover, they heavily rely on technologies such as NVM and burst buffers to improve performance. Additional tiers of storage hardware will increase the importance of hierarchical storage management. Many of these changes will be disruptive and require application developers to rethink their approaches to data management and I/O. A thorough understanding of today's storage infrastructures, including their strengths and weaknesses, is crucially important for designing and implementing scalable storage systems suitable for demands of exascale computing.",
      "axisX": "-0.503413169159464",
      "axisY": "0.4568186153806713",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180104",
      "title": "The High-Q Club: Experience with Extreme-scaling Application Codes",
      "authors": "Dirk Br\u00f6mmel, Wolfgang Frings, Brian J. N. Wylie, Bernd Mohr, Paul Gibbon, Thomas Lippert",
      "keywords": "The High-Q Club: Experience with Extreme-scaling Application CodesJUQUEEN , IBM Blue Gene/Q, extreme scaling, application codes, High-Q ClubJ\u00fclich Supercomputing Centre (JSC) started running (extreme) scaling workshops with its first IBM Blue Gene supercomputer, finally spanning three generations each seeing an increase in the number of cores and available threads.  Over the years, this workshop series attracted numerous international code teams and resulted in many applications capable of running on all available cores of each system.This article reviews some of the knowledge gained with running and tuning highly-scalable applications, focussing on JUQUEEN, the IBM Blue Gene/Q at JSC. The ability to execute successfully on all 458752 cores with up to 1.8 million processes or threads may qualify codes for the High-Q Club, which serves as a showcase for diverse codes scaling to the entire 28 racks, effectively defining a collection of the highest scaling codes on JUQUEEN. The intention was to encourage other developers to invest in tuning and scaling their codes while identifying the necessary key aspects for that goal.As this era closes, it is timely to compare the characteristics of the 32 High-Q Club member codes, considering their strong and/or weak scaling, exploitation of hardware threading, and whether/how intra-node multi-threading is employed combined with message-passing.  We also identify the obstacles for scaling such as inefficient use of limited compute node memory and file I/O as key governing factors. Overall, the analysis provides guidance as to how applications may (need to) be designed in future to exploit expected exa-scale computer systems.",
      "axisX": "-0.5948846249113197",
      "axisY": "0.02189285263898827",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180105",
      "title": "Exploiting the Performance Benefits of Storage Class Memory for HPC and HPDA Workflows",
      "authors": "Michele Weiland, Adrian Jackson, Nick Johnson, Mark Parsons",
      "keywords": "Exploiting the Performance Benefits of Storage Class Memory for HPC and HPDA WorkflowsNVRAM, 3D XPoint, SCM, workflows, resource schedulingByte-addressable storage class memory (SCM) is an upcoming technology that will transform the memory and storage hierarchy of HPC systems by dramatically reducing the latency gap between DRAM and persistent storage. In this paper, we discuss general SCM characteristics, including the different hardware configurations and data access mechanisms SCM is likely to provide. We outline the performance challenges I/O requirements place on traditional scientific workflows and present how data access through SCM can have a beneficial impact on the performance of such workflows, in particular those with large scale data dependencies. We describe the system software components that are required to enabled workflow and data aware resource allocation scheduling in order to optimise both system throughput and time to solution for individual applications; these include a data scheduler and data movers. We also present an illustration of the performance improvement potential of the technology, based on initial workflow performance benchmarks with I/O dependencies.",
      "axisX": "-0.35643190611916686",
      "axisY": "0.25477979724800004",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180106",
      "title": "A General Guide to Applying Machine Learning to Computer Architecture",
      "authors": "Daniel Nemirovsky, Tugberk Arkose, Nikola Markovic, Mario Nemirovsky, Osman Unsal, Adrian Cristal, Mateo Valero",
      "keywords": "A General Guide to Applying Machine Learning to Computer Architecturemachine learning, computer architecture, data science, parameter engineering, performance prediction, schedulingThe resurgence of machine learning since the late 1990s has been enabled by significant advancesin computing performance and the growth of big data. The ability of these algorithms to detect complex patterns in data which are extremely difficult to achieve manually, helps to produce effective predictive models. Whilst computer architects have been accelerating the performance of machine learning algorithms with GPUs and custom hardware,\u00a0there have been few implementations leveraging these algorithms toimprove the computer system performance. The work that has been conducted, however, has produced considerably promising results. The purpose of this paper is to serve as a foundational base and guide to future computer architecture research seeking to make use of machine learning models for improving system efficiency. We describe a method that highlights when, why, and how to utilize machine learning models for improving system performance and provide a relevant example showcasing the effectiveness of applying machine learning in computer architecture. We describe a process of data generation every execution quantum and parameter engineering. This is followed by a survey of a set of popular machine learning models. We discuss their strengths and weaknesses and provide an evaluation of implementations for the purpose of creating a workload performance predictor for different core types in an x86 processor. The predictions can then be exploited by a scheduler for heterogeneous processors to improve the system throughput. The algorithms of focus are stochastic gradient descent based linear regression, decision trees, random forests, artificial neural networks, and $k$-nearest neighbors.\u00a0",
      "axisX": "-0.30867334767672977",
      "axisY": "0.06893539154516319",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180201",
      "title": "Deep Analysis of Job State Statistics on Lomonosov-2 Supercomputer",
      "authors": "Dmitry A. Nikitenko, Vadim V. Voevodin, Sergey A. Zhumatiy",
      "keywords": "Deep Analysis of Job State Statistics on Lomonosov-2 SupercomputerHPC, supercomputer, parallel computing, efficiency analysis, job stateIt is a common knowledge that the increasingly growing capabilities of HPC systems are always limited by a number of efficiency related issues. The reasons can be very different: hardware failures, incorrect job scheduling, peculiarities of algorithm, chosen programming technology specifics, etc. Most of these issues can be detected after precise analysis, but is a very resourceful way to study every application run. Therefore we performed less complicated analysis of the whole supercomputer job flow. In this paper we share our experience of analyzing user applications\u2019 job states assigned by the SLURM resource manager that is used on the Lomonosov-2 system at Supercomputing center of Lomonosov Moscow State University. The statistics on job states was collected and it revealed that the ratio of correctly finished jobs (with the COMPLETED state) was rather low. The jobs owners were asked if the distribution of their jobs\u2019 states is normal regarding their applications. This user feedback was processed, and some new ways of efficiency gain were revealed as the result.",
      "axisX": "-0.4819385299302704",
      "axisY": "-0.18604750223604735",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180202",
      "title": "Scalability Evaluation of Cimmino Algorithm for Solving Linear Inequality Systems on Multiprocessors with Distributed Memory",
      "authors": "Leonid B. Sokolinsky, Irina M. Sokolinskaya",
      "keywords": "Scalability Evaluation of Cimmino Algorithm for Solving Linear Inequality Systems on Multiprocessors with Distributed Memorysystem of linear inequalities, iterative algorithm, projection algorithm, Cimmino\nalgorithm, parallel computation model, bulk synchronous farm, scalability estimation, speedup,\nparallel efficiency, cluster computing systemsThe paper is devoted to a scalability study of Cimmino algorithm for linear inequality systems.\u00a0This algorithm belongs to the class of iterative projection algorithms. For the analytical analysis\u00a0of the scalability, the BSF (Bulk Synchronous Farm) parallel computation model is used. An\u00a0implementation of the Cimmino algorithm in the form of operations on lists using higher-order\u00a0functions Map and Reduce is presented. An analytical estimation of the scalability boundary of\u00a0the algorithm for cluster computing systems is derived. An information about the implementation\u00a0of Cimmino algorithm on lists in C++ language using the BSF program skeleton and MPI parallel\u00a0programming library is given. The results of large-scale computational experiments performed on\u00a0a cluster computing system are demonstrated. A conclusion about the adequacy of the analytical\u00a0estimations by comparing them with the results of computational experiments is made.",
      "axisX": "0.41197040866539414",
      "axisY": "0.35086626461830045",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180203",
      "title": "On the Inversion of Multiple Matrices on GPU in Batched Mode",
      "authors": "Nikolay M. Evstigneev, Oleg I. Ryabkov, Eugene A. Tsatsorin",
      "keywords": "On the Inversion of Multiple Matrices on GPU in Batched ModeQR algorithm, LU Matrix Inversion, Batched Solver, Matrix solver, GPU Batched\nSolverIn this research we are considering the benchmarking of batched matrix inversion and solution of linear systems. The problem of multiple matrix inversion with the same fill sparsity is usually considered in problems of fluid mechanics with chemistry. In this case the system is stiff, and an implicit method is required to solve the problem. The core of such method is the multiple matrix inversion. We benchmark different methods based on cuSPARSE and MAGMA libraries and CPU LAPACK version depending on the matrix filling. We also provide our own experimental code that implements GaussJordan elimination on GPU using register shuffle. It is shown that the fastest method is the QR matrix inversion for single precision calculations. We also show that the suggested Gauss\u2013Jordan elimination method looks promising being about 8\u201310 times faster than cuSPARSE QR method. We also demonstrate the application of batch solvers in the coupled reactive flow problem.",
      "axisX": "0.4678986390519964",
      "axisY": "0.10278369940608108",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180204",
      "title": "Parallel Numerical Algorithm for Solving Advection Equation for Coagulating Particles",
      "authors": "Sergey Alexandrovich Matveev, Rishat R. Zagidullin, Alexander P. Smirnov, Eugene E. Tyrtyshnikov",
      "keywords": "Parallel Numerical Algorithm for Solving Advection Equation for Coagulating Particlesaggregation equations, parallel algorithms, low-rank matrices, convolutionIn this work we present a parallel implementation of numerical algorithm solving the Cauchy problem for equation of advection of coagulating particles. This equation describes time-evolution of the concentration f(x, v, t) of particles of size v at the point x at the time-moment t. Our numerical algorithm is based on use of total variation diminishing (TVD) scheme and perfectly matching layers (PML) for approximation of advection operator along spatial coordinate x and utilization of the fast numerical method for evaluation of coagulation integrals exploiting low-rank decomposition of coagulation kernel coefficients and fast FFT-based implementation of convolution operation along particle size coordinate v. In our work we exploit one-dimensional domain decomposition approach along spatial coordinate x because it allows to avoid use of parallel FFT implementations which are very expensive in terms of data exchanges and have poor parallel scalability. Moreover, locality of finite-difference operator from TVD-scheme along x coordinate allows to obtain good scalability even for computing clusters with slow network interconnect due to modest volumes of data necessary for synchronization exchanges between times integration steps.",
      "axisX": "0.5858823340875535",
      "axisY": "-0.1240485519062606",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180205",
      "title": "Generation of Multiple Turbulent Flow States for the Simulations with Ensemble Averaging",
      "authors": "Boris I. Krasnopolsky",
      "keywords": "Generation of Multiple Turbulent Flow States for the Simulations with Ensemble AveragingIncompressible turbulent flow, Generalized sparse matrix vector multiplication,\nMultiple right-hand sides, Ensemble averagingThe paper deals with the problem of improving the performance of high-fidelity incompressible turbulent flow simulations on high performance computing systems. The ensemble averaging approach, combining averaging in time together with averaging over multiple ensembles, allows to speedup the corresponding simulations by increasing the computing intensity of the numerical method (flops per byte ratio). The current paper focuses on further improvement of the proposed computational methodology, and particularly, on the optimization of procedure to generate multiple independent turbulent flow states.",
      "axisX": "0.4008488752285326",
      "axisY": "-0.1305781106038983",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180206",
      "title": "High Performance Computing with Coarse Grained Model of Biological Macromolecules",
      "authors": "Emilia Agnieszka Lubecka, Adam Kazimierz Sieradzan, Cezary Czaplewski, Pawe\u0142 Krupa, Adam Liwo",
      "keywords": "High Performance Computing with Coarse Grained Model of Biological MacromoleculesUnified Coarse Grained Model, UNRES force field, molecular dynamics simulations, fine-grained parallelization, coarse-grained parallelizationThe Unified Coarse Grained Model of biological macromolecules (UCGM) that is being developed in our laboratory is a model designed to carry out large-scale simulations of biological macromolecules. The simplified chain representation used in the model allows to obtain 3-4 orders of magnitude extention of the time-scale of simulations, compared to that of all-atom simulations. Unlike most of the other coarse-grained force fields, UCGM is a physics-based force field, independent of structural databases and applicable to treat non-standard systems. In this communication, the efficiency and scalability of the new version of UCGM package with Fortran 90, with two parallelization levels: coarse-grained and fine-grained, is reported for systems with various size and oligomeric state. The performance was tested in the canonical- and replica exchange MD mode, with small- and moderate-size proteins and protein complexes (20 to 1,636 amino-acid residues), as well as with large systems such as, e.g., human proteosome 20S with size over 6,200 aminoacid residues, which show the advantage of using coarse-graining. It is demonstrated that, with using massively parallel architectures, and owing to the physics-based nature of UCGM, real-time simulations of the behavior of subcellular systems are feasible.",
      "axisX": "0.3447727486670998",
      "axisY": "0.30077775163204307",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180207",
      "title": "3D Problems of Rotating Detonation Wave in a Ramjet Engine Modeled on a Supercomputer",
      "authors": "Valeriy F. Nikitin, Yurii G. Filippov, Lyuben I. Stamov, Elena V. Mikhalchenko",
      "keywords": "3D Problems of Rotating Detonation Wave in a Ramjet Engine Modeled on a SupercomputerMathematical Modeling, Detonation, Deflagration, RDE, RamjetA rotating detonation engine (RDE) combustion chamber was modeled in the work numerically using 3D geometry. The RDE is a new type of engines capable to create higher thrust than the traditional ones, which are based on the combustible mixture deflagration process. In the numerical experiment, different scenarios of the engine performance were obtained. The calculations were made at a compact super-computer APK-5 with a peak performance of 5.5 Tera Flops.",
      "axisX": "0.51791965441612",
      "axisY": "0.0016209532529021936",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180208",
      "title": "Numerical Simulations of Black Hole Accretion Flows",
      "authors": "Agnieszka Janiuk, Konstantinos Sapountzis, Jeremy Mortier, Ireneusz Janiuk",
      "keywords": "Numerical Simulations of Black Hole Accretion Flowsastrophysical flows, black hole accretion, hydrodynamics, numerical simulations,\ngeneral relativistic MHDWe model the structure and evolution of black hole accretion disks using numerical simulations. The numerics is governed by the equations of general relativistic magneto-hydrodynamics (GRMHD). Accretion disks and outflows can be found at the base of very energetic ultra-relativistic jets produced by cosmic explosions, so called gamma-ray bursts (GRBs). Another type of phenomena are blazars, with jets emitted from the centers of galaxies.Long-lasting, detailed computations are essential to determine the physics of these explosions, and confront the theory with potential observables. From the point of view of numerical methods and techniques, three ingredients need to be considered. First, the numerical scheme must work in a conservative manner, which is achieved by solving a set of non-linear equations to advance the conserved quantities from one time step to the next. Second, the efficiency of computations depends on the code parallelization methods. Third, the analysis of results is possible via the post-processing of computed physical quantities, and visualization of the flow properties. This is done via implementing packages and libraries that are standardized in the field of computational astrophysics and supported by community developers.In this paper, we discuss the physics of the cosmic sources. We also describe our numerical framework and some technical issues, in the context of the GRMHD code which we develop. We also present a suite of performance tests, done on the High-Performance Computer cluster (HPC) in the Center for Mathematical Modeling of the Warsaw University.",
      "axisX": "0.26527030355891684",
      "axisY": "-0.2438139728946196",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180301",
      "title": "Continuum Computing - on a New Performance Trajectory beyond Exascale",
      "authors": "Maciej Brodowicz, Thomas Sterling, Matthew Anderson",
      "keywords": "Continuum Computing - on a New Performance Trajectory beyond Exascalehigh performance computing, parallel computing, exascale, non-von Neumann architectureThe end of Moore's Law is a cliche that none the less is a hard barrier to future scaling of high performance computing systems. A factor of about 4x in device density is all that is left of this form of improved throughput with a 5x gain required just to get to the milestone of exascale. The remaining sources of performance improvement are better delivered efficiency of more than 10x and alternative architectures to make better use of chip real estate. This paper will discuss the set of principles guiding a potential future of non-von Neumann architectures as adopted by the experimental class of Continuum Computer Architecture (CCA). It is being explored by the Semantic Memory Architecture Research Team (SMART) at Indiana University. CCA comprises a homogeneous aggregation of cellular components (function cells) which are orders of magnitude smaller than lightweight cores and individually is unable to accomplish a computation but in combination can do so with extreme cost efficiency and unprecedented scalability. It will be seen that a path exists based on such unconventional methods like neuromorphic computing or dataflow that not only will meet the likely exascale milestone in the same time with much better power, cost, and size but also will set a new performance trajectory leading to Zetaflops capability before 2030.",
      "axisX": "-0.25583726694950987",
      "axisY": "-0.3590714778833632",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180302",
      "title": "Three-dimensional Inversion of Electromagnetic Geophysical Data with Parallel Computational Code on Supercomputer Complex \"Lomonosov\"",
      "authors": "Sergey V. Zaytsev, Viktor A. Kulikov, Andrei G. Yakovlev, Denis V. Yakovlev",
      "keywords": "Three-dimensional Inversion of Electromagnetic Geophysical Data with Parallel Computational Code on Supercomputer Complex \"Lomonosov\"magnetotelluric, three-dimensional inversion, supercomputer, geophysicsUsage of 2D inversion of magnetotelluric data for real geological objects can cause distortion, but it is more often used in commercial projects, because of its effectiveness and great experience. Whereas in the case of 3D inversion is not such a great experience and there are a number of global problems. When switching to 3D inversion of MT data, the requirement for computer technology is significantly increased. In this paper we will discuss a few examples of 3D inversion of electromagnetic geophysical field data with the usage of \"Lomonosov\" supercomputer and show its effectiveness on several geological objects. Each object is associated with a variety of problems: from search for shallow ore to regional hydrocarbon exploration. But all these objects contain a large volume of measurements obtaining qualitative results for which requires a huge amount of time. So that the use of 3D inversion with a high-performance computational complex makes it possible to obtain a qualitative result of solving a wide range of problems.",
      "axisX": "-0.054928779811181105",
      "axisY": "-0.3304043061907238",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180303",
      "title": "Simulating the Long-timescale Structural Behavior of Bacterial and Influenza Neuraminidases with Different HPC Resources",
      "authors": "Yana A. Sharapova, Dmitry A. Suplatov, Vytas K. \u0160vedas",
      "keywords": "Simulating the Long-timescale Structural Behavior of Bacterial and Influenza Neuraminidases with Different HPC Resourcesneuraminidases, molecular dynamics, long-timescale trajectories, GPU, co-designUnderstanding the conformational dynamics which affects ligand binding by Neuraminidases is needed to improve the in silico selection of novel drug candidates targeting these pathogenicity factors and to adequately estimate the efficacy of potential drugs. Conventional molecular dynamics (MD) is a powerful tool to study conformational sampling, drug-target recognition and binding, but requires significant computational effort to reach timescales relevant for biology. In this work the advances in a computer power and specialized architectures were evaluated at simulating long MD trajectories of the structural behavior of Neuraminidases. We conclude that modern GPU accelerators enable calculations at the timescales that would previously have been intractable, providing routine access to microsecond-long trajectories in a daily laboratory practice. This opens an opportunity to move away from the \"static\" affinity-driven strategies in drug design towards a deeper understanding of ligand-specific conformational adaptation of target sites in protein structures, leading to a better selection of efficient drug candidates in silico. However, the performance of modern GPUs is yet far behind the deeply-specialized supercomputers co-designed for MD. Further development of affordable specialized architectures is needed to move towards the much-desired millisecond timescale to simulate large proteins at a daily routine.",
      "axisX": "-0.28444642282302396",
      "axisY": "-0.3261128848965466",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180304",
      "title": "Parallel GPU-based Implementation of One-Way Wave Equation Migration",
      "authors": "Alexander L. Pleshkevich, Vadim V. Lisitsa, Dmitry M. Vishnevsky, Vadim D. Levchenko, Boris M. Moroz",
      "keywords": "Parallel GPU-based Implementation of One-Way Wave Equation MigrationGPU, nested OMP, MPI, seismic imagingWe present an original algorithm for seismic imaging, based on the depth wavefield extrapolation by the\u00a0 one-way wave equation. Parallel implementation of the algorithm is based on the several levels of parallelism. The input data parallelism allows processing full coverage for some area (up to one square km); thus, data are divided into several subsets and each subset is processed by a single MPI process. The mathematical approach allows dealing with each frequency independently and treating solution layer-by-layer; thus, a set of 2D cross-sections instead of the initial 3D common-offset vector gathers are processed simultaneously. This part of the algorithm is implemented suing GPU. Next, each common-offset vector image can be stacked, processed and stored independently. As a result, we designed and implemented the parallel algorithm based on the use of CPU-GPU architecture which allows computing common-offset vector images using one-way wave equation-based amplitude preserving migration. The algorithm was used to compute seismic images from real seismic land data.",
      "axisX": "0.38671941087457584",
      "axisY": "0.20489786673485727",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180305",
      "title": "High-performance Computational Modeling of Chromosome Structure",
      "authors": "Yuri A. Eidelman, Svetlana V. Slanina, Oleg A. Gusev, Sergey G. Andreev",
      "keywords": "High-performance Computational Modeling of Chromosome Structurechromosome conformation capture, chromosome structure, computational modeling, mouse chromosome 18We present a polymer modeling approach to generate the ensemble of 3D chromosome conformations at different time points of mitosis-interphase transition. Dynamics of structure during mitosis-G1 transition indicates quick and slow stages of chromosome shape alterations. At intermediate and late time scale the changes in chromosome compaction are small. To assess time dependence of contact map establishment during G1 we calculate contact maps at different times after mitotic decondensation. We demonstrate that the patterns of contacts observed soon after mitotic decondensation remain similar during G1. Whole contact map for mouse chromosome 18 at late G1 time correlates with the experimental chromosome conformation capture data. The simulations reproduce the main experimental findings, contact map persistence during G1 as well as specific pattern of long-range interactions in interphase chromosome. Our results suggest that spatial compartmentalization of an interphase chromosome is driven by interactions between different types of megabase sized chromatin domains during the formation of globular chromosome state at the end of mitotis to G1 transition.",
      "axisX": "0.4994193844041377",
      "axisY": "-0.31287636021336895",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180306",
      "title": "Numerical Simulations of Structural Chromosomal Instability",
      "authors": "Yuri A. Eidelman, Svetlana V. Slanina, Valentina S. Pyatenko, Sergey G. Andreev",
      "keywords": "Numerical Simulations of Structural Chromosomal Instabilitychromosomal instability, ionizing radiation, delayed chromosomal damage, prediction, dose responseThe origin of dose-response curves for radiation-induced chromosomal instability (CI) is studied using the mechanistic CI model. The model takes into account DNA damage generation and repair in the progeny of irradiated cells and cell passage through mitotic cycle. We consider the formation of DNA double-strand breaks (DSBs) de novo in the S phase, where predominantly chromatid-type aberrations are formed. Among them sister chromatid exchanges of the \u201cisochromatid deletion\u201d type, or \u201cchromatid dicentrics\u201d are of primary interest. When the cell enters mitosis, the fate of chromosomal aberrations depends on their types. Chromosomal and chromatid fragments, having entered mitosis, either are transmitted into one of the daughter cells, or are lost. A chromatid dicentric in mitosis forms an anaphase bridge. These mechanistic assumptions were used to demonstrate that the dose-response curves are closely related to the dynamic curves for CI. The principles underlying this relationship are analyzed.",
      "axisX": "0.32477492341274333",
      "axisY": "-0.48060193530024636",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180307",
      "title": "GPU-based Implementation of Discrete Element Method for Simulation of the Geological Fault Geometry and Position",
      "authors": "Vadim V. Lisitsa, Vladimir A. Tcheverda, Victoria V. Volianskaia",
      "keywords": "GPU-based Implementation of Discrete Element Method for Simulation of the Geological Fault Geometry and PositionDiscrete Element Method, geological faults, CUDA, statistical simulationWe present an algorithm for numerical simulation of the geological fault formation. The approach is based on the discrete elements method, which allows modeling of the deformations and structural discontinuity of the Upper part of the Earth crust. In the discrete elements method, the medium is represented as an combination of discrete particles which interact as elastic or viscoelastic bodies. Additionally, external potential forces, for example gravitational forces, may be introduced. At each time step the full set of forces acting at each particle is computed, after that the position of the particle is evaluated on the base of Newtonian mechanics. We implement the algorithm using CUDA technology to simulate single statistical realization of the model, whereas MPI is used to parallelize with respect to different statistical realizations. Obtained numerical results show that for low dip angles of the tectonic displacements relatively narrow faults form, whereas high dip angles of the tectonic displacements lead to a wide V-shaped deformation zones.",
      "axisX": "0.5788060613188786",
      "axisY": "-0.43242005701768255",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180308",
      "title": "Modelling of Quantum Qubit Behaviour for Future Quantum Computers",
      "authors": "Andrey N. Chibisov, Mary A. Chibisova",
      "keywords": "Modelling of Quantum Qubit Behaviour for Future Quantum Computersquantum qubit, quantum computer, quantum-mechanical calculations, spinThis work deals with quantum qubit modelling based on a silicon material with embedded phosphorus atoms because a future quantum computer can be built on the basis of this qubit. The building of atomic models of bulk crystalline silicon and silicene, as well as calculation of their total energies, were performed using the Quantum ESPRESSO software package, using highperformance computing (HPC). For silicon and phosphorus atoms the generalized gradient approximation (GGA) was used in terms of the spin-orbit non-collinear interaction by means of the Quantum ESPRESSO package. The equilibrium orientations of the phosphorus qubit spins and localization of the wave functions in the 2D and bulk crystalline silicon phases were theoretically investigated by means of quantum-mechanical calculations. The existence of an exchange interaction between qubits has been confirmed, which leads to a change in the wave function\u2019s localization and spin orientation, and in the case of silicene, this interaction was stronger.",
      "axisX": "0.5734211238634407",
      "axisY": "-0.032612382626377986",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180309",
      "title": "Multiscale Simulations Approach: Crosslinked Polymer Matrices",
      "authors": "Pavel V. Komarov, Daria V. Guseva, Vladimir Yu. Rudyak, Alexander V. Chertovich",
      "keywords": "Multiscale Simulations Approach: Crosslinked Polymer Matricespolymers, networks, atomistic molecular dynamics, mesoscale simulations, multiscale simulationsAtomistic molecular dynamics simulations can usually cover only a very limited range in space and time. Thus, the materials like polymer resin networks, the properties of which are formed on macroscopic scale, are hard to study thoroughly using only molecular dynamics. Our work presents a multiscale simulation methodology to overcome this shortcoming. To demonstrate its effectiveness, we conducted a study of thermal and mechanical properties of complex polymer matrices and establish a direct correspondence between simulations and experimental results. We believe this methodology can be successfully used for predictive simulations of a broad range of polymer matrices in glassy state.",
      "axisX": "0.1618058016317327",
      "axisY": "-0.13975794599381078",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180310",
      "title": "Application of High Performance Computing for Comparison of Two Highly Branched Lysine Molecules of Different Topology",
      "authors": "Igor M. Neelov, Oleg V. Shavykin, Maxim Y. Ilyash, Valeriy V. Bezrodnyi, Sofia E. Mikhtaniuk, Anna A. Marchenko, Emil I. Fatullaev, Anatolii A. Darinskii, Frans A. M. Leermakers",
      "keywords": "Application of High Performance Computing for Comparison of Two Highly Branched Lysine Molecules of Different Topologyhigh performance computing, dendrimer, dendritic brush, Poly-L-lysineHigh performance computations were performed for comparison of size and other properties of big heavily charged biocompatible molecules of complex topology in water. Lysine dendrimer and short dendritic brush of the same molecular weight were studied by molecular dynamics simulation method and GROMACS software package. The size and structural properties of these two systems were compared. It was shown that dendritic brush has smaller size and more dense core than the dendrimer. Radial density profile for both molecules is not monotonous and has minimum near core of molecules. This minimum is wider and deeper for dendrimer than for dendritic brush. Thus dendrimer has larger region of low density than dendritic brush and is more suitable for use for encapsulation and delivery of hydrophobic drugs.",
      "axisX": "0.4372819677385685",
      "axisY": "-0.23420464138405273",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180311",
      "title": "Developing Efficient Implementations of Bellman\u2013Ford and Forward-Backward Graph Algorithms for NEC SX-ACE",
      "authors": "Ilya V. Afanasyev, Alexander S. Antonov, Dmitry A. Nikitenko, Vadim V. Voevodin, Vladimir V. Voevodin, Kazuhiko Komatsu, Osamu Watanabe, Akihiro Musa, Hiroaki Kobayashi",
      "keywords": "Developing Efficient Implementations of Bellman\u2013Ford and Forward-Backward Graph Algorithms for NEC SX-ACEgraph algorithms, NEC SX-ACE, vector computing, data-intensive applicationsThe main goal of this work is to demonstrate that the development of data-intensive appli- cations for vector systems is not only important and interesting, but is also very possible. In this paper we describe possible implementations of two fundamental graph-processing algorithms for an NEC SX-ACE vector computer: the Bellman\u2013Ford algorithm for single source shortest paths computation and the Forward-Backward algorithm for strongly connected components detection. The proposed implementations have been developed and optimised in accordance with features and properties of the target architecture, which allowed them to achieve performance comparable to other traditional platforms, such as Intel Skylake, Intel Knight Landing or IBM Power processors.",
      "axisX": "-0.05453043528501612",
      "axisY": "0.4237953822315647",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180312",
      "title": "Applications of High Performance Computing: Born\u2013Oppenheimer Molecular Dynamics of Complex Formation in Aqueous Solutions",
      "authors": "Maria G. Khrenova, Dmitry P. Kapusta, Ilya V. Babchuk, Yulia I. Meteleshko",
      "keywords": "Applications of High Performance Computing: Born\u2013Oppenheimer Molecular Dynamics of Complex Formation in Aqueous SolutionsBorn-Oppenheimer molecular dynamic, free binding energy, parallel algorithms,\ncalixareneThe progress of supercomputer technologies initiated the development of methods of computational chemistry and their applications, particularly molecular dynamic simulations with ab initio potentials. These new methods allow to solve important problems of chemistry and technology. Particularly, solvent extraction and separation techniques are widely used to decrease the amount of radioactive wastes, especially radioactive caesium isotopes present in liquid phases. We demonstrated that the calculated binding constants between the alkali cation and calix[4]arene differ 103 times for Cs+ and Na+ ions, that is in good agreement with the experimental value. We report the results of benchmark calculations of our model system composed of 929 atoms described in the\u00a0 density functional theory approximation with the GGA-type functional PBE with the empirical dispersion correction D3 and combined basis of Gaussian functions and plane waves DZVP with Goedecker-Teter-Hutter pseudopotentials. We demonstrate that efficiency of calculations decrease to about half if the amount of nodes is 16 on the Lomonosov-2 supercomputer.",
      "axisX": "0.6269851684180335",
      "axisY": "-0.39574328554097077",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180313",
      "title": "Analysis of the Effect of Dispersion Forces on the Dielectric Film Properties Using Parallel Computing",
      "authors": "Kirill A. Emelyanenko, Ludmila B. Boinovich, Alexandre M. Emelyanenko",
      "keywords": "Analysis of the Effect of Dispersion Forces on the Dielectric Film Properties Using Parallel ComputingCFDM, thin film properties, dispersion forces, local polarizabilityThe paper presents the analysis of dispersion forces effect on local properties in thin free films. Using a Coupled Fluctuated Dipole Method with developed methods for numerical calculations of dielectric properties, the films with different lateral sizes and thicknesses were studied. In particular, the molecular polarizabilities at different distance from the film interface were analyzed. It was shown that dispersion interaction between the molecules, even for the case of nonpolar liquid with weak intermolecular interactions, causes a notable variation in dielectric properties of thin film, which is associated with the boundary layer formation. This variation, in turn, causes a\u00a0strong dependence of polarizability accuracy on the cut-off radius. It is demonstrated that parallel computing algorithms can be effectively applied for obtaining the reliable data on properties of liquids in wetting films and boundary layers even under resource-imposed constraint on the size of ensemble of molecules to be handled in the numerical studies.",
      "axisX": "0.6367450065842485",
      "axisY": "-0.4208302199142005",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180314",
      "title": "Quantum Chemistry Research of Interaction between 3D-Transition Metal Ions and a Defective Graphene on the Supercomputer Base",
      "authors": "Nikolai V. Khokhriakov, Santiago Melchor",
      "keywords": "Quantum Chemistry Research of Interaction between 3D-Transition Metal Ions and a Defective Graphene on the Supercomputer Basegraphene, transition metals, quantum chemistry, density functionalQuantum chemistry research is presented in the article, and it concerns the interaction within the complexes formed by the defective graphene clusters and ions of 3d-transition metals V,Cr,Mn, Fe,Co,Ni,Cu. The charges of all regarded ions were +1. All calculations were made at UDFT B3LYP/6-31G level of theory with the BSSE error taken into account. The strongest interaction with the defective clusters is observed in the case of Co+ ion. At the same time, this ion has demonstrated rather weak interaction with the defect-free graphene. Thus, the presence of Co+ in the reaction media increases probability of defect formation with the further forming of short nanotubes and curved carbon clusters with complex topology of their own.",
      "axisX": "0.36015841401336607",
      "axisY": "-0.5268959333062896",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180315",
      "title": "A Flux Splitting Method for the SHTC Model for High-performance Simulations of Two-phase Flows",
      "authors": "Nadezhda S. Smirnova, Michael Dumbser, Mikhail N. Petrov, Alexander V. Chikitkin, Evgeniy I. Romenski",
      "keywords": "A Flux Splitting Method for the SHTC Model for High-performance Simulations of Two-phase Flowsflux splitting, two-phase compressible flow, complete Riemann solver, finite-volume\nmethod, hyperbolic equations, supercomputer computationsIn this paper we propose a new flux splitting approach for the symmetric hyperbolic thermodynamically compatible (SHTC) equations of compressible two-phase flow which can be used in finite-volume methods. The approach is based on splitting the entire model into acoustic and pseudo-convective submodels. The associated acoustic system is numerically solved applying HLLC-type Riemann solver for its Lagrangian form. The convective part of the pseudo-convective submodel is solved by a standart upwind scheme. For other parts of the pseudo-convective submodel we apply the FORCE method. A comparison is carried out with unsplit methods. Numerical results are obtained on several test problems. Results show good agreement with exact solutions and reference calculations.",
      "axisX": "0.6777865259308885",
      "axisY": "-0.08035156138733848",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180316",
      "title": "Magnetic Properties of LaAlO3/SrTiO3 Heterostructure Modelled on a Supercomputer",
      "authors": "Irina Piyanzina, Volker Eyert, Thilo Kopp, Dmitrii Tayurskii",
      "keywords": "Magnetic Properties of LaAlO3/SrTiO3 Heterostructure Modelled on a SupercomputerDFT, LaAlO 3/SrTiO 3heterostructure, defects, magnetic propertiesThe oxide heterostructure composed of LaAlO3 (LAO) thin film on top of SrTiO3 (STO) substrate is the best known example of a system where a metallic state is formed in the STO layers next to the interface [1]. In the frame of present work we analyze an impact of oxygen vacancies and hydrogen dopants located in the AlO2 surface layer and in the TiO2 interfacial plane of LAO/STO heterostructure onto the magnetic properties by performing spin-polarized calculations based on density functional theory (DFT). We found stable local magnetic moments formed within atomically thin magnetic layers at the interface. We confirmed that agnetism can be generated by oxygen vacancies located either at the surface or at the interface. In addition, we demonstrate magnetic moments formation by hydrogen dopants located at the interface. Finally, the case of two defects combination was investigated, when negligibly small magnetic moment induction was found to take place.",
      "axisX": "0.43060714534891714",
      "axisY": "-0.362106294054945",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180317",
      "title": "Optimization of BWB Aircraft Using Parallel Computing",
      "authors": "Kirill S. Anisimov, Andrey A. Savelyev, Innocentiy A. Kursakov, Alexander V. Lysenkov, Prajwal S. Prakasha",
      "keywords": "Optimization of BWB Aircraft Using Parallel Computingoptimization, CFD, Blended Wing Body, nacelle, power plantNacelle shape optimization for Blended Wing Body (BWB) is performed. The optimization procedure is based on numerical calculations of the Reynolds\u2013averaged Navier\u2013Stokes equations. For the Top Level Aircraft Requirements, formulated in AGILE project, the propulsion system was designed. The optimization procedure was divided in two steps. At first step, the isolated nacelle was designed and optimized for cruise regimes. This step is listed in paragraph 3. At second step the nacelles positions over airframe were optimized. To find the optimum solution, surrogate\u2013based Efficient Global Optimization algorithm is used. An automatic structural computational mesh creation is realized for the effective optimization algorithm working. This whole procedure is considered in the context of the third generation multidisciplinary optimization techniques, developed within AGILE project. During the project, new techniques should be implemented for the novel aircraft configurations, chosen as test cases for application of AGILE technologies. It is shown that the optimization technology meets all requirements and is suitable for using in the AGILE project.",
      "axisX": "0.09590378665291083",
      "axisY": "0.06607090832667428",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180318",
      "title": "Supercomputer Simulations of Nondestructive Tomographic Imaging with Rotating Transducers",
      "authors": "Sergey Y. Romanov",
      "keywords": "Supercomputer Simulations of Nondestructive Tomographic Imaging with Rotating Transducerssupercomputer, ultrasound tomography, nondestructive testing, inverse problemsA method of nondestructive ultrasound tomographic imaging employing a rotating transducer system is proposed. The rotating transducer system increases the number of emitters and detectors in a tomographic scheme by several times and makes it possible to neutralize image artifacts resulting from incomplete-data tomography. The inverse problem of tomographic reconstructing the velocity structure inside the inspected object is considered as a nonlinear coefficient inverse problem for a scalar wave equation. Scalable iterative algorithms for reconstructing the longitudinal wave velocity inside the object are discussed. The methods are based on the explicit representation for the gradient of the residual functional. The algorithms employ parallelizing the computations over emitter positions. Numerical simulations performed on the \u201cLomonosov-2\u201d supercomputer showed that the tomographic methods developed can not only detect boundaries of defects, but also determine the wave velocity distribution inside the defects with high accuracy provided that both reflected and transmitted waves are registered.",
      "axisX": "0.7346813859032875",
      "axisY": "-0.13343843309036119",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180319",
      "title": "Reverse Mapping Algorithm for Multi-scale Numerical Simulation of Polylactic Acid",
      "authors": "Mikhail K. Glagolev, Valentina V. Vasilevskaya",
      "keywords": "Reverse Mapping Algorithm for Multi-scale Numerical Simulation of Polylactic Acidmolecular dynamics, multiscale simulation, reverse mapping, poly(lactic acid)An algorithm is proposed to convert the coarse-grained A-graft-B model of polylactic acid into the atomistic representation. In the A-graft-B model the atoms of the backbone are mapped onto A beads, which form the linear backbone of the coarse-grained macromolecule, the methyl groups are mapped onto B side pendants. The algorithm restores atomic positions based on positions of coarse-grained beads with the help of pre-defined chain fragments, called templates. The dimensions of the templates are adjusted by affine transformation to ensure coincidence of the backbone in coarse-grained and atomistic representation. The transition between coarse-grained and atomistic models conserves information about the fine structure of polymer chains. The restored configurations are suitable for further molecular-dynamic simulations. Both atomistic and coarse-grained representations require standard GROMACS software. The algorithm can be used for reverse mapping of other A-graft-B polymer models.",
      "axisX": "0.734107288488123",
      "axisY": "0.21131528065520255",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180320",
      "title": "Supercomputer Technologies as a Tool for High-resolution Atmospheric Modelling towards the Climatological Timescales",
      "authors": "Vladimir S. Platonov, Mikhail I. Varentsov",
      "keywords": "Supercomputer Technologies as a Tool for High-resolution Atmospheric Modelling towards the Climatological Timescalesregional climate model, long-term simulations, supercomputer technologies, extreme\nwind, urban climate, urban precipitation, COSMOEstimation of the recent and future climate changes is the most important challenge in the modern Earth sciences. Numerical climate models are an essential tool in this field of research. However, modelling results are highly sensitive to the spatial resolution of the model. The most of the climate change studies utilize the global atmospheric models with a grid cell size of tens of kilometres or more. High-resolution mesoscale models are much more detailed, but require significantly more computational resources. Applications of such high-resolution models in climate studies are usually limited by regional simulations and by relatively short timespan. In this paper we consider the experience of the long-term regional climate studies based on the mesoscale modelling. On the examples of urban climate studies and extreme wind assessments, we demonstrate the principle advantage of long-term high-resolution simulations, which were carried out on the modern supercomputers.",
      "axisX": "-0.34819101506063715",
      "axisY": "-0.34399081870364895",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180321",
      "title": "Supercomputer Simulations in Design of Ultrasound Tomography Devices",
      "authors": "Alexander V. Goncharsky, Sergey Y. Seryozhnikov",
      "keywords": "Supercomputer Simulations in Design of Ultrasound Tomography Devicesultrasound tomography, coefficient inverse problem, spatial resolution, supercomputer, GPUThe paper considers the use of supercomputers in design of medical ultrasound tomography devices. The mathematical models describing the wave propagation in ultrasound tomography should take into account such physical phenomena as diffraction, multiple scattering, and so on. The inverse problem of wave tomography is posed as a coefficient inverse problem with respect to the wave propagation velocity and the absorption factor. Numerous simulations made it possible to determine the optimal parameters of an ultrasound tomograph in order to obtain a spatial resolution of 1.5 mm suitable for early-stage breast cancer diagnosis. The developed methods were tested both on model problems and on real data obtained at the experimental test bench for tomographic studies. The computations were performed on GPU devices of Lomonosov-2 supercomputer at Lomonosov Moscow State University.",
      "axisX": "0.3936385418946119",
      "axisY": "-0.35892658077494766",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180322",
      "title": "Recent Progress on Supercomputer Modelling of High-Speed Rarefied Gas Flows Using Kinetic Equations",
      "authors": "Anna A. Frolova, Vladimir A. Titarev",
      "keywords": "Recent Progress on Supercomputer Modelling of High-Speed Rarefied Gas Flows Using Kinetic EquationsBoltzmann kinetic equation, S-model, rarefied, high-speed, unstructuredNumerical solution of the Boltzmann equation for stationary high-speed flows around complex three-dimensional bodies is an extremely difficult computational problem. This is because of high dimension of the equation and lack of efficient implicit methods for the calculation of the collision integral on arbitrary non-uniform velocity grids. Therefore, the use of the so-called model (approximate) kinetic equations appears to be more appropriate and attractive. This article uses the numerical methodology recently developed by the second author which includes an implicit method for solving the approximating kinetic equation of E.M. Shakhov (S-model) on arbitrary unstructured grids in both velocity and physical spaces. Since most of model equations have a well-known drawback associated with the velocityindependent collision frequency it is important to determine the deviations of solutions of these equations from the solution of the complete Boltzmann equation or DSMC for high-speed gas flows. Our recent comparison of the DSMC and S-model solutions for monatomic gases with a soft interaction potential shows good agreement of surface coefficients of the pressure, heat transfer and friction, which are most important for industrial applications. In this paper, we compare the solution of model equations and the Boltzmann equation for the problem of supersonic gas flow around a cylinder when molecules interact according to the law of hard spheres. Since this law of molecular interaction is the most rigid, the difference in solutions can show the maximum error that can be obtained by using model equations instead of the exact Boltzmann equation in such problems. Our high-fidelity computations show that the use of model kinetic equations with adaptation in phase space is very promising for industrial applications.",
      "axisX": "0.5191651635077492",
      "axisY": "-0.5042159226994152",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180323",
      "title": "Supercomputer Modeling of Parachute Flight Dynamics",
      "authors": "Alexey V. Setukha, Vladimir A. Aparinov, Andrey A. Aparinov",
      "keywords": "Supercomputer Modeling of Parachute Flight Dynamicsparallel algorithms, numerical simulation methods, fluid dynamics, vortex methods,\nparachute aerodynamicsIn this article the authors present parallel implementation of numerical method for computer modeling of dynamics of a parachute with filled canopy. To solve the 3D problem of parachute free motion numerically, authors formulate tied problem of dynamics and aerodynamics where aerodynamic characteristics are found with discrete vortices method on each step of integration in time, and to find motion law the corresponding motion equations have to be solved. The solution of such problems requires high computational resources because it is important to model parachute motion during a long physical time period. Herewith the behavior of vortex wake behind the parachute is important and has to be modeled. In the approach applied by the authors the wake is modeled as a set of flexible vortex elements. So to increase computational efficiency, the authors used methods of low-rank matrix approximations, as well as parallel implementations of algorithms. Short description of numerical method is presented, as well as the examples of numerical modeling.",
      "axisX": "0.3661448151727094",
      "axisY": "-0.19619115380122362",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180324",
      "title": "Supercomputer Simulation of MATIS-H Problem",
      "authors": "Mikhail A. Zaitsev, Vasilij M. Goloviznin, Sergej A. Karabasov",
      "keywords": "Supercomputer Simulation of MATIS-H ProblemNavier-Stokes equations, parallel computation, modelling, Cabaret methodA supercomputer simulation of the benchmark MATiS-H problem is considered. A highresolution CABARET code is applied for solving Navier-Stokes equations in the framework of the Monotonically Integrated LES approach for the MATiS-H problem. The code is based on a generalisation of low-dissipative, low-dispersive and non-oscillatory CABARET scheme to hybrid topology meshes in the supercomputing framework. The solutions for the time-averaged fields are reported. These show a relatively small sensitivity to the grid density. Comparison with the experiment data available is provided.",
      "axisX": "0.5842020103374865",
      "axisY": "0.09725758398608272",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180325",
      "title": "High-performance Full-atomistic Simulation of Optical Thin Films",
      "authors": "Fedor Grigoriev, Vladimir Sulimov, Alexander Tikhonravov",
      "keywords": "High-performance Full-atomistic Simulation of Optical Thin Filmsthin film structure, deposition process, molecular dynamic simulation, silicon dioxideThe experimental study of the dependence of thin film properties on the deposition conditions may be still a great challenge. Today the progress in high performance computing allows one to perform the investigation of these dependencies on the atomistic level using the classical molecular dynamics (MD) simulation. In the present work the computational cost and efficiency of classical full-atomistic simulation of thin film deposition process using the Lonmonosov-2 supercomputer facilities is discussed. It is demonstrated that using 512 computational cores of the Lomonosov-2 supercomputer ensures the simulation of thin film cluster with technologically meaningful thickness of an optical film. Because of a relatively slow growth of the simulation time with the increase of film thickness we guess that simulations clusters with thicknesses that are several times higher than the currently achieved thicknesses about one hundred nanometers is quite realistic if the number of available computational cores will be increased up to several thousands.",
      "axisX": "-0.18771156135887349",
      "axisY": "-0.44052637918996895",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180326",
      "title": "Supercomputer Docking: Investigation of Low Energy Minima of Protein-Ligand Complexes",
      "authors": "Danil C. Kutov, Alexey V. Sulimov, Vladimir B. Sulimov",
      "keywords": "Supercomputer Docking: Investigation of Low Energy Minima of Protein-Ligand Complexesdocking, protein-ligand, global minimum, force field, quantum-chemical methodIt is shown that the global energy minimum of a protein-ligand complex, when the energy is calculated by the PM7 quantum-chemical semiempirical method with the COSMO implicit solvent model, can be determined as follows. First, the low energy minima are found by a docking program when the protein-ligand energy is calculated with the MMFF94 force field in vacuum. Second, energies of all these minima are recalculated with the PM7 method and the COSMO implicit solvent model. Third, among these recalculated energies the minimal energy is determined and the respective minimum is the global energy minimum when the energy is calculated with the PM7 method and the COSMO implicit solvent model. The optimal width of the spectrum of low energy minima found with MMFF94 in vacuum is determined to perform minimal quantity of quantum-chemical recalculations. The proposed approach allows to perform docking in solvent with the quantum-chemical method and to increase the docking positioning accuracy.",
      "axisX": "0.10491092942099597",
      "axisY": "-0.5997404733561132",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140101",
      "title": "Toward Exascale Resilience: 2014 update",
      "authors": "Franck Cappello, Al Geist, William Gropp, Sanjay Kale, Bill Kramer, Marc Snir",
      "keywords": "Toward Exascale Resilience: 2014 updateExascale, Resilience, Fault-tolerance techniquesResilience is a major roadblock for HPC executions on future exascale systems. These systems will typically gather millions of CPU cores running up to a billion threads. Projections from current large systems and technology evolution predict errors will happen in exascale systems many times per day. These errors will propagate and generate various kinds of malfunctions, from simple process crashes to result corruptions.The past five years have seen extraordinary technical progress in many domains related to exascale resilience. Several technical options, initially considered inapplicable or unrealistic in the HPC context, have demonstrated surprising successes. Despite this progress, the exascale resilience problem is not solved, and the community is still facing the difficult challenge of ensuring that exascale applications complete and generate correct results while running on unstable systems. Since 2009, many workshops, studies, and reports have improved the definition of the resilience problem and provided refined recommendations. Some projections made during the previous decades and some priorities established from these projections need to be revised. This paper surveys what the community has learned in the past five years and summarizes the research problems still considered critical by the HPC community.",
      "axisX": "-0.6232569826125809",
      "axisY": "-0.4957986336662588",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140102",
      "title": "Runtime-Aware Architectures: A First Approach",
      "authors": "Mateo Valero, Miquel Moreto, Marc Casas, Eduard Ayguade, Jesus Labarta",
      "keywords": "Runtime-Aware Architectures: A First ApproachParallel architectures, runtime system, hardware-software co-designIn the last few years, the traditional ways to keep the increase of hardware performance at the rate predicted by Moore's Law have vanished. When uni-cores were the norm, hardware design was decoupled from the software stack thanks to a well defined Instruction Set Architecture (ISA). This simple interface allowed developing applications without worrying too much about the underlying hardware, while hardware designers were able to aggressively exploit instruction-level parallelism (ILP) in superscalar processors. With the irruption of multi-cores and parallel applications, this simple interface started to leak. As a consequence, the role of decoupling again applications from the hardware was moved to the runtime system. Efficiently using the underlying hardware from this runtime without exposing its complexities to the application has been the target of very active and prolific research in the last years.Current multi-cores are designed as simple symmetric multiprocessors (SMP) on a chip.\u00a0However, we believe that this is not enough to overcome all the problems that multi-cores already have to face.\u00a0It is our position that the runtime has to drive the design of future multi-cores\u00a0to overcome the restrictions in terms of power, memory, programmability and resilience that multi-cores have.\u00a0In this paper, we introduce a first approach towards a Runtime-Aware Architecture (RAA),\u00a0a massively parallel architecture designed from the runtime's perspective.",
      "axisX": "-0.3609301272918256",
      "axisY": "-0.030948119445760792",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140103",
      "title": "Towards a performance portable, architecture agnostic implementation strategy for weather and climate models",
      "authors": "Oliver Fuhrer, Carlos Osuna, Xavier Lapillonne, Tobias Gysi, Ben Cumming, Mauro Bianco, Andrea Arteaga, Thomas Christoph Schulthess",
      "keywords": "Towards a performance portable, architecture agnostic implementation strategy for weather and climate modelsnumerical weather prediction, climate modeling, hybrid computing, programming\nmodelsWe propose a software implementation strategy for complex weather and climate models that produces performance portable, architecture agnostic codes. It relies on domain and data structure specific tools that are usable within common model development frameworks -- Fortran today and possibly high-level programming environments like Python in the future. We present the strategy in terms of a refactoring project of the atmospheric model COSMO, where we have rewritten the dynamical core and refactored the remaining Fortran code. The dynamical core is built on top of the domain specific ``Stencil Loop Language'' for stencil computations on structured grids, a generic framework for halo exchange and boundary conditions, as well as a generic communication library that handles data exchange on a distributed memory system. All these tools are implemented in C++ making extensive use of generic programming and template metaprogramming. The refactored code is shown to outperform the current production code and is performance portable to various hybrid CPU-GPU node architectures.",
      "axisX": "-0.07213834099447798",
      "axisY": "0.48673868427341976",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140104",
      "title": "Communication Complexity of the Fast Multipole Method and its Algebraic Variants",
      "authors": "Rio Yokota, George Turkiyyah, David Keyes",
      "keywords": "Communication Complexity of the Fast Multipole Method and its Algebraic Variantscommunication complexity, hierarchical low-rank approximation, fast multipole\nmethods, H-matrices, sparse solversA combination of hierarchical tree-like data structures and data access patterns from fast multipole methods and hierarchical low-rank approximation of linear operators from H-matrix methods appears to form an algorithmic path forward for efficient implementation of many linear algebraic operations of scientific computing at the exascale. The combination provides asymptot- ically optimal computational and communication complexity and applicability to large classes of operators that commonly arise in scientific computing applications. A convergence of the mathe- matical theories of the fast multipole and H-matrix methods has been underway for over a decade. We recap this mathematical unification and describe implementation aspects of a hybrid of these two compelling hierarchical algorithms on hierarchical distributed-shared memory architectures, which are likely to be the first to reach the exascale. We present a new communication complexity estimate for fast multipole methods on such architectures. We also show how the data structures and access patterns of H-matrices for low-rank operators map onto those of fast multipole, leading to an algebraically generalized form of fast multipole that compromises none of its architecturally ideal properties.",
      "axisX": "0.39409538952725787",
      "axisY": "-0.06691544334316854",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140105",
      "title": "Model-Driven One-Sided Factorizations on Multicore Accelerated Systems",
      "authors": "Jack Dongarra, Azzam Haidar, Jakub Kurzak, Piotr Luszczek, Stanimire Tomov, Asim YarKhan",
      "keywords": "Model-Driven One-Sided Factorizations on Multicore Accelerated Systemshardware accelerators, dense linear algebra, task superscalar schedulingHardware heterogeneity of the HPC platforms is no longer considered unusual but instead have become the most viable way forward towards Exascale.\u00a0 In fact, the multitude of the heterogeneous resources available to modern computers are designed for different workloads and their efficient use is closely aligned with the specialized role envisaged by their design.\u00a0 Commonly in order to efficiently use such GPU resources, the workload in question must have a much greater degree of parallelism than workloads often associated with multicore processors (CPUs).\u00a0 Available GPU variants differ in their internal architecture and, as a result, are capable of handling workloads of varying degrees of complexity and a range of computational patterns.\u00a0 This vast array of applicable workloads will likely lead to an ever accelerated mixing of multicore-CPUs and GPUs in multi-user environments with the ultimate goal of offering adequate computing facilities for a wide range of scientific and technical workloads.\u00a0 In the following paper, we present a research prototype that uses a lightweight runtime environment to manage the resource-specific workloads, and to control the dataflow and parallel execution in hybrid systems.\u00a0 Our lightweight runtime environment uses task superscalar concepts to enable the developer to write serial code while providing parallel execution.\u00a0 This concept is reminiscent of dataflow and systolic architectures in its conceptualization of a workload as a set of side-effect-free tasks that pass data items whenever the associated work assignment have been completed.\u00a0 Additionally, our task abstractions and their parametrization enable uniformity in the algorithmic development across all the heterogeneous resources without sacrificing precious compute cycles.\u00a0 We include performance results for dense linear algebra functions which demonstrate the practicality and effectiveness of our approach that is aptly capable of full utilization of a wide range of accelerator hardware.",
      "axisX": "-0.06179432193200312",
      "axisY": "0.4648779002183676",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140106",
      "title": "Exascale Storage Systems -- An Analytical Study of Expenses",
      "authors": "Julian Martin Kunkel, Michael Kuhn, Thomas Ludwig",
      "keywords": "Exascale Storage Systems -- An Analytical Study of ExpensesParallel I/O, Exascale, data center, storage expensesThe computational power and storage capability of supercomputers are growing at a different pace, with storage lagging behind; the widening gap necessitates new approaches to keep the investment and running costs for storage systems at bay. In this paper, we aim to unify previous models and compare different approaches for solving these problems.By extrapolating the characteristics of the German Climate Computing Center's previous supercomputers to the future, cost factors are identified and quantified in order to foster adequate research and development.Using models to estimate the execution costs of two prototypical use cases, we are discussing the potential of three concepts: re-computation, data deduplication and data compression.",
      "axisX": "-0.5242102242131674",
      "axisY": "-0.41864519082225415",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180401",
      "title": "Adaptive Load Balancing in the Modified Mind Evolutionary Computation Algorithm",
      "authors": "Maxim K. Sakharov, Anatoly P. Karpenko",
      "keywords": "Adaptive Load Balancing in the Modified Mind Evolutionary Computation Algorithmload balancing, landscape analysis, mind evolutionary computation, global optimizationThe\u00a0paper presents an adaptive load balancing method for the modified parallel Mind Evolutionary Computation (MEC) algorithm. The proposed method takes into account an objective function's topology utilizing the information obtained during the landscape analysis stage as well as the information on available computational resources. The modified\u00a0MEC algorithm and proposed static load balancing method are designed for loosely coupled parallel computing systems and imply a minimal number of interactions between computational nodes when solving global optimization problems. A description of the proposed method is presented in this work along with the results of computational experiments, which were carried out with a use of multi-dimensional benchmark functions of various classes. Obtained results demonstrate that an effective use of available computational resources in the proposed method helps finding a better solution comparing to the traditional parallel\u00a0MEC algorithm balancing. Further development of the proposed method requires more advanced termination criteria in order to avoid excessive iterations.",
      "axisX": "0.02149405760415459",
      "axisY": "-0.13185720746087706",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180402",
      "title": "Multicore Platform Efficiency Across Remote Sensing Applications",
      "authors": "Ekaterina O. Tyutlyaeva, Alexander A. Moskovsky, Igor O. Odintsov, Sergey S. Konyukhov, Alexey A. Poyda, Mikhail N. Zhizhin, Igor V. Polyakov",
      "keywords": "Multicore Platform Efficiency Across Remote Sensing Applicationsenergy consumption, performance analysis and optimization, cross-platform analysis, energy consumption analysis, multispectral image processing, nighttime remote sensingA wide range of modern system architectures and platforms targeted for different algorithms and application areas is now available.Even general-purpose systems have advantages in some computation areas and bottlenecks in another. Scientific applications on specific areas, on the other hand, have different requirements for CPU performance, scalability and power consumption.The best practice now is algorithm/architecture co-exploration approach, where scientific problem requirements influence the hardware configuration; on the other hand, algorithm implementation is re factored and optimized in accordance with the platform architectural features.In this research, two typical modules used for multispectral nighttime satellite image processing are studied:\u2022 measurement of local perceived sharpness in visible band using the Fourier transform;\u2022 cross-correlation in a moving window between visible and infrared bands.Both modules are optimized and studied on wide range of up-to-date testbeds, based on different architectures. Our testbeds include computational nodes based on Intel Xeon E5-2697A v4, Intel Xeon Phi, Texas Instruments Sitara AM5728 dual-core ARM Cortex-A15, and NVIDIA JETSON TX2.The study includes performance testing and energy consumption measurements. The results achieved can be used for assessing serviceability for multispectral nighttime satellite image processing by two key parameters: execution time and energy consumption.",
      "axisX": "0.279911132495569",
      "axisY": "0.6820874860920547",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180403",
      "title": "A Study on Cross-Architectural Modelling of Power Consumption Using Neural Networks",
      "authors": "Vadim V. Elisseev, Milos Puzovic, Eun Kyung Lee",
      "keywords": "A Study on Cross-Architectural Modelling of Power Consumption Using Neural Networkshpc, power consumption, modelling, exascale, cross-architectural, neural networksOn the path to Exascale, the goal of High Performance Computing (HPC) to achieve maximum performance becomes the goal of achieving maximum performance under strict power constraint. Novel approaches to hardware and software co-design of modern HPC systems have to be developed to address such challenges. In this paper, we study prediction of power consumption of HPC systems using metrics obtained from hardware performance counters. We argue that this methodology is portable across different micro architecture implementations and compare results obtained on Intel 64, IBMR\u00a0and Cavium ThunderXR ARMv8 microarchitectures.We discuss optimal number and type of hardware performance counters required to accurately predict power consumption.We compare accuracy of power predictions provided by models based on Linear Regression (LR) and Neural Networks (NN). We find that the NN-based model provides better accuracy of predictions than the LR model. We also find, that presently it is not yet possible to predict power consumption on a given microarchitecture using data obtained on a different microarchitecture. Results of our work can be used as a starting point for developing unified, cross-architectural models for predicting power consumption.",
      "axisX": "-0.2264976167794578",
      "axisY": "0.07050695691318093",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180404",
      "title": "Autotuning Techniques for Performance-Portable Point Set Registration in 3D",
      "authors": "Piotr Luszczek, Jakub Kurzak, Ichitaro Yamazaki, David Keffer, Vasileios Maroulas, Jack Dongarra",
      "keywords": "Autotuning Techniques for Performance-Portable Point Set Registration in 3Dportable performance engineering, point set registration, autotuning with code generation,\ncombinatorial optimizationWe present an autotuning approach applied to exhaustive performance engineering of the EM-ICP algorithm for the point set registration problem with a known reference. We were able to achieve progressively higher performance levels through a variety of code transformations and an automated procedure of generating a large number of implementation variants. Furthermore, we managed to exploit code patterns that are not common when only attempting manual optimization but which yielded in our tests better performance for the chosen registration algorithm. Finally, we also show how we maintained high levels of the performance rate in a portable fashion across a wide range of hardware platforms including multicore, manycore coprocessors, and accelerators. Each of these hardware classes is much different from the others and, consequently, cannot reliably be mastered by a single developer in a short time required to deliver a close-to-optimal implementation. We assert in our concluding remarks that our methodology as well as the presented tools provide a valid automation system for software optimization tasks on modern HPC hardware.",
      "axisX": "-0.33292577179373",
      "axisY": "0.10503154365764547",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180405",
      "title": "Benchmarking Quantum Chemistry Methods in Calculations of Electronic Excitations",
      "authors": "Bella L. Grigorenko, Vladimir A. Mironov, Igor V. Polyakov, Alexander V. Nemukhin",
      "keywords": "Benchmarking Quantum Chemistry Methods in Calculations of Electronic Excitationsquantum chemistry, multi-scale approaches, parallel algorithms, fluorescent proteinsQuantum chemistry methods are applied to obtain numerical solutions of the Schr\u00a8odinger equation for molecular systems. Calculations of transitions between electronic states of large molecules present one of the greatest challenges in this field which require the use of supercomputer resources. In this work we describe the results of benchmark calculations of electronic excitation in the protein domains which were designed to engineer novel fluorescent markers operating in the near-infrared region. We demonstrate that such complex systems can be efficiently modeled with the hybrid qunatum mechanics/molecular mechanics approach (QM/MM) using the modern supercomputers. More specifically, the time-dependent density functional theory (TD-DFT) method was primarily tested with respect to its performance and accuracy. GAMESS (US) and NWChem software were benchmarked in direct and storage-based TDDFT calculations with the hybrid B3LYP density functional, both showing good scaling up to 32 nodes. We note that conventional SCF calculations greatly outperform direct SCF calculations for our test system. Accuracy of TD-DFT excitation energies was estimated by a comparison to the more accurate ab initio XMCQDPT2 method.",
      "axisX": "0.5554278577360632",
      "axisY": "-0.0932983511096456",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180406",
      "title": "Developing Quasi-Steady Model for Studying Hemostatic Response Using Supercomputer Technologies",
      "authors": "Petr V. Trifanov, Valeria N. Kaneva, Sergei V. Strijhak, Mikhail A. Panteleev, Fazoil I. Ataullakhanov, Joanne Dunster, Vadim V. Voevodin, Dmitry Yu. Nechipurenko",
      "keywords": "Developing Quasi-Steady Model for Studying Hemostatic Response Using Supercomputer Technologiescfd, particle-based model, biorheology, thrombosis, program profilingFormation of the platelet plug represents a primary response to the vessel wall injury, but may also result in vessel occlusion. The decrease of the local blood flow due to platelet thrombus formation may lead to serious complications, such as ischemic stroke and myocardial infarction. However, mechanisms responsible for regulation of thrombus dynamics are not clear. In order to get a deeper insight into the role of blood flow and platelet interactions in the formation of the primary platelet plug we developed a particle-based model of microvascular thrombosis using quasisteady flow approximation. In order to simulate thrombus dynamics at physiologically relevant timescales of several minutes, we took advantage of the supercomputer technologies. Our in silico analysis revealed the importance of platelet size heterogeneity for describing experimental data on microvascular thrombus formation. Thus, our model represents a useful tool for the supercomputeraided computational analysis of thrombus dynamics in the microvessels on physiologically relevant timescales.",
      "axisX": "0.19687247684842088",
      "axisY": "-0.5007616828660918",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180407",
      "title": "New Binding Mode of SLURP Protein to a7 Nicotinic Acetylcholine Receptor Revealed by Computer Simulations",
      "authors": "Igor D. Diankin, Denis S. Kudryavtsev, Arthur O. Zalevsky, Victor I. Tsetlin, Andrey V. Golovin",
      "keywords": "New Binding Mode of SLURP Protein to a7 Nicotinic Acetylcholine Receptor Revealed by Computer Simulationsmolecular dynamics, gromacs, clustering, affinity propagation, protein docking,\nbiomolecules\u00a0SLURP-1 is a member of three-finger toxin-like proteins. Their characteristic feature is a set of three beta strands extruding from hydrophobic core stabilized by disulfide bonds. Each beta-strand carries a flexible loop, which is responsible for recognition. SLURP-1 was recently shown to act as an endogenous growth regulator of keratinocytes and tumor suppressor by reducing cell migration and invasion by antagonizing the pro-malignant effects of nicotine. This effect is achieved through allosteric interaction with alpha7 nicotinic acetylcholine receptors (alpha-7 nAChRs) in an antagonist-like manner. Moreover, this interaction is unaffected by several well-known agents specifically alpha-bungarotoxin.In this work, we carry out the conformational analysis of the SLURP-1 by a microsecond-long full-atom explicit solvent molecular dynamics simulations followed by clustering, to identify representative states. To achieve this timescale we employed a GPU-accelerated version of GROMACS modeling package. To avoid human bias in clustering we used a non-parametric clustering algorithm Affinity Propagation adapted for biomolecules and HPC environments. Then, we applied protein-protein molecular docking of the ten most massive clusters to alpha7-nAChRs in order to test if structural variability can affect binding. Docking simulations revealed the unusual binding mode of one of the minor SLURP-1 conformations.\u00a0\u00a0",
      "axisX": "0.48888067137953434",
      "axisY": "-0.20354792310272998",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180408",
      "title": "Supercomputer Simulations of Fluid-Structure Interaction Problems Using an Immersed Boundary Method",
      "authors": "Natalya S. Zhdanova, Andrey V. Gorobets, Ilya V. Abalakin",
      "keywords": "Supercomputer Simulations of Fluid-Structure Interaction Problems Using an Immersed Boundary MethodParallel CFD, immersed boundary method, unstructured mesh, turbulent flow,\nMPI+OpenMPThe paper describes a supercomputer application in simulations of fluid-structure interaction problems. A compressible flow solver based on a high-accuracy scheme for unstructured hybrid meshes is considered. It combines an immersed boundary method with a dynamic mesh adaptation method in order to represent motion of solid objects in a turbulent flow. The use of immersed boundaries allows you to dynamically adapt the mesh resolution near moving solid surfaces without changing the mesh topology. Multilevel MPI + OpenMP parallelization of these components fits well with the architecture of modern cluster systems. The proposed implementation can engage thousands of CPU cores in one simulation efficiently. An example application is presented in which a high-speed turbulent flow around a cavity with a deflector is simulated.",
      "axisX": "0.399433881876316",
      "axisY": "0.40980659737806163",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180409",
      "title": "Test of Computational Approaches for Gold-Thiolate Clusters Calculation",
      "authors": "Nadezhda N. Nikitina, Daria A. Pichugina, Alexander V. Oleynichenko, Oxana N. Ryzhova, Kirill E. Kopylov, Vladimir V. Krotov, Nikolay E. Kuz\u2019menko",
      "keywords": "Test of Computational Approaches for Gold-Thiolate Clusters Calculationdensity functional theory, parallel calculation, cluster, gold, dispersion correction,\nrelativistic effects, computational chemistryHigh-level procedures (MP2, CCSD, CCSD(T)) and reliable experimental data have been used to assess the performance of a variety of exchange-correlation functionals for the calculation of structures and energies of small models of thiolate-protected gold clusters. Clusters represent rather complicated objects for examination, therefore the simple models including Au2, AuS were considered to find an appropriate method to calculate Au-Au and Au-S interactions in protected clusters. The mean unsigned errors of the quantum chemical methods were evaluated via reliable experimental bond distances and dissociation energies of Au2 and AuS. Based on the calculation, the SVWN5, TPSS+D3, PBE96+D3, and PBE0+D3 were found to give the most reliable results and can be recommended for calculation of the structure and properties of thiolate-protected gold clusters. The influence of the relativistic corrections calculated in Dirac-Coulomb-Breit framework and inclusion of dispersion corrections on the structure and energy of thiolate-protected gold clusters have been analyzed.",
      "axisX": "0.4572306927132439",
      "axisY": "-0.4867458694143408",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180410",
      "title": "Supercomputer Modeling of Dual-Site Acetylcholinesterase (AChE) Inhibition",
      "authors": "Sofya V. Lushchekina, Galina F. Makhaeva, Dana A. Novichkova, Irina V. Zueva, Nadezhda V. Kovaleva, Rudy R. Richardson",
      "keywords": "Supercomputer Modeling of Dual-Site Acetylcholinesterase (AChE) Inhibitionacetylcholinesterase, Alzheimer\u2019s disease, molecular docking, atomic chargesMolecular docking is one of the most popular tools of molecular modeling. However, in certain cases, like development of inhibitors of cholinesterases as therapeutic agents for Alzheimer's disease, there are many aspects, which should be taken into account to achieve accurate docking results. For simple molecular docking with popular software and standard protocols, a personal computer is sucient, however quite often the results are irrelevant. Due to the complex biochemistry and biophysics of cholinesterases, computational research should be supported with quantum mechanics (QM) and molecular dynamics (MD) calculations, what requires the use of supercomputers. Experimental studies of inhibition kinetics can discriminate between dierent types of inhibition\u2014competitive, non-competitive or mixed type\u2014that is quite helpful for assessment of the docking results. Here we consider inhibition of human acetylcholinesterase (AChE) by the conjugate of MB and 2,8-dimethyl-tetrahydro-y-carboline, study its interactions with AChE in relation to the experimental data, and use it as an example to elucidate crucial points for reliable docking studies of bulky AChE inhibitors. Molecular docking results were found to be extremely sensitive to the choice of the X-ray AChE structure for the docking target and the scheme selected for the distribution of partial atomic charges. It was demonstrated that exible docking should be used with an additional caution, because certain protein conformational changes might not correspond with available X-ray and MD data.",
      "axisX": "0.3090192259786691",
      "axisY": "-0.29986435310825604",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180411",
      "title": "Supercomputer Simulations of Dopamine-Derived Ligands Complexed with Cyclooxygenases",
      "authors": "Valentina D. Maslova, Roman V. Reshetnikov, Vladimir V. Bezugolov, Igor I. Lyubimov, Andrey V. Golovin",
      "keywords": "Supercomputer Simulations of Dopamine-Derived Ligands Complexed with Cyclooxygenasesmolecular docking, non-steroidal anti-inflammatory drugs, ibuprofen, dopamine,\ncyclooxygenaseAn in silico approach was adopted to identify potential cyclooxygenase inhibitors through molecular docking studies. Four potentially active molecules were generated by fusion of dopamine with ibuprofen or ketorolac derivatives. The binding mode of the considered ligands to cyclooxygenase-1 and cyclooxygenase-2 isoforms was described using Autodock Vina. Preliminary docking to full cyclooxygenase isoforms\u2019 structures was used to determine possible binding sites for the described dopamine-derived ligands. The following more accurate docking iteration to the described binding sites was used to achieve better conformational sampling. Among the studied molecules, IBU-GABA-DA showed preferable binding to cyclooxygenase active site of cyclooxygenase-1, while IBU-DA bound to peroxidase site of cyclooxygenase-1, making these ibuprofen-comprising ligands a base for further research and design of selective cyclooxygenase-1 inhibitors. Keterolac-derived ligands KET-DA and KET-GABA-DA demonstrated binding to both cyclooxygenase isoforms at a side pocket, which does not relate to any known functional site of cyclooxygenases and needs to be further investigated.",
      "axisX": "0.49162773555084016",
      "axisY": "-0.21909495370139842",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180412",
      "title": "High Performance Computing of Magnetized Galactic Disks",
      "authors": "Sergey A. Khoperskov, Yulia A. Venichenko, Sergey S. Khrapov, Eugene O. Vasiliev",
      "keywords": "High Performance Computing of Magnetized Galactic Disksmagnetohydrodynamics, galactic dynamics, galactic magnetic field, parallelizationA parallel implementation of the magneto-hydrodynamical code for global modeling of the galactic evolution is reported. The code is parallelized by using MPI interface, and it shows ideal scaling up to 200\u2013300 cores on Lomonosov supercomputer with fast interconnect. In the benchmarking of this code, we study the dynamics of a magnetized gaseous disk of a galaxy with a bar. We run a high-resolution 3D magnetohydrodynamic simulation taking into account the Milky Way-like gravitational potential, gas self-gravity and a network of cooling and heating processes in the interstellar medium. By using this simulation the evolution of morphology and enhancement of the magnetic field are explored. In agreement to hydrodynamical models, when the bar is strong enough, the gas develops sharp shocks at the leading side of the bar. In such a picture we found that when typically the magnetic field strength traces the location of the largescale shocks along the bar major axis, the magnetic field pressure weakens the shocks and reduces the inflow of gas towards the galactic center.",
      "axisX": "0.28257800719860743",
      "axisY": "-0.02753679787059733",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180413",
      "title": "Regional Climate Model for the Lower Volga: Parallelization Efficiency Estimation",
      "authors": "Alexander V. Titov, Alexander V. Khoperskov",
      "keywords": "Regional Climate Model for the Lower Volga: Parallelization Efficiency Estimationregional climate model, domain size, simulations, parallelizationWe have deployed the regional climate model (RCM) RegCM 4.5 for the Lower Volga and adjacent territories with a horizontal spatial resolution of 20 km. The problems of choosing the computational domain in the RCM RegCM version 4.5 are considered. We demonstrate the influence of this factor on the forecast of rainfall distribution in the numerical simulations. The study of rainfall and snowfall is a more demanding test in comparison with temperature or pressure distributions. We investigate dependencies of calculation time, parallel speedup and parallelization efficiency on the number of processes for different multi-core CPUs. Our analysis of the efficiency of parallel implementation of RegCM for various multi-core and multi-processor systems show a strong dependence of the simulation speed on the CPU type. The best effect is achieved when the number of CPU threads and the number of parallel processes are equal. The parallel code speedup is in the range of 1.8 \u2013 11 for different CPUs.",
      "axisX": "0.08744035617705091",
      "axisY": "-0.3827785138153756",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180414",
      "title": "Performance Analysis of Different Computational Architectures: Molecular Dynamics in Application to Protein Assemblies, Illustrated by Microtubule and Electron Transfer Proteins",
      "authors": "Vladimir A. Fedorov, Ekaterina G. Kholina, Ilya B. Kovalenko, Nikita B. Gudimchuk",
      "keywords": "Performance Analysis of Different Computational Architectures: Molecular Dynamics in Application to Protein Assemblies, Illustrated by Microtubule and Electron Transfer Proteinsmolecular dynamics, tubulin, microtubule, plastocyanin-cytochrome fAll-atom molecular dynamics simulation represents a computationally challenging, but powerful approach for studying conformational changes and interactions of biomolecules and their assemblies of different kinds. Usually, the numbers of simulated particles in modern molecular dynamics studies range from thousands to tens of millions, while the simulated timescales span from nanoseconds to microseconds. \u00a0For cost and computation efficiency, it is important to determine the optimal computer hardware for simulations of biomolecular systems of different size and timescale. Here we compare performance and scalability of 17 commercially available computational architectures, using molecular dynamics simulations of water and two different protein systems in GROMACS-5 package as computing benchmarks. We report typical single-node performance of various combinations of modern CPUs and GPUs, as well as multiple-node performance of \"Lomonosov-2\" supercomputer in molecular dynamics simulations of different protein systems in nanoseconds per day. These data can be used as practical guidelines for selection of optimal computer hardware for various molecular dynamics simulation tasks.\u00a0",
      "axisX": "0.2336922794507638",
      "axisY": "0.09786562508488149",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi180415",
      "title": "Algorithm of the Parallel Sweep Method for Numerical Solution of the Gross\u2013Pitaevskii Equation with Highest Nonlinearities",
      "authors": "Andrey D. Bulygin",
      "keywords": "Algorithm of the Parallel Sweep Method for Numerical Solution of the Gross\u2013Pitaevskii Equation with Highest Nonlinearitiesnonlinear Schrodinger equation, fast parallel algorithm, fully conservative numerical scheme, motion integralIn this paper, we for the first time introduce a numerical scheme the solution of a nonlinear equation of the Gross\u2013Pitaevskii type (GP) or the nonlinear Schrodinger equation (NLSE) with highest nonlinearities, which provides implementation of a complete set of motion integrals. This scheme was parallelly implemented on a non-uniform grid. Propagation of a ring laser beam with non-zero angular momentum in the filamentation mode is studied using the implemented numerical scheme. It is shown, that filaments under exposure to centrifugal forces escape to the periphery. Based on a number of numerical experiments, we have found the universal property of motion integrals in the non-conservative case for a given class of equations. Research of dynamics of angular momentum for a dissipative case are also presented. We found, that angular moment, particularly normed by initial energy during filamentation process, is quasi-constant.",
      "axisX": "0.5567159356151532",
      "axisY": "-0.1407931945398142",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190101",
      "title": "Efficient Parallel Implementation of Multi-Arrival 3D Prestack Seismic Depth Migration",
      "authors": "Alexander L. Pleshkevich, Anton V. Ivanov, Vadim D. Levchenko, Sergey A. Khilkov, Boris P. Moroz",
      "keywords": "Efficient Parallel Implementation of Multi-Arrival 3D Prestack Seismic Depth Migrationseismic imaging, multi-arrival seismic migration, HPC, aiwlibThe goal of seismic migration is to reconstruct the image of Earth's depth inhomogeneities on the base of seismic data. Seismic data is obtained using shots in shallow wells that are located in a dense grid points. Those shots could be considered as special point sources. A reflected and scattered seismic waves from the depth inhomogeneities are received by geophones located also in a dense grid points on a surface. A seismic image of depth inhomogeneities can be constructed based on these waves. The implementation of 3-D seismic migration implies the solution of about 104\u00f75 3-D direct problems of wave propagation. Hence efficient asymptotic methods are of a great practical importance. The multi-arrival 3-D seismic migration program is implemented based on a new asymptotic method. It takes into account multi-pass wave propagation and caustics. The program uses parallel calculations in an MPI environment on hundreds and thousands of processor cores. The program was successfully tested on an international synthetic \"SEG salt\" data set and on real data. A seismic image cube for Timan-Pechora region is given as an example.",
      "axisX": "-0.06826181959390086",
      "axisY": "-0.2570696835634757",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190102",
      "title": "LAMMPS Code Simulation of the Defect Formation Induced by Ion Incidence in Carbon Nanotubes",
      "authors": "Andrey A. Shemukhin, Anton V. Nazarov, Anton V. Stepanov",
      "keywords": "LAMMPS Code Simulation of the Defect Formation Induced by Ion Incidence in Carbon Nanotubesion irradiation, multiwall carbon nanotube, defects, molecular dynamics, sputtering, thermal mechanismA molecular dynamic calculation of the multi-walled carbon nanotube thermal sputtering induced by ion irradiation is carried out. Sputtering results comparable to experimental data are obtained. There are two models of ion and thermal sputtering discussed in the paper. The simulation tested the model of thermal amorphization and revealed that the disordering of multi-walled carbon nanotubes structure occurs as a result of their heating under ion irradiation. Classical molecular dynamic simulation was performed using LAMMPS code. Simulation cell with 14 layers multi-walled carbon nanotube 12\u00d712\u00d730 nm size contains 285600 atoms. Multi-walled carbon nanotube was irradiated by 80 keV energy Ar+ ions in cumulative mode. Simulation was performed on the Lomonosov-1 supercomputer. About 24600 nodes-hours were spent on one simulation as a whole. The balancing of MPI ows for a spatial grid of counting nodes occurred according to the scheme 8\u00d78\u00d7128 MPI-stream. LAMMPS code was built with Intel 12.0 compiler. This configuration allowed to speed up the calculation in comparison with the calculation on a single-processor Xeon CPU X5570 2.93 GHz machine by 60 times.",
      "axisX": "0.5565112179337575",
      "axisY": "0.02966036363754817",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190103",
      "title": "A Fully Conservative Parallel Numerical Algorithm with Adaptive Spatial Grid for Solving Nonlinear Diffusion Equations in Image Processing",
      "authors": "Andrey D. Bulygin, Denis A. Vrazhnov",
      "keywords": "A Fully Conservative Parallel Numerical Algorithm with Adaptive Spatial Grid for Solving Nonlinear Diffusion Equations in Image ProcessingPerona{Malik method, nonlinear Schr\u007f odinger equation, fast parallel algorithm,\nfully conservative numerical schemeIn this paper we present simple yet efficient parallel program implementation of grid-difference method for solving nonlinear parabolic equations, which satisfies both fully conservative property and second order of approximation on non-uniform spatial grid according to geometrical sanity of a task. The proposed algorithm was tested on Perona\u2013Malik method for image noise ltering task based on differential equations. Also in this work we propose generalization of the Perona\u2013Malik equation, which is a one of diffusion in complex-valued region type. This corresponds to the conversion to such types of nonlinear equations like Leontovich\u2013Fock equation with a dependent on the gradient field according to the nonlinear law coefficient of diffraction. This is a special case of generalization of the Perona\u2013Malik equation to the multicomponent case. This approach makes noise removal process more flexible by increasing its capabilities, which allows achieving better results for the task of image denoising.",
      "axisX": "0.4730568753050925",
      "axisY": "-0.2684363862103726",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190104",
      "title": "Parametrization of the Elastic Network Model Using High-Throughput Parallel Molecular Dynamics Simulations",
      "authors": "Philipp S. Orekhov, Ilya V. Kirillov, Vladimir A. Fedorov, Ilya B. Kovalenko, Nikita B. Gudimchuk, Artem A. Zhmurov",
      "keywords": "Parametrization of the Elastic Network Model Using High-Throughput Parallel Molecular Dynamics Simulationsmolecular dynamics, coarse-grained models, tubulin, elastic network model, highthroughput simulations, parallel simulationsEven when modern computational platforms and parallel techniques are used, conventional all-atom simulations are limited both in terms of reachable timescale and number of atoms in the biomolecular system of interest. On the other hand, coarse-grained models, which allow to overcome this limitation, rely on proper and rigorous parametrization of the underlying force field. Here, we present a novel iterative approach for parametrization of coarse-grained models based on direct comparison of equilibrium simulations at all-atom and coarse-grained resolutions. In order to assess the accuracy of our method, we have built and parametrized an elastic network model (ENM) of the tubulin protolament consisting of four monomers. For this system, our method shows good convergence and the parametrized ENM reproduces protein dynamics in a finer way when compared to ENMs parametrized using the conventional approach. The presented method can be extended to other coarse-grained models with a slight adjustment of the equations describing the iterative scheme.",
      "axisX": "0.6814651010964812",
      "axisY": "-0.13447499302537966",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190105",
      "title": "Facilitating HPC Operation and Administration via Cloud",
      "authors": "Chaoqun Sha, Jingfeng Zhang, Lei An, Yongsheng Zhang, Zhipeng Wang, Tomi Ilijas, Nejc Bat, Miha Verlic, Qing Ji",
      "keywords": "Facilitating HPC Operation and Administration via CloudHPC, supercomputer, monitoring, notifications, cloud, operation, administration,\nEasyOPExperiencing a tremendous growth, Cloud Computing offers a number of advantages over other distributed platforms. Introducing the advantages of High Performance Computing (HPC) also brought forward the development of HPCaaS (HPC as a Service), which has mainly focused on flexible access to resources, cost-effectiveness, and the no-maintenance-needed for end-users. Besides providing and using HPCaaS, HPC centers could leverage more from Cloud Computing technology, for instance to facilitate operation and administration of deployed HPC systems, commonly faced by most supercomputer centers.This paper reports the product, EasyOP, developed to realize the idea that one or more Cloud or HPC facilities can be run over a centralized and unified control platform. The main purpose of EasyOP is that the information of HPC systems hardware and system software, failure alarms, jobs scheduling, etc. is sent to the Wuxi cloud computing center. After a series of analysis and processing, we are able to share many valuable data, including alarm and job scheduling status, to HPC users through SMS, email, and WeChat. More importantly, with the data accumulated on the cloud computing center, EasyOP can offer several easy-to-use functions, such as user(s) management, monthly/yearly reports, one-screen monitoring and so on. By the end of 2016, EasyOP successfully served more than 50 HPC systems with almost 10000 nodes and over of 300 regular users.",
      "axisX": "-0.6426028734183421",
      "axisY": "0.21502616851240935",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190106",
      "title": "Performance Evaluation of Different Implementation Schemes of an Iterative Flow Solver on Modern Vector Machines",
      "authors": "Kenta Yamaguchi, Takashi Soga, Yoichi Shimomura, Thorsten Reimann, Kazuhiko Komatsu, Ryusuke Egawa, Akihiro Musa, Hiroyuki Takizawa, Hiroaki Kobayashi",
      "keywords": "Performance Evaluation of Different Implementation Schemes of an Iterative Flow Solver on Modern Vector Machinesperformance evaluation, legacy code, numerical \nuid dynamics simulation, vectorization, hyperplane method, red-black methodModern supercomputers consist of multi-core processors, and these processors have recently employed vector instructions, or so-called SIMD instructions, to improve performances. Numerical simulations need to be vectorized in order to achieve higher performance on these processors. Various legacy numerical simulation codes that have been utilized for a long time often contain two versions of source codes: a non-vectorized version and a vectorized version that is optimized for old vector supercomputers. It is important to clarify which version is better for modern supercomputers in order to achieve higher performance. In this paper, we evaluate the performances of a legacy fluid dynamics simulation code called FASTEST on modern supercomputers in order to provide a guidepost for migrating such codes to modern supercomputers. The solver has a nonvectorized version and a vectorized version, and the latter uses the hyperplane ordering method for vectorization. For the evaluation, we also implement the red-black ordering method, which is another way to vectorize the solver. Then, we examine the performance on NEC SX-ACE, SXAurora TSUBASA, Intel Xeon Gold, and Xeon Phi. The results show that the shortest execution times are with the red-black ordering method on SX-ACE and SX-Aurora TSUBASA, and with the non-vectorized version on Xeon Gold and Xeon Phi. Therefore, achieving a higher performance on multiple modern supercomputers potentially requires maintenance of multiple code versions. We also show that the red-black ordering method is more promising to achieve high performance on modern supercomputers.",
      "axisX": "-0.12701143056696562",
      "axisY": "0.2736897291986488",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190107",
      "title": "Comparative Analysis of Virtualization Methods in Big Data Processing",
      "authors": "Gleb I. Radchenko, Ameer B. A. Alaasam, Andrei N. Tchernykh",
      "keywords": "Comparative Analysis of Virtualization Methods in Big Data ProcessingBig Data, visualization, containerization, cloud computing, Xen, KVM, Docker,\norchestrationCloud computing systems have become widely used for Big Data processing, providing access to a wide variety of computing resources and a greater distribution between multi-clouds. This trend has been strengthened by the rapid development of the Internet of Things (IoT) concept. Virtualization via virtual machines and containers is a traditional way of organization of cloud computing infrastructure. Containerization technology provides a lightweight virtual runtime environment. In addition to the advantages of traditional virtual machines in terms of size and flexibility, containers are particularly important for integration tasks for PaaS solutions, such as application packaging and service orchestration. In this paper, we overview the current state-of-the-art of virtualization and containerization approaches and technologies in the context of Big Data tasks solution. We present the results of studies which compare the efficiency of containerization and virtualization technologies to solve Big Data problems. We also analyze containerized and virtualized services collaboration solutions to support automation of the deployment and execution of Big Data applications in the cloud infrastructure.",
      "axisX": "-0.427199007877266",
      "axisY": "0.44407894744548365",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190201",
      "title": "Supercomputer Lomonosov-2: Large Scale, Deep Monitoring and Fine Analytics for the User Community",
      "authors": "Vladimir V. Voevodin, Alexander S. Antonov, Dmitry A. Nikitenko, Pavel A. Shvets, Sergey I. Sobolev, Igor Yu. Sidorov, Konstantin S. Stefanov, Vadim V. Voevodin, Sergey A. Zhumatiy",
      "keywords": "Supercomputer Lomonosov-2: Large Scale, Deep Monitoring and Fine Analytics for the User Communitysupercomputer, peak performance, sustained performance, e\u000eciency, parallel computing, supercomputer center, software tools, scalability, monitoring, system level data, data analyticsThe huge number of hardware and software components, together with a large number of parameters affecting the performance of each parallel application, makes ensuring the efficiency of a large scale supercomputer extremely difficult. In this situation, all basic parameters of the supercomputer should be constantly monitored, as well as many decisions about its functioning should be made by special software automatically. In this paper we describe the tight connection between complexity of modern large high performance computing systems and special techniques and tools required to ensure their efficiency in practice. The main subsystems of the developed complex (Octoshell, DiMMoN, Octotron, JobDigest, and an expert software system to bring fine analytics on parallel applications and the entire supercomputer to users and sysadmins) are actively operated on the large supercomputer systems at Lomonosov Moscow State University. A brief description of the architecture of Lomonosov-2 supercomputer is presented, and questions showing both a wide variety of emerging complex issues and the need for an integrated approach to solving the problem of effectively supporting large supercomputer systems are discussed.",
      "axisX": "-0.6918741841990903",
      "axisY": "-0.08792517604702307",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190202",
      "title": "HPC Processors Benchmarking Assessment for Global System Science Applications",
      "authors": "Damian Kaliszan, Norbert Meyer, Sebastian Petruczynik, Michael Gienger, Sergiy Gogolenko",
      "keywords": "HPC Processors Benchmarking Assessment for Global System Science ApplicationsGlobal Systems Science, HPC benchmarks, parallel applications, e-Infrastructure\nevaluationThe work undertaken in this paper was done in the Centre of Excellence for Global Systems Science (CoeGSS) \u2013 an interdisciplinary project funded by the European Commission. CoeGSS project provides a computer-aided decision support in the face of global challenges (e.g. development of energy, water and food supply systems, urbanisation processes and growth of the cities, pandemic control, etc.) and tries to bring together HPC and global systems science. This paper presents a proposition of GSS benchmark which evaluates HPC architectures with respect to GSS applications and seeks for the best HPC system for typical GSS software environments. The outcome of the analysis is defining a benchmark which represents the average GSS environment and its challenges in a good way: spread of smoking habits and development of tobacco industry, development of green cars market and global urbanisation processes. Results of the tests that have been run on a number of recently appeared HPC platforms allow comparing processors\u2019 architectures with respect to different applications using execution times, TDPs3 and TCOs4 as the basic metrics for ranking HPC architectures. Finally, we believe that our analysis of the results conveys a valuable information to the broadened GSS audience which might help to determine the hardware demands for their specific applications, as well as to the HPC community which requires a mature benchmark set reflecting requirements and traits of the GSS applications. Our work can be considered as a step into direction of development of such mature benchmark.",
      "axisX": "-0.7287660158175367",
      "axisY": "-0.2919139729887209",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190203",
      "title": "How File-access Patterns Influence the Degree of I/O Interference between Cluster Applications",
      "authors": "Aamer Shah, Chih-Song Kuo, Akihiro Nomura, Satoshi Matsuoka, Felix Wolf",
      "keywords": "How File-access Patterns Influence the Degree of I/O Interference between Cluster Applicationsperformance, I/O, file-access pattern, interference, benchmarkingOn large-scale clusters, tens to hundreds of applications can simultaneously access a parallel file system, leading to contention and, in its wake, to degraded application performance. In this article, we analyze the influence of file-access patterns on the degree of interference. As it is by experience most intrusive, we focus our attention on write-write contention. We observe considerable differences among the interference potentials of several typical write patterns. In particular, we found that if one parallel program writes large output files while another one writes small checkpointing files, then the latter is slowed down when the checkpointing files are small enough and the former is vice versa. Moreover, applications with a few processes writing large output files already can significantly hinder applications with many processes from checkpointing small files. Such effects can seriously impact the runtime of real applications\u2014up to a factor of five in one instance. Our insights and measurement techniques offer an opportunity to automatically classify the interference potential between applications and to adjust scheduling decisions accordingly.",
      "axisX": "-0.5702426355689353",
      "axisY": "-0.4573405254220724",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190204",
      "title": "Investigating the Dirac Operator Evaluation with FPGAs",
      "authors": "Grzegorz Korcyl, Piotr Korcyl",
      "keywords": "Investigating the Dirac Operator Evaluation with FPGAshigh performance computing, FPGA, lattice QCD, Dirac operator evaluationIn recent years, computational capacity of single Field Programmable Gate Array (FPGA) devices as well as their versatility have increased significantly. Adding to that fact, the High Level Synthesis frameworks allowing to program such processors in a high-level language like C++, makes modern FPGA devices a serious candidate as building blocks of a general-purpose High Performance Computing solution. In this contribution we describe benchmarks which we performed using a kernel from the Lattice QCD code, a highly compute-demanding HPC academic code for elementary particle simulations on the newest device from Xilinx, the U250 accelerator card. We describe the architecture of our solution and benchmark its performance on a single FPGA device running in two modes: using either external or embedded memory. We discuss both approaches in detail and provide assessment for the necessary memory throughput and the minimal amount of resources needed to deliver optimal performance depending on the available hardware. Our considerations can be used as guidelines for estimating the performance of some larger, manynode systems.",
      "axisX": "0.12249754600372222",
      "axisY": "0.430083487832152",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190205",
      "title": "Development of a RISC-V-Conform Fused Multiply-Add Floating-Point Unit",
      "authors": "Felix Kaiser, Stefan Kosnac, Ulrich Br\u00fcning",
      "keywords": "Development of a RISC-V-Conform Fused Multiply-Add Floating-Point Unit\noating-point, multiply-add, risc-v, hardware-design, verification, uvm, synthesis,\nasic, gf22fdx, ieee754Despite the fact that the open-source community around the RISC-V instruction set architecture is growing rapidly, there is still no high-speed open-source hardware implementation of the IEEE 754-2008 floating-point standard available. We designed a Fused Multiply-Add Floating-Point Unit compatible with the RISC-V ISA in SystemVerilog, which enables us to conduct detailed optimizations where necessary. The design has been verified with the industry standard simulation-based Universal Verification Methodology using the Specman e Hardware Verification Language. The most challenging part of the verification is the reference model, for which we integrated the Floating-Point Unit of an existing Intel processor using the Function Level Interface provided by Specman e. With the use of Intel's Floating-Point Unit we have a ``known good\" and fast reference model. The Back-End flow was done with Global Foundries' 22 nm Fully-Depleted Silicon-On-Insulator (GF22FDX) process using Cadence tools. We reached 1.8 GHz over PVT corners with a 0.8 V forward body bias, but there is still a large potential for further RTL optimization. A power analysis was conducted with stimuli generated by the verification environment and resulted in 212 mW.",
      "axisX": "-0.12681916093594156",
      "axisY": "0.5593157047129654",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190206",
      "title": "Fully Implicit Time Stepping Can Be Efficient on Parallel Computers",
      "authors": "Brandon Cloutier, Benson K. Muite, Matteo Parsani",
      "keywords": "Fully Implicit Time Stepping Can Be Efficient on Parallel Computersincompressible Navier{Stokes equations, parallel computing, spectral methods, time\nsteppingBenchmarks in high performance computing often involve a single component used in the full solution of a computational problem, such as the solution of a linear system of equations. In many cases, the choice of algorithm, which can determine the components used, is also important when solving a full problem. Numerical evidence suggests that for the Taylor-Green vortex problem at a Reynolds number of 1600, a second order implicit midpoint rule method can require less computational time than the often used linearly implicit Carpenter-Kennedy method for solving the equations of incompressible fluid dynamics for moderate levels of accuracy at the beginning of the flow evolution. The primary reason is that even though the implicit midpoint rule is fully implicit, it can use a small number of iterations per time step, and thus require less computational work per time step than the Carpenter-Kennedy method. For the same number of timesteps, the Carpenter-Kennedy method is more accurate since it uses a higher order timestepping method.",
      "axisX": "0.21002526685631032",
      "axisY": "-0.6117491299930577",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190207",
      "title": "Performance Limits Study of Stencil Codes on Modern GPGPUs",
      "authors": "Ilya S. Pershin, Vadim D. Levchenko, Anastasia Y. Perepelkina",
      "keywords": "Performance Limits Study of Stencil Codes on Modern GPGPUsstencil computations, parallel algorithm, GPU, CUDA, Roo\nine modelWe study the performance limits of different algorithmic approaches to the implementation of a sample problem of wave equation solution with a cross stencil scheme.\u00a0With this, we aim to find the highest limit of the achievable performance efficiency for stencil computing.To estimate the limits, we use a quantitative Roofline model to make a thorough analysis of the performance bottlenecks and develop the model further to account for the latency of different levels of GPU memory.\u00a0These estimates provide an incentive to use spatial and temporal blocking algorithms.\u00a0Thus, we study stepwise, domain decomposition, and domain decomposition with halo algorithms in that order. The knowledge of the limit incites the motivation to optimize the implementation. This led to the analysis of the block synchronization methods in CUDA, which is also provided in the text. \u00a0After all optimizations, we have achieved 90% of the peak performance, which amounts to more than 1 trillion cell updates per second on one consumer level GPU device.",
      "axisX": "0.29840786521180634",
      "axisY": "-0.16195513868609734",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190208",
      "title": "Distinct Element Simulation of Mechanical Properties of Hypothetical CNT Nanofabrics",
      "authors": "Igor A. Ostanin",
      "keywords": "Distinct Element Simulation of Mechanical Properties of Hypothetical CNT Nanofabricsnanofibers, carbon nanotubes, distinct element method, parallel computingA universal framework for modeling composites and fabrics of micro- and nanofibers, such as carbon nanotubes, carbon fibers and amyloid fibrils, is presented. Within this framework, fibers are represented with chains of rigid bodies, linked with elastic bonds. Elasticity of the bonds utilizes recently developed enhanced vector model formalism. The type of interactions between fibers is determined by their nature and physical length scale of the simulation. The dynamics of fibers is computed using the modification of rigid particle dynamics module of the waLBerla multiphysics framework. Our modeling system demonstrates exceptionally high parallel performance combined with the physical accuracy of the modeling. The efficiency of our technique is demonstrated with an illustrative mechanical test on a hypothetical carbon nanotube textile. In this example, the elasticity of the fibers represents the coarse-grained covalent bond within CNT surface, whereas interfiber interactions represent coarse-grained van der Waals forces between cylindrical segments of nanotubes. Numerical simulation demonstrates stability and extremal strength of a hypothetical carbon nanotube fabric.",
      "axisX": "0.6873651589329398",
      "axisY": "-0.2503182466874344",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190301",
      "title": "Collecting and Presenting Reproducible Intranode Stencil Performance: INSPECT",
      "authors": "Julian Hornich, Julian Hammer, Georg Hager, Thomas Gruber, Gerhard Wellein",
      "keywords": "Collecting and Presenting Reproducible Intranode Stencil Performance: INSPECTperformance modeling, performance analysis, stencils, single-node, multi-core,\nECM, Roofline, memory hierarchy, cache effectsStencil algorithms have been receiving considerable interest in HPC research for decades. The techniques used to approach multi-core stencil performance modeling and engineering span basic runtime measurements, elaborate performance models, detailed hardware counter analysis, and thorough scaling behavior evaluation. Due to the plurality of approaches and stencil patterns, we set out to develop a generalizable methodology for reproducible measurements accompanied by state-of-the-art performance models. Our open-source toolchain and collected results are publicly available in the \"Intranode Stencil Performance Evaluation Collection\" (INSPECT). We present the underlying methods, models and tools involved in gathering and documenting the performance behavior of a collection of typical stencil patterns across multiple architectures and hardware configuration options. Our aim is to endow performance-aware application developers with reproducible baseline performance data and validated models to initiate a well-defined process of performance assessment and optimization. All data is available for inspection: source code, produced assembly, performance measurements, hardware performance counter data, single-core and multicore Roofline and ECM (execution-cache-memory) performance models, and machine properties. Deviations between measured performance and performance models become immediately evident and can be investigated. We also give hints as to how INSPECT can be used in practice for custom code analysis.",
      "axisX": "-0.02282989134978167",
      "axisY": "0.29036260889558196",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190302",
      "title": "Supercomputer Docking",
      "authors": "Alexey V. Sulimov, Danil C. Kutov, Vladimir B. Sulimov",
      "keywords": "Supercomputer Dockingdocking, protein\u2013ligand, global optimization, tensor train, force field, quantum\u2013\nchemical methodThis review is based on the peer\u2013reviewed research literature including the author\u2019s own publications devoted to supercomputer docking. The general view on docking and its role at the initial stage of the rational drug design is presented. Molecules of medicine compounds selectively bind to the active site of a protein, which is responsible for the disease progression, and stop it. Docking programs perform positioning of molecules (ligands) in the active site of the protein and estimate the protein\u2013ligand binding energy. The larger this energy is, the less concentration of the respective compound should be used to observe the desired effect. Several classical docking programs are described in short. Examples of the adaptation of existing docking programs to supercomputing and using them for virtual screening of millions of ligands are presented. Two novel generalized docking programs specially designed for multi\u2013core docking of a single ligand on a supercomputer are described shortly. These programs find a sufficiently wide spectrum of low energy minima of a protein\u2013ligand complex in the frame of a given force field. The quasi\u2013docking procedure using the generalized docking program is described. Quasi\u2013docking allows to perform docking with quantum\u2013chemical semiempirical methods. Finally a summary is made based on the materials presented.",
      "axisX": "-0.0011814574233205751",
      "axisY": "-0.39137796772102457",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190303",
      "title": "Automatic Port to OpenACC/OpenMP for Physical Parameterization in Climate and Weather Code Using the CLAW Compiler",
      "authors": "Valentin Clement, Philippe Marti, Xavier Lapillonne, Oliver Fuhrer, William Sawyer",
      "keywords": "Automatic Port to OpenACC/OpenMP for Physical Parameterization in Climate and Weather Code Using the CLAW Compilercompiler, directive, GPU, OpenACC, OpenMP, automatic portIn order to benefit from emerging high-performance computing systems, weather and climate models need to be adapted to run efficiently on different hardware architectures such as accelerators. This is a major challenge for existing community models that represent extremely large codebase written in Fortran. Large parts of the code can be ported using OpenACC compiler directives but for time-critical components such as physical parameterizations, code restructuring and optimizations specific to a hardware architecture are necessary to obtain high performance. In an effort to retain a single source code for multiple target architectures, the CLAW Compiler and the CLAW Single Column Abstraction were introduced. We report on the extension of the CLAW SCA to handle ELEMENTAL functions and subroutines. We demonstrate the new capability on the JSBACH land surface scheme of the ICON climate model. With the extension, JSBACH can be automatically ported to OpenACC or OpenMP for accelerators with minimal to no change to the original code.",
      "axisX": "-0.1703618087847824",
      "axisY": "0.508740813064493",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190304",
      "title": "Optimizing Deep Learning RNN Topologies on Intel Architecture",
      "authors": "Kunal Banerjee, Evangelos Georganas, Dhiraj D. Kalamkar, Barukh Ziv, Eden Segal, Cristina Anderson, Alexander Heinecke",
      "keywords": "Optimizing Deep Learning RNN Topologies on Intel ArchitectureLSTM, Intel Xeon, GEMM, compute-bound kernel, bandwidth-bound kernelRecurrent neural network (RNN) models have been found to be well suited for processing temporal data. In this work, we present an optimized implementation of vanilla RNN cell and its two popular variants: LSTM and GRU for Intel Xeon architecture. Typical implementations of these RNN cells employ one or two large matrix multiplication (GEMM) calls and then apply the element-wise operations (sigmoid/tanh) onto the GEMM results. While this approach is easy to implement by exploiting vendor-optimized GEMM library calls, the data reuse relies on how GEMMs are parallelized and is sub-optimal for GEMM sizes stemming from small minibatch. Also, the element-wise operations are exposed as a bandwidth-bound kernel after the GEMM which is typically a compute-bound kernel. To address this discrepancy, we implemented a parallel blocked matrix GEMM in order to (a) achieve load balance, (b) maximize weight matrix reuse, (c) fuse the element-wise operations after partial GEMM blocks are computed and while they are hot in cache. Additionally, we bring the time step loop in our cell to further increase the weight reuse and amortize the overhead to transform the weights into blocked layout. The results show that our implementation is generally faster than Intel MKL-DNN library implementations, e.g. for RNN, forward pass is up to ~3\u00d7 faster whereas the backward/weight update pass is up to ~5\u00d7 faster. Furthermore, we investigate high-performance implementations of sigmoid and tanh activation functions that achieve various levels of accuracy. These implementations rely on minimax polynomial approximations, rational polynomials, Taylor expansions and exponential approximation techniques. Our vectorized implementations can be flexibly integrated into deep learning computations with different accuracy requirements without compromising performance; in fact, these are able to outperform vectorized and reduced accuracy vendor-optimized (Intel SVML) libraries by 1.6\u20132.6\u00d7 while speep up over GNU libm is close to two orders of magnitude. All our experiments are conducted on Intel\u2019s latest CascadeLake architecture.",
      "axisX": "0.3998653630725436",
      "axisY": "0.018470164345217054",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190305",
      "title": "A Skewed Multi-banked Cache for Many-core Vector Processors",
      "authors": "Hikaru Takayashiki, Masayuki Sato, Kazuhiko Komatsu, Hiroaki Kobayashi",
      "keywords": "A Skewed Multi-banked Cache for Many-core Vector ProcessorsHPC, vector architecture, cache, skewed-associativityAs the number of cores and the memory bandwidth have increased in a balanced fashion, modern vector processors achieve high sustained performances, especially in memory-intensive applications in the fields of science and engineering. However, it is difficult to significantly increase the off-chip memory bandwidth owing to the limitation of the number of input/output pins integrated on a single chip. Under the circumstances, modern vector processors have adopted a shared cache to realize a high sustained memory bandwidth. The shared cache can effectively reduce the pressure to the off-chip memory bandwidth by keeping reusable data that multiple vector cores require. However, as the number of vector cores sharing a cache increases, more different blocks requested from multiple cores simultaneously use the same set. As a result, conflict misses caused by these blocks degrade the performance.In order to avoid increasing the conflict misses in the case of the increasing number of cores, this paper proposes a skewed cache for many-core vector processors. The skewed cache prevents the simultaneously requested blocks from being stored into the same set. This paper discusses how the most important two features of the skewed cache should be implemented in modern vector processors: hashing function and replacement policy. The proposed cache adopts the oddmultiplier displacement hashing for effective skewing and the static re-reference interval prediction policy for reasonable replacing. The evaluation results show that the proposed cache significantly improves the performance of a many-core vector processor by eliminating conflict misses.",
      "axisX": "-0.3495074034012563",
      "axisY": "-0.38366979992634953",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190401",
      "title": "An Energy-aware Dynamic Data Allocation Mechanism for Many-channel Memory Systems",
      "authors": "Masayuki Sato, Takuya Toyoshima, Hikaru Takayashiki, Ryusuke Egawa, Hiroaki Kobayashi",
      "keywords": "An Energy-aware Dynamic Data Allocation Mechanism for Many-channel Memory SystemsDRAM, main memory, low-power mode, address-mapping scheme, energy consumptionA modern memory system is equipped with many memory channels to obtain a high memory bandwidth. To take the advantage of this organization, applications\u2019 data are distributed among the channels and transferred in an interleaved fashion. Although memory-intensive applications benefit from a high bandwidth by many memory channels, applications such as compute-intensive ones do not need the high bandwidth. To reduce the energy consumption for such applications, the memory system has low-power modes. During no memory request, the main memory can enter these modes and reduce energy consumption. However, these applications often cause intermittent memory requests to the channels that handle their data, resulting in not entering the low-power modes. Hence, the memory system cannot enter the low-power modes even though the applications do not need the high bandwidth. To solve this problem, this paper proposes a dynamic data allocation mechanism for many-channel memory systems. This mechanism forces data of such applications to use the specified channels by dynamically changing the address-mapping schemes and migrating the data. As a result, the other channels to which the data are not allocated can have a chance to enter the low-power modes for a long time. Therefore, the proposed mechanism has the potential to reduce the energy consumption of many-channel memory systems. The evaluation results show that this mechanism can reduce the energy consumption by up to 11.8% and 1.7% on average.",
      "axisX": "-0.4362338868618715",
      "axisY": "-0.3607725412448976",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190402",
      "title": "Towards Heterogeneous Multi-scale Computing on Large Scale Parallel Supercomputers",
      "authors": "Saad A. Alowayyed, Maxime Vassaux, Ben Czaja, Peter V. Coveney, Alfons G. Hoekstra",
      "keywords": "Towards Heterogeneous Multi-scale Computing on Large Scale Parallel Supercomputersmulti-scale modelling, surrogate model, computational science, heterogeneous multiscale computing, high performance computing, exascaleNew applications that can exploit emerging exascale computing resources efficiently, while providing meaningful scientific results, are eagerly anticipated. Multi-scale models, especially multi-scale applications, will assuredly run at the exascale. We have established that a class of multi-scale applications implementing the heterogeneous multi-scale model follows, a heterogeneous multi-scale computing (HMC) pattern, which typically features a macroscopic model synchronising numerous independent microscopic model simulations. Consequently, communication between microscopic simulations is limited. Furthermore, a surrogate model can often be introduced between macro-scale and micro-scale models to interpolate required data from previously computed micro-scale simulations, thereby substantially reducing the number of micro-scale simulations. Nonetheless, HMC applications, though versatile, remain constrained by load balancing issues. We discuss two main issues: the a priori unknown and variable execution time of microscopic simulations, and the dynamic number of micro-scale simulations required. We tackle execution time variability using a pilot job mechanism to handle internal queuing and multiple sub-model execution on large-scale supercomputers, together with a data-informed execution time prediction model. To dynamically select the number of micro-scale simulations, the HMC pattern automatically detects and identifies three surrogate model phases that help control the available and used core amount. After relevant phase detection and micro-scale simulation scheduling, any idle cores can be used for surrogate model update or for processor release back to the system. We demonstrate HMC performance by testing it on two representative multi-scale applications. We conclude that, considering the subtle interplay between the macroscale model, surrogate models and micro-scale simulations, HMC provides a promising path towards exascale for many multiscale applications.",
      "axisX": "-0.266162536289849",
      "axisY": "-0.025145245228270285",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190403",
      "title": "Improving Reliability of Supercomputer CFD Codes on Unstructured Meshes",
      "authors": "Andrey V. Gorobets, Pavel A. Bakhvalov",
      "keywords": "Improving Reliability of Supercomputer CFD Codes on Unstructured MeshesCFD, supercomputer, unstructured mesh, data structure, MPI, OpenMP, OpenCLThe paper describes a particular technical solution targeted at improving reliability and quality of a highly-parallel computational fluid dynamics code written in C++. The code considered is based on rather complex high-accuracy numerical methods and models for simulation of turbulent flows on unstructured hybrid meshes. The cost of software errors is very high in largescale supercomputer simulations. Reproducing and localizing errors, especially \u201cmagic\u201d unstable bugs related with wrong memory access, are extremely problematic due to the large amount of computing resources involved. In order to prevent, or at least notably filter out memory bugs, an approach of increased reliability is proposed for representing mesh data and organizing memory access. A set of containers is proposed, which causes no overhead in the release configuration compared to plain arrays. At the same time, it provides throughout access control in the safe mode configuration and additional compile-time protection from programming errors. Furthermore, it is fully compatible with heterogeneous computing within the OpenCL standard. The proposed approach provides internal debugging capabilities that allow us to localize problems directly in a supercomputer simulation.",
      "axisX": "-0.008740457374867722",
      "axisY": "0.331580978812841",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi190404",
      "title": "Survey on Software Tools that Implement Deep Learning Algorithms on Intel/x86 and IBM/Power8/Power9 Platforms",
      "authors": "Denis Shaikhislamov, Andrey Sozykin, Vadim Voevodin",
      "keywords": "Survey on Software Tools that Implement Deep Learning Algorithms on Intel/x86 and IBM/Power8/Power9 PlatformsHPC, neural networks, deep learning frameworks, distributed trainingNeural networks are becoming more and more popular in scientific field and in the industry. It is mostly because new solutions using neural networks show state-of-the-art results in the domains previously occupied by traditional methods, eg. computer vision, speech recognition etc. But to get these results neural networks become progressively more complex, thus needing a lot more training. The training of neural networks today can take weeks. This problems can be solved by parallelization of the neural networks training and using modern clusters and supercomputers, which can significantly reduce the learning time. Today, a faster training for data scientist is essential, because it allows to get the results faster to make the next decision.In this paper we provide an overview of distributed learning provided by the popular modern deep learning frameworks, both in terms of provided functionality and performance. We consider multiple hardware choices: training on multiple GPUs and multiple computing nodes.",
      "axisX": "-0.6028428064567394",
      "axisY": "0.2254412247595342",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200101",
      "title": "State of the Art and Future Trends in Data Reduction for High-Performance Computing",
      "authors": "Kira Duwe, Jakob L\u00fcttgau, Georgiana Mania, Jannek Squar, Anna Fuchs, Michael Kuhn, Eugen Betke, Thomas Ludwig",
      "keywords": "State of the Art and Future Trends in Data Reduction for High-Performance Computingdata reduction, lossless compression, lossy compression, dimensionality reduction,\nadaptive approaches, deduplication, in situ, recomputation, scientific data setResearch into data reduction techniques has gained popularity in recent years as storage capacity and performance become a growing concern. This survey paper provides an overview of leveraging points found in high-performance computing (HPC) systems and suitable mechanisms to reduce data volumes. We present the underlying theories and their application throughout the HPC stack and also discuss related hardware acceleration and reduction approaches. After introducing relevant use-cases, an overview of modern lossless and lossy compression algorithms and their respective usage at the application and file system layer is given. In anticipation of their increasing relevance for adaptive and in situ approaches, dimensionality reduction techniques are summarized with a focus on non-linear feature extraction. Adaptive approaches and in situ compression algorithms and frameworks follow. The key stages and new opportunities to deduplication are covered next. An unconventional but promising method is recomputation, which is proposed at last. We conclude the survey with an outlook on future developments.",
      "axisX": "-0.3326315153703111",
      "axisY": "-0.12551865666959702",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200102",
      "title": "Development of Computational Pipeline Software for Genome/Exome Analysis on the K Computer",
      "authors": "Kento Aoyama, Masanori Kakuta, Yuri Matsuzaki, Takashi Ishida, Masahito Ohue, Yutaka Akiyama",
      "keywords": "Development of Computational Pipeline Software for Genome/Exome Analysis on the K Computerpipeline software, software development, K computer, message passing interface,\ngenome analysis, exome analysisPipeline software that comprise tool and application chains for specific data processing have found extensive utilization in the analysis of several data types, such as genome, in bioinformatics research. Recent trends in genome analysis require use of pipeline software for optimum utilization of computational resources, thereby facilitating efficient handling of large-scale biological data accumulated on a daily basis. However, use of pipeline software in bioinformatics tends to be problematic owing to their large memory and storage capacity requirements, increasing number of job submissions, and a wide range of software dependencies. This paper presents a massive parallel genome/exome analysis pipeline software that addresses these difficulties. Additionally, it can be executed on a large number of K computer nodes. The proposed pipeline incorporates workflow management functionality that performs effectively when considering the task-dependency graph of internal executions via extension of the dynamic task distribution framework. Performance results pertaining to the core pipeline functionality, obtained via evaluation experiments performed using an actual exome dataset, demonstrate good scalability when using over a thousand nodes. Additionally, this study proposes several approaches to resolve performance bottlenecks of a pipeline by considering the domain knowledge pertaining to internal pipeline executions as a major challenge facing pipeline parallelization.\u00a0",
      "axisX": "-0.3210729028300281",
      "axisY": "0.14387245643456328",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200103",
      "title": "Supercomputing Technologies as Drive for Development of Enterprise Information Systems and Digital Economy",
      "authors": "Oleg V. Loginovsky, Alexander L. Shestakov, Alexander A. Shinkarev",
      "keywords": "Supercomputing Technologies as Drive for Development of Enterprise Information Systems and Digital Economyenterprise information systems, parallel computing, supercomputing technologies,\nbig data, machine learning, scalability, event stream, analysis, digital economyThe article presents an analysis of approaches to the development of enterprise information systems that are in use today. One of the major trends that predetermines the agenda of information technology is the focus on parallel computing of large volumes of data using supercomputing technologies. The article considers the resulting ubiquitous move to distributed patterns of building enterprise information systems and avoiding monolithic architectures. The emphasis is placed on the importance of such fundamental characteristics of enterprise information systems as reliability, scalability, and maintainability. The article justifies the importance of machine learning in the context of effective big data analysis and competitive gain for business, vital for both maintaining a leading position in the market and surviving in conditions of global instability and digitalization of economy. Transition from storing the current state of a enterprise information system to storing a full log and history of all changes in the event stream is proposed as an instrument of achieving linearization of the data stream for subsequent parallel computing. There is a new view that is being shaped of specialists at the intersection of engineering and analytical disciplines, who would be able to effectively develop scalable systems and algorithms for data processing and integration of its results into company business processes.",
      "axisX": "-0.6476722547608345",
      "axisY": "-0.004886252938447153",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200104",
      "title": "Online MPI Process Mapping for Coordinating Locality and Memory Congestion on NUMA Systems",
      "authors": "Mulya Agung, Muhammad Alfian Amrizal, Ryusuke Egawa, Hiroyuki Takizawa",
      "keywords": "Online MPI Process Mapping for Coordinating Locality and Memory Congestion on NUMA Systemscommunication, congestion, locality, MPI, multi-core, NUMA, process mappingMapping MPI processes to processor cores, called process mapping, is crucial to achieving the scalable performance on multi-core processors. By analyzing the communication behavior among MPI processes, process mapping can improve the communication locality, and thus reduce the overall communication cost. However, on modern non-uniform memory access (NUMA) systems, the memory congestion problem could degrade performance more severely than the locality problem because heavy congestion on shared caches and memory controllers could cause long latencies. Most of the existing work focus only on improving the locality or rely on offline profiling to analyze the communication behavior.We propose a process mapping method that dynamically performs the process mapping for adapting to communication behaviors while coordinating the locality and memory congestion. Our method works online during the execution of an MPI application. It does not require modifications to the application, previous knowledge of the communication behavior, or changes to the hardware and operating system. Experimental results show that our method can achieve performance and energy efficiency close to the best static mapping method with low overhead to the application execution. In experiments with the NAS parallel benchmarks on a NUMA system, the performance and total energy improvements are up to 34% (18.5% on average) and 28.9% (13.6% on average), respectively. In experiments with two GROMACS applications on a larger NUMA system, the average improvements in performance and total energy consumption are 21.6% and 12.6%, respectively.",
      "axisX": "-0.3375559642146164",
      "axisY": "-0.12111603616383974",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200105",
      "title": "Tools for GPU Computing \u2013 Debugging and Performance Analysis of Heterogenous HPC Applications",
      "authors": "Michael Knobloch, Bernd Mohr",
      "keywords": "Tools for GPU Computing \u2013 Debugging and Performance Analysis of Heterogenous HPC Applicationsperformance analysis, debugging, GPU computingGeneral purpose GPUs are now ubiquitous in high-end supercomputing. All but one (the Japanese Fugaku system, which is based on ARM processors) of the announced (pre-)exascale systems contain vast amounts of GPUs that deliver the majority of the performance of these systems. Thus, GPU programming will be a necessity for application developers using high-end HPC systems.However, programming GPUs efficiently is an even more daunting task than traditional HPC application development. This becomes even more apparent for large-scale systems containing thousands of GPUs. Orchestrating all the resources of such a system imposes a tremendous challenge to developers. Luckily a rich ecosystem of tools exist to assist developers in every development step of a GPU application at all scales.In this paper we present an overview of these tools and discuss their capabilities. We start with an overview of different GPU programming models, from low-level with CUDA over pragma-based models like OpenACC to high-level approaches like Kokkos. We discuss their respective tool interfaces as the main method for tools to obtain information on the execution of a kernel on the GPU. The main focus of this paper is on two classes of tools, debuggers and performance analysis tools. Debuggers help the developer to identify problems both on the CPU and GPU side as well as in the interplay of both. Once the application runs correctly, performance analysis tools can be used to pinpoint bottlenecks in the execution of the code and help to increase the overall performance.",
      "axisX": "-0.3247285886521175",
      "axisY": "0.48990399366236753",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200106",
      "title": "Building a Vision for Reproducibility in the Cyberinfrastructure Ecosystem: Leveraging Community Efforts",
      "authors": "Dylan Chapp, Victoria Stodden, Michela Taufer",
      "keywords": "Building a Vision for Reproducibility in the Cyberinfrastructure Ecosystem: Leveraging Community Effortsreproducibility, replicability, transparency, high-performance computing, molecular\ndynamics, in situ analyticsThe scientific computing community has long taken a leadership role in understanding and assessing the relationship of reproducibility to cyberinfrastructure, ensuring that computational results - such as those from simulations - are \"reproducible\", that is, the same results are obtained when one re-uses the same input data, methods, software and analysis conditions. Starting almost a decade ago, the community has regularly published and advocated for advances in this area. In this article we trace this thinking and relate it to current national efforts, including the 2019 National Academies of Science, Engineering, and Medicine report on \"Reproducibility and Replication in Science\".To this end, this work considers high performance computing workflows that emphasize workflows combining traditional simulations (e.g. Molecular Dynamics simulations) with in situ analytics. We leverage an analysis of such workflows to (a) contextualize the 2019 National Academies of Science, Engineering, and Medicine report's recommendations in the HPC setting and (b) envision a path forward in the tradition of community driven approaches to reproducibility and the acceleration of science and discovery. The work also articulates avenues for future research at the intersection of transparency, reproducibility, and computational infrastructure that supports scientific discovery.",
      "axisX": "-0.5070793513111017",
      "axisY": "-0.14755634846322893",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200201",
      "title": "Performance Reduction For Automatic Development of Parallel Applications  For Reconfigurable Computer Systems",
      "authors": "Alexey I. Dordopulo, Ilya I. Levin",
      "keywords": "Performance Reduction For Automatic Development of Parallel Applications  For Reconfigurable Computer Systemsperformance reduction, hardware costs, reconfigurable computer system, parallel\napplications development, information graphIn the paper, we review a suboptimal methodology of mapping of a task information graph\u00a0on the architecture of a reconfigurable computer system. Using performance reduction methods,\u00a0we can solve computational problems which need hardware costs exceeding the available hardware\u00a0resource. We proved theorems, concerning properties of sequential reductions. In our case, we have\u00a0the following types of reduction such as the reduction by number of basic subgraphs, by number\u00a0of computing devices, and by data width. On the base of the proved theorems and corollaries,\u00a0we developed the methodology of reduction transformations of a task information graph for its\u00a0automatic adaptation to the architecture of a reconfigurable computer system. We estimated the\u00a0maximum number of transformations, which, according to the suggested methodology, are needed\u00a0for balanced reduction of the performance and hardware costs of applications for reconfigurable\u00a0computer systems.",
      "axisX": "-0.001356036830626704",
      "axisY": "0.10781468999879693",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200202",
      "title": "Long Distance Geographically Distributed InfiniBand Based Computing",
      "authors": "Karol Niedzielewski, Marcin Semeniuk, Jaros\u0142aw Skomia\u0142, Jerzy Proficz, Piotr Sumioka, Bartosz Pliszka, Marek Michalewicz",
      "keywords": "Long Distance Geographically Distributed InfiniBand Based ComputingHPC, distributed computing and systems, InfiniBand, federated supercomputing,\ngeographically distributed workflows, ADIOS, HPX, High Performance ParallexCollaboration between multiple computing centres, referred as federated computing is becoming important pillar of High Performance Computing (HPC) and will be one of its key components in the future. To test technical possibilities of future collaboration using 100Gb optic fiber link (Connection was 900 km in length with 9ms RTT time) we prepared two scenarios of operation.In the first one, Interdisciplinary Centre for Mathematical and Computational Modelling (ICM) in Warsaw and Centre of Informatics - Tricity Academic Supercomputer & networK (CI-TASK) in Gda\u0144sk prepared a long distance geographically distributed computing cluster. System consisted of 14 nodes (10 nodes at ICM facility and 4 at TASK facility) connected using InfiniBand. Our tests demonstrate that it is possible to perform computationally intensive data analysis on systems of this class without substantial drop in performance for a certain type of workloads. Additionally, we show that it is feasible to use High Performance Parallex [1], high level abstraction libraries for distributed computing, to develop software for such geographically distributed computing resources and maintain desired efficiency.In the second scenario, we prepared distributed simulation-postprocessing-visualization workflow using ADIOS2 [2] and two programming languages (C++ and python). In this test we prove capabilities of performing different parts of analysis in seperate sites.",
      "axisX": "-0.11625425451051843",
      "axisY": "0.2668739000956543",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200203",
      "title": "Potential of I/O Aware Workflows in Climate and Weather",
      "authors": "Julian M. Kunkel, Luciana R. Pedro",
      "keywords": "Potential of I/O Aware Workflows in Climate and Weatherworkflow, heterogeneous storage, data-driven, climate/weatherThe efficient, convenient, and robust execution of data-driven workflows and enhanced data\u00a0management are essential for productivity in scientific computing. In HPC, the concerns of storage\u00a0and computing are traditionally separated and optimised independently from each other and the\u00a0needs of the end-to-end user. However, in complex workflows, this is becoming problematic. These\u00a0problems are particularly acute in climate and weather workflows, which as well as becoming\u00a0increasingly complex and exploiting deep storage hierarchies, can involve multiple data centres.The key contributions of this paper are: 1) A sketch of a vision for an integrated data-driven\u00a0approach, with a discussion of the associated challenges and implications, and 2) An architecture\u00a0and roadmap consistent with this vision that would allow a seamless integration into current\u00a0climate and weather workflows as it utilises versions of existing tools (ESDM, Cylc, XIOS, and\u00a0DDN\u2019s IME).The vision proposed here is built on the belief that workflows composed of data, computing,\u00a0and communication-intensive tasks should drive interfaces and hardware configurations to\u00a0better support the programming models. When delivered, this work will increase the opportunity\u00a0for smarter scheduling of computing by considering storage in heterogeneous storage systems.\u00a0We illustrate the performance-impact on an example workload using a model built on measured\u00a0performance data using ESDM at DKRZ.",
      "axisX": "-0.5377948994400376",
      "axisY": "0.314811147242267",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200204",
      "title": "Bridging the Architecture Gap: Abstracting Performance-Relevant Properties of Modern Server Processors",
      "authors": "Johannes Hofmann, Christie L. Alappat, Georg Hager, Dietmar Fey, Gerhard Wellein",
      "keywords": "Bridging the Architecture Gap: Abstracting Performance-Relevant Properties of Modern Server Processorsmicroarchitecture comparison, Intel, AMD, ARM, IBM, performance evaluation,\nperformance modeling, analytic modeling, execution-cache-memory modelWe propose several improvements to the execution-cache-memory (ECM) model, an analytic performance model for predicting single- and multicore runtime of steady-state loops on server processors. The model is made more general by strictly differentiating between application and machine models: an application model comprises the loop code, problem sizes, and other runtime parameters, while a machine model is an abstraction of all performance-relevant properties of a processor. Moreover, new first principles underlying the model\u2019s estimates are derived from common microarchitectural features implemented by today\u2019s server processors to make the model more architecture independent, thereby extending its applicability beyond Intel processors. We introduce a generic method for determining machine models, and present results for relevant server-processor architectures by Intel, AMD, IBM, and Marvell/Cavium. Considering this wide range of architectures, the set of features required for adequate performance modeling is surprisingly small. To validate our approach, we compare performance predictions to empirical data for an OpenMP-parallel preconditioned CG algorithm, which includes compute- and memory-bound kernels. Both single- and multicore analysis shows that the model exhibits average and maximum relative errors of 5 % and 10 %. Deviations from the model and insights gained are discussed in detail. ",
      "axisX": "0.008049710942129083",
      "axisY": "-0.03130182882807144",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200205",
      "title": "Dawn: a High-level Domain-Specific Language Compiler Toolchain for Weather and Climate Applications",
      "authors": "Carlos Osuna, Tobias Wicky, Fabian Thuering, Torsten Hoefler, Oliver Fuhrer",
      "keywords": "Dawn: a High-level Domain-Specific Language Compiler Toolchain for Weather and Climate ApplicationsGPGPU computing, DSL, weather and climate, code optimization, compiler, performance portabilityHigh-level programming languages that allow to express numerical methods and generate efficient parallel implementations are of key importance for the productivity of domain-scientists. The diversity and complexity of hardware architectures is imposing a huge challenge for large and complex models that must be ported and maintained for multiple architectures combining various parallel programming models. Several domain-specific languages (DSLs) have been developed to address the portability problem, but they usually impose a parallel model for specific numerical methods and support optimizations for limited scope operators. Dawn provides a high-level concise language for expressing numerical finite difference/volume methods using a sequential and descriptive language. The sequential statements are transformed into an efficient target-dependent parallel implementation by the Dawn compiler toolchain. We demonstrate our approach on the dynamical solver of the COSMO model, achieving performance improvements and code size reduction of up to 2x and 5x, respectively.",
      "axisX": "-0.13242331054892892",
      "axisY": "0.43582546155311735",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200301",
      "title": "Accounting of Receptor Flexibility in Ultra-Large Virtual Screens with VirtualFlow Using a Grey Wolf Optimization Method",
      "authors": "Christoph Gorgulla, Konstantin Fackeldey, Gerhard Wagner, Haribabu Arthanari",
      "keywords": "Accounting of Receptor Flexibility in Ultra-Large Virtual Screens with VirtualFlow Using a Grey Wolf Optimization Methodultra-large virtual screening, molecular docking, drug discovery, COVID-19,\nstructure-based drug design, CADD, computer aided drug design, AutoDock, grey wolf optimization, cloud computingStructure-based virtual screening approaches have the ability to dramatically reduce the time and costs associated to the discovery of new drug candidates. Studies have shown that the true hit rate of virtual screenings improves with the scale of the screened ligand libraries. Therefore, we have recently developed an open source drug discovery platform (VirtualFlow), which is able to routinely carry out ultra-large virtual screenings. One of the primary challenges of molecular docking is the circumstance when the protein is highly dynamic or when the structure of the protein cannot be captured by a static pose. To accommodate protein dynamics, we report the extension of VirtualFlow to allow the docking of ligands using a grey wolf optimization algorithm using the docking program GWOVina, which substantially improves the quality and efficiency of flexible receptor docking compared to AutoDock Vina. We demonstrate the linear scaling behavior of VirtualFlow utilizing GWOVina up to 128 000 CPUs. The newly supported docking method will be valuable for drug discovery projects in which protein dynamics and flexibility play a significant role.",
      "axisX": "-0.19804679769375555",
      "axisY": "-0.33600134135781284",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200302",
      "title": "Perspectives on Supercomputing and Artificial Intelligence Applications in Drug Discovery",
      "authors": "Jun Xu, Jiming Ye",
      "keywords": "Perspectives on Supercomputing and Artificial Intelligence Applications in Drug Discoverydrug discovery, big data, artificial intelligence, HPCThis review starts with outlining how science and technology evaluated from last century into high throughput science and technology in modern era due to the Nobel-Prize-level inventions of combinatorial chemistry, polymerase chain reaction, and high-throughput screening. The evolution results in big data accumulated in life sciences and the fields of drug discovery. The big data demands for supercomputing in biology and medicine, although the computing complexity is still a grand challenge for sophisticated biosystems in drug design in this supercomputing era. In order to resolve the real-world issues, artificial intelligence algorithms (specifically machine learning approaches) were introduced, and have demonstrated the power in discovering structure-activity relations hidden in big biochemical data. Particularly, this review summarizes on how people modernize the conventional machine learning algorithms by combing non-numeric pattern recognition and deep learning algorithms, and successfully resolved drug design and high throughput screening issues. The review ends with the perspectives on computational opportunities and challenges in drug discovery by introducing new drug design principles and modeling the process of packing DNA with histones in micrometer scale space, a n example of how a macrocosm object gets into microcosm world.",
      "axisX": "-0.2409199900433646",
      "axisY": "-0.18974311455032436",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200303",
      "title": "Computational Modeling of the SARS-CoV-2 Main Protease Inhibition by the Covalent Binding of Prospective Drug Molecules",
      "authors": "Alexander V. Nemukhin, Bella L. Grigorenko, Igor V. Polyakov, Sofya V. Lushchekina",
      "keywords": "Computational Modeling of the SARS-CoV-2 Main Protease Inhibition by the Covalent Binding of Prospective Drug MoleculesSARS-CoV2 main protease, QM/MM, molecular docking, molecular dynamics,\ncovalent inhibitorWe illustrate modern modeling tools applied in the computational design of drugs acting as\u00a0covalent inhibitors of enzymes. We take the Main protease (Mpro) from the SARS-CoV-2 virus as\u00a0an important present-day representative. In this work, we construct a compound capable to block\u00a0Mpro, which is composed of fragments of antimalarial drugs and covalent inhibitors of cysteine proteases. To characterize the mechanism of its interaction with the enzyme, the algorithms based\u00a0on force fields, including molecular mechanics (MM), molecular dynamics (MD) and molecular\u00a0docking, as well as quantum-based approaches, including quantum chemistry and quantum mechanics/molecular mechanics (QM/MM) methods, should be applied. The use of supercomputers\u00a0is indispensably important at least in the latter approach. Its application to enzymes assumes that\u00a0energies and forces in the active sites are computed using methods of quantum chemistry, whereas\u00a0the rest of protein matrix is described using conventional force fields. For the proposed compound,\u00a0containing the benzoisothiazolone fragment and the substitute at the uracil ring, we show that it\u00a0can form a stable covalently bound adduct with the target enzyme, and thus can be recommended\u00a0for experimental trials.",
      "axisX": "0.6066425789278754",
      "axisY": "-0.07221180226184643",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200304",
      "title": "Computational Characterization of the Substrate Activation in the Active Site of SARS-CoV-2 Main Protease",
      "authors": "Maria G. Khrenova, Vladimir G. Tsirelson, Alexander V. Nemukhin",
      "keywords": "Computational Characterization of the Substrate Activation in the Active Site of SARS-CoV-2 Main ProteaseSARS-CoV-2 main protease, QM/MM MD, GPU-accelerated algorithms, substrate\nactivationMolecular dynamics simulations with the QM(DFT)/MM potentials are utilized to discriminate\u00a0between reactive and nonreactive complexes of the SARS-CoV-2 main protease and its substrates.\u00a0Classification of frames along the molecular dynamic trajectories is utilized by analysis of\u00a0the 2D maps of the Laplacian of electron density. Those are calculated in the plane formed by the carbonyl group of the substrate and a nucleophilic sulfur atom of the cysteine residue that initiates\u00a0enzymatic reaction. Utilization of the GPU-based DFT code allows fast and accurate simulations with the hybrid functional PBE0 and double-zeta basis set. Exclusion of the polarization functions\u00a0accelerates the calculations 2-fold, however this does not describe the substrate activation. Larger\u00a0basis set with d-functions on heavy atoms and p-functions on hydrogen atoms enables to disclose\u00a0equilibrium between the reactive and nonreactive species along the MD trajectory. The suggested\u00a0approach can be utilized to choose covalent inhibitors that will readily interact with the catalytic\u00a0residue of the selected enzyme.",
      "axisX": "0.8062775246583717",
      "axisY": "-0.03665601420805954",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200305",
      "title": "In Search of Non-covalent Inhibitors of SARS-CoV-2 Main Protease: Computer Aided Drug Design Using Docking and Quantum Chemistry",
      "authors": "Alexey V. Sulimov, Danil C. Kutov, Anna S. Taschilova, Ivan S. Ilin, Nadezhda V. Stolpovskaya, Khidmet S. Shikhaliev, Vladimir B. Sulimov",
      "keywords": "In Search of Non-covalent Inhibitors of SARS-CoV-2 Main Protease: Computer Aided Drug Design Using Docking and Quantum Chemistrydocking, global optimization, quantum docking, inhibitors, CADD, SARS\u2013CoV\u20132,\nCOVID\u201319, MproTwo stages virtual screening of a database containing several thousand low molecular weight organic compounds is performed with the goal to find inhibitors of SARS-CoV-2 main protease. Overall near 41000 different 3D molecular structures have been generated from the initial molecules taking into account several conformers of most molecules. At the first stage the classical SOL docking program is used to determine most promising candidates to become inhibitors. SOL employs the MMFF94 force field, the genetic algorithm (GA) of the global energy optimization, takes into account the desolvation effect arising upon protein-ligand binding and the internal stress energy of the ligand. Parameters of GA are selected to perform the meticulous global optimization, and for docking of one ligand several hours on one computing core are needed on the average. The main protease model is constructed on the base of the protein structure from the Protein Data Bank complex 6W63. More than 1000 ligands structures have been selected for further postprocessing. The SOL score values of these ligands are\u00a0 more negative than the threshold of \u20136.3 kcal/mol obtained for the native X77 ligand docking. Subsequent calculation of the protein-ligand binding enthalpy by the PM7 quantum-chemical semiempirical method with COSMO solvent model have narrowed down the number of best candidates. Finally, the diverse set of 20 most perspective candidates for the in vitro validation are selected.",
      "axisX": "0.32728577753991006",
      "axisY": "-0.39679932889369046",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200306",
      "title": "Computational Approaches To Identify A Hidden Pharmacological Potential In Large Chemical Libraries",
      "authors": "Dmitry S. Druzhilovskiy, Leonid A. Stolbov, Polina I. Savosina, Pavel V. Pogodin, Dmitry A. Filimonov, Alexander V. Veselovsky, Karen Stefanisko, Nadya I. Tarasova, Marc C. Nicklaus, Vladimir V. Poroikov",
      "keywords": "Computational Approaches To Identify A Hidden Pharmacological Potential In Large Chemical Librariesdrug discovery, chemical-pharmacological space, big data analysis, similarity assessment, machine learning, molecular modeling, virtual screening, HIV/AIDS, SAVI, COVID-19To improve the discovery of more effective and less toxic pharmaceutical agents, large virtual repositories of synthesizable molecules have been generated to increase the explored chemical-pharmacological space diversity. Such libraries include billions of structural formulae of drug-like molecules associated with data on synthetic schemes, required building blocks, estimated physical-chemical parameters, etc. Clearly, such repositories are \u201cBig Data\u201d. Thus, to identify the most promising compounds with the required pharmacological properties (hits) among billions of available opportunities, special computational methods are necessary. We have proposed using a combined computational approach, which combines structural similarity assessment, machine learning, and molecular modeling. Our approach has been validated in a project aimed at finding new pharmaceutical agents against HIV/AIDS and associated comorbidities from the Synthetically Accessible Virtual Inventory (SAVI), a 1.75 billion compound database. Potential inhibitors of HIV-1 protease and reverse transcriptase and agonists of toll-like receptors and STING, affecting innate immunity, were computationally identified. The activity of the three synthesized compounds has been confirmed in a cell-based assay. These compounds belong to the chemical classes, in which the agonistic effect on TLR\u00a07/8 had not been previously shown. Synthesis and biological testing of several dozens of compounds with predicted antiretroviral activity are currently taking place at the NCI/NIH. We also carried out virtual screening among one billion substances to find compounds potentially possessing anti-SARS-CoV-2 activity. The selected hits' information has been accepted by the European Initiative \u201cJEDI Grand Challenge against COVID-19\u201d for synthesis and further biological evaluation. The possibilities and limitations of the approach are discussed.",
      "axisX": "0.06598579910472979",
      "axisY": "-0.32727510912522445",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200401",
      "title": "Effects of Using a Memory Stalled Core for Handling MPI Communication Overlapping in the SOR Solver on SX-ACE and SX-Aurora TSUBASA",
      "authors": "Takashi Soga, Kenta Yamaguchi, Raghunandan Mathur, Osamu Watanabe, Akihiro Musa, Ryusuke Egawa, Hiroaki Kobayashi",
      "keywords": "Effects of Using a Memory Stalled Core for Handling MPI Communication Overlapping in the SOR Solver on SX-ACE and SX-Aurora TSUBASAThermal plasma flows, SOR method, MPI, OpenMP, Performance tuning, SXACE, SX-Aurora TSUBASAModern high-performance computing (HPC) systems consist of a large number of nodes featuring\u00a0multi-core processors. Many computational fluid dynamics (CFD) codes utilize a Message\u00a0Passing Interface (MPI) to exploit the potential of such systems. In general, the MPI communication\u00a0costs increase as the number of MPI processes increases. In this paper, we discuss performance\u00a0of the code in which a core is used as a dedicated communication core when the core\u00a0cannot contribute to the performance improvement due to memory-bandwidth limitations. By\u00a0using the dedicated communication core, the communication operations are overlapped with computation\u00a0operations, thus enabling highly efficient computation by exploiting the limited memory\u00a0bandwidth and idle cores. The performance evaluation shows that this code can hide the MPI\u00a0communication times of 90% on the supercomputer SX-ACE system and 80% on the supercomputer\u00a0SX-Aurora TSUBASA system, and the performance of the successive over-relaxation (SOR)\u00a0method is improved by 32% on SX-ACE and 20% on SX-Aurora TSUBASA.",
      "axisX": "0.07419612699052672",
      "axisY": "0.44650573827932183",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200402",
      "title": "Enhancing the in Situ Visualization of Performance Data in Parallel CFD Applications",
      "authors": "Rigel F. C. Alves, Andreas Kn\u00fcpfer",
      "keywords": "Enhancing the in Situ Visualization of Performance Data in Parallel CFD Applicationsparallel computing, performance analysis, in situ processing, computational fluid\ndynamicsThis paper continues the work initiated by the authors on the feasibility of using ParaView as\u00a0visualization software for the analysis of parallel CFD codes\u2019 performance. Current performance\u00a0tools are unable to show their data on top of complex simulation geometries (e.g. an aircraft\u00a0engine). In our previous paper, a plugin for the open-source performance tool Score-P has been\u00a0introduced, which intercepts an arbitrary number of manually selected code regions (mostly functions)\u00a0and send their respective measurements \u2013 amount of executions and cumulative time spent\u00a0\u2013 to ParaView (through its in situ library, Catalyst), as if they are any other flow-related variable.\u00a0This paper adds to such plugin the capacity to also show communication data (messages sent\u00a0between MPI ranks) on top of the CFD mesh. Testing is done again with Rolls-Royce\u2019s in-house\u00a0CFD code, Hydra. The plugin\u2019s original feature (regions\u2019 measurements) is here revisited, in a bigger\u00a0test-case, which is also used to illustrate the new feature (communication data). The benefits\u00a0and overhead of the tool are discussed.",
      "axisX": "-0.06833416348856271",
      "axisY": "0.4138778056129221",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200403",
      "title": "Improving Quantum Annealing Performance on Embedded Problems",
      "authors": "Michael R. Zielewski, Mulya Agung, Ryusuke Egawa, Hiroyuki Takizawa",
      "keywords": "Improving Quantum Annealing Performance on Embedded Problemsquantum annealing, quantum computer, job-shop scheduling, combinatorial optimizationRecently, many researchers are investigating quantum annealing as a solver for real-world combinatorial optimization problems. However, due to the format of problems that quantum annealing solves and the structure of the physical annealer, these problems often require additional setup prior to solving. We study how these setup steps affect performance and provide insight into the interplay among them using the job-shop scheduling problem for our evaluation. We show that the empirical probability of success is highly sensitive to problem setup and that excess variables and large embeddings reduce performance. We then show that certain problem instances are unable to be solved without the use of additional post-processing methods. Finally, we investigate the effect of pausing during the anneal. Our results show that pausing within a certain time window can improve the probability of success, which is consistent with other work. However, we also show that the performance improvement due to pausing can be masked depending on properties of the embedding, and thus, special care must be taken for embedded problems.",
      "axisX": "-0.1532985559510323",
      "axisY": "-0.45134479055505267",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200404",
      "title": "Developing an Architecture-independent Graph Framework for Modern Vector Processors and NVIDIA GPUs",
      "authors": "Ilya V. Afanasyev",
      "keywords": "Developing an Architecture-independent Graph Framework for Modern Vector Processors and NVIDIA GPUsvector computers, NVIDIA GPUs, graph algorithms, graph framework, VGL,\nCUDA, optimisationThis paper describes the first-in-the-world attempt to develop an architectural-independent\u00a0graph framework named VGL, designed for different modern architectures with high-bandwidth\u00a0memory. Currently VGL supports two classes of architectures: NEC SX-Aurora TSUBASA vector\u00a0processors and NVIDIA GPUs. However, VGL can be easily extended to other architectures due\u00a0to its flexible software structure. VGL is designed to provide users with the possibility of selecting\u00a0the most suitable architecture for solving a specific graph problem on a given input data, which, in\u00a0return, allows to significantly outperform existing frameworks and libraries, developed for modern\u00a0multicore CPUs and NVIDIA GPUs. Since VGL uses an identical set of computational and data\u00a0abstractions for all architectures, its users can easily port graph algorithms between different target\u00a0architectures without any source code modifications. Additionally, in this paper we show how\u00a0graph algorithms should be implemented and optimised for NVIDIA GPU and NEC SX-Aurora\u00a0TSUBASA architectures, demonstrating that both architectures have multiple similar properties\u00a0and hardware features.",
      "axisX": "-0.11368645278622477",
      "axisY": "0.6022053399758135",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200405",
      "title": "Update on Performance Analysis of Different Computational Architectures: Molecular Dynamics in Application to Protein-Protein Interactions",
      "authors": "Vladimir A. Fedorov, Ekaterina G. Kholina, Ilya B. Kovalenko, Nikita B. Gudimchuk, Philipp S. Orekhov, Artem A. Zhmurov",
      "keywords": "Update on Performance Analysis of Different Computational Architectures: Molecular Dynamics in Application to Protein-Protein Interactionsmolecular dynamics, coarse grain, tubulin, microtubule, Ndc80Molecular dynamics has proved itself as a powerful computer simulation method to study\u00a0dynamics, conformational changes, and interactions of biological macromolecules and their complexes.\u00a0In order to achieve the best performance and efficiency, it is crucial to benchmark various\u00a0hardware platforms for the simulations of realistic biomolecular systems with different size and\u00a0timescale. Here, we compare performance and scalability of a number of commercially available\u00a0computing architectures using all-atom and coarse-grained molecular dynamics simulations of water\u00a0and the Ndc80-microtubule protein complex in the GROMACS-2019.4 package. We report\u00a0typical single-node performance of various combinations of modern CPUs and GPUs, as well as\u00a0multiple-node performance of the \u201cLomonosov-2\u201d supercomputer. These data can be used as the\u00a0practical guidelines for choosing optimal hardware for molecular dynamics simulations.",
      "axisX": "0.3145987131530448",
      "axisY": "0.35603377854483764",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi200406",
      "title": "Computer Design of Structure of Molecules of High-Energy Tetrazines. Calculation of Thermochemical Properties",
      "authors": "Vadim M. Volokhov, Elena S. Amosova, Alexander V. Volokhov, Tatiana S. Zyubina, David B. Lempert, Leonid S. Yanovskiy, Ilya D. Fateev",
      "keywords": "Computer Design of Structure of Molecules of High-Energy Tetrazines. Calculation of Thermochemical Propertieshigh-performance computing, enthalpy of formation, quantum-chemical calculations, high-enthalpy compounds, IR spectra of gaseous molecules, combined CBS-4M method, combined G4 methodThe article presents high-performance calculations, using quantum chemical ab initio methods, of thermochemical characteristics of high-energy compounds: C2N6O4, C2N6O5, C2N6O6, C2H2N6O4, C3HN7O6, C3HN7O4F2, C4N10O12, C3HN6O4F, C4N10O8F4, C4N8O8F2. The IR absorption spectra, structural parameters and atomic displacements for the most intense vibrations, as well as the enthalpies of formation are provided in the article. The calculations were performed at the B3LYP/6-311+G(2d,p) level and using the combined methods CBS-4M and G4 within the Gaussian 09 application package (Linda paralellization). It is shown that the enthalpy of formation depends on the molecule structure.",
      "axisX": "0.7804418236083315",
      "axisY": "-0.03717431486626402",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210102",
      "title": "Forecastability Measures that Describe the Complexity of a Site for Deep Learning Wind Predictions",
      "authors": "Jaume Manero, Javier B\u00e9jar",
      "keywords": "Forecastability Measures that Describe the Complexity of a Site for Deep Learning Wind Predictionswind forecasting, time series, wind time series, deep learning, CNN, convolutional\nnetworks, forecastabilityThe application of deep learning to wind time series for multi-step prediction obtains good\u00a0results at short horizons. The accuracy of a wind forecast is highly dependent on the specific\u00a0structure of wind in the specific location, as many local features influence wind behaviour. The\u00a0characterization of the complexity of a site for wind prediction is defined as forecastability or\u00a0predictability and can be obtained from the inner structure of the meteorological time series\u00a0observations from a site. We analyze the time series structure searching for properties that have\u00a0a high correlation with the prediction result, properties that can create measures that have the\u00a0potential to describe the forecastability of a site. The best measures will show a high correlation\u00a0with the accuracy of the predictions. In this work, we analyze wind time series from 126,692 wind\u00a0locations in the US, where we apply several deep learning methods first, and then we verify several\u00a0forecastability descriptors with the accuracy deep learning results. We require High-Performance\u00a0Computing (HPC) resources for this task as the deep learning algorithms have sensible resource\u00a0requirements and are applied to a large set of data. The measures defined and explored in this work\u00a0are based on several techniques that decompose or transform the wind time-series. By combining\u00a0several of these measures, we can obtain better predictors of the site complexity, which will allow\u00a0us to evaluate the future error of a prediction on this site. Forecastability measures can contribute\u00a0to a wind site multi-dimensional description, becoming a valuable tool for wind resource analysts\u00a0and wind forecasters.",
      "axisX": "-0.2768128965682333",
      "axisY": "-0.29770266380943944",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210103",
      "title": "Size & Shape Matters: The Need of HPC Benchmarks of High Resolution Image Training for Deep Learning",
      "authors": "Ferran Par\u00e9s Pont, Pedro Megias, Dario Garcia-Gasulla, Marta Garcia-Gasulla, Eduard Ayguad\u00e9, Jes\u00fas Labarta",
      "keywords": "Size & Shape Matters: The Need of HPC Benchmarks of High Resolution Image Training for Deep Learningdeep learning, convolutional neural networks, high-resolution images, variableshape images, HPC benchmarksOne of the purposes of HPC benchmarks is to identify limitations and bottlenecks in hardware.\u00a0This functionality is particularly influential when assessing performance on emerging tasks,\u00a0the nature and requirements of which may not yet be fully understood. In this setting, a proper\u00a0benchmark can steer the design of next generation hardware by properly identifying said requirements,\u00a0and quicken the deployment of novel solutions. With the increasing popularity of deep\u00a0learning workloads, benchmarks for this family of tasks have been gaining popularity. Particularly\u00a0for image based tasks, which rely on the most well established family of deep learning models:\u00a0Convolutional Neural Networks. Significantly, most benchmarks for CNN use low-resolution and\u00a0fixed-shape (LR&FS) images. While this sort of inputs have been very successful for certain purposes,\u00a0they are insufficient for some domains of special interest (e.g., medical image diagnosis or\u00a0autonomous driving) where one requires higher resolutions and variable-shape (HR&VS) images to\u00a0avoid loss of information and deformation. As of today, it is still unclear how does image resolution\u00a0and shape variability affect the nature of the problem from a computational perspective. In this\u00a0paper we assess the differences between training with LR&FS and HR&VS, as means to justify\u00a0the importance of building benchmarks specific for the latter. Our results on three different HPC\u00a0clusters show significant variations in time, resources and memory management, highlighting the\u00a0differences between LR&FS and HR&VS image deep learning.",
      "axisX": "-0.5821673680371454",
      "axisY": "-0.4861279842008435",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210104",
      "title": "Computational Resource Consumption in Convolutional Neural Network Training \u2013 A Focus on Memory",
      "authors": "Luis A. Torres, Carlos J. Barrios, Yves Denneulin",
      "keywords": "Computational Resource Consumption in Convolutional Neural Network Training \u2013 A Focus on MemoryHigh-Performance Computing, deep learning, profiling, performance characterization, memory consumptionDeep neural networks (DNNs) have grown in popularity in recent years thanks to the increase\u00a0in computing power and the size and relevance of data sets. This has made it possible to build\u00a0more complex models and include more areas of research and application. At the same time, the\u00a0amount of data generated during the training process of these models puts great pressure on\u00a0the capacity and bandwidth of the memory subsystem and, as a direct consequence, has become\u00a0one of the biggest bottlenecks for the scalability of neural networks. Therefore, the optimizing of\u00a0the workloads produced by DNNs in the memory subsystem requires a detailed understanding of\u00a0access to the memory and the interactions between the processor, accelerator devices, and the\u00a0system memory hierarchy. However, contrary to what would be expected, most DNN profilers\u00a0work at a high level, so they only perform an analysis of the model and individual layers of the\u00a0network leaving aside the complex interactions between all the hardware components involved\u00a0in the training. This article shows the characterization performed using a convolutional neural\u00a0network implemented in the two most popular frameworks: TensorFlow and Pytorch. Likewise,\u00a0the behavior of the component interactions is discussed by varying the batch size for two sets of\u00a0synthetic data and showing the results obtained by the profiler created for the study. Moreover,\u00a0the results obtained when evaluating the AlexNet version on TensorFlow and its similarity in\u00a0behavior when using a basic CNN are included.",
      "axisX": "-0.5716487058623032",
      "axisY": "-0.25258279088591845",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210105",
      "title": "The MareNostrum Experimental Exascale Platform (MEEP)",
      "authors": "Alexander Fell, Daniel J. Mazure, Teresa C. Garcia, Borja Perez, Xavier Teruel, Pete Wilson, John D. Davis",
      "keywords": "The MareNostrum Experimental Exascale Platform (MEEP)high performance computing (HPC), accelerator, software stack, open source hardwareNascent Open Source Instruction Set Architectures such as OpenPOWER or RISC-V, allow software/hardware co-designers to fully utilize the underlying hardware, modify it or extend it based on their needs. In this paper, we introduce the vision of the MareNostrum Experimental Exascale Platform (MEEP), an Open Source platform enabling software and hardware stack experimentation\u00a0 targeting the High-Performance Computing (HPC) ecosystem. MEEP is built with state-of-the-art FPGAs that support PCIe and High Bandwidth Memory (HBM), making it ideal to emulate chiplet-based HPC accelerators such as ACME, at the chip, package, and/or system level. MEEP provides an FPGA Shell containing standardized interfaces (I/O and memory), enabling an emulated accelerator to communicate with the hardware of the FPGA and ensures quick integration. The first demonstration of MEEP is mapping a new accelerator, the Accelerated Compute and Memory Engine (ACME), on to this digital laboratory. This enables exploration of this novel disaggregated architecture, which separates the computation from the memory operations, optimizing the accelerator for both dense (compute-bound) as well as sparse (memory-bandwidth bound) workloads. Dense workloads focus on the computational capabilities of the engine, while dedicated processors for memory accesses optimize non-unit stride and/or random memory accesses required by sparse workloads. MEEP is an open source digital laboratory that can provide a future environment for full-stack co-design and pre-silicon exploration.\u00a0 MEEP invites software developers and hardware engineers to build the application, compiler, libraries and the hardware to solve future challenges in the HPC, AI, ML, and DL domains.",
      "axisX": "-0.05881243039900786",
      "axisY": "0.8396216649887797",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210106",
      "title": "Micro-Workflows Data Stream Processing Model for Industrial Internet of Things",
      "authors": "Ameer B. A. Alaasam, Gleb I. Radchenko, Andrey N. Tchernykh",
      "keywords": "Micro-Workflows Data Stream Processing Model for Industrial Internet of Thingsstream processing, fog computing, cloud computing, scientific workflow, microworkflow, IoTThe fog computing paradigm has become prominent in stream processing for IoT systems\u00a0where cloud computing struggles from high latency challenges. It enables the deployment of computational\u00a0resources between the edge and cloud layers and helps to resolve constraints, primarily\u00a0due to the need to react in real-time to state changes, improve the locality of data storage, and\u00a0overcome external communication channels\u2019 limitations. There is an urgent need for tools and\u00a0platforms to model, implement, manage, and monitor complex fog computing workflows. Traditional\u00a0scientific workflow management systems (SWMSs) provide modularity and flexibility to\u00a0design, execute, and monitor complex computational workflows used in smart industry applications.\u00a0However, they are mainly focused on batch execution of jobs consisting of tightly coupled\u00a0tasks. Integrating data streams into SWMSs of IoT systems is challenging. We proposed a microworkflow\u00a0model to redesign the monolith architecture of workflow systems into a set of smaller\u00a0and independent workflows that support stream processing. Micro-workflow is an independent\u00a0data stream processing service that can be deployed on different layers of the fog computing\u00a0environment. To validate the feasibility and practicability of the micro-workflow refactoring, we\u00a0provide intensive experimental analysis evaluating the interval between sensor messages, the time\u00a0interval required to create a message, between sending sensor message and receiving the message\u00a0in SWMS, including data serialization, network latency, etc. We show that the proposed decoupling\u00a0support of the independence of implementation, execution, development, maintenance, and\u00a0cross-platform deployment, where each micro-workflow becomes a standalone computational unit,\u00a0is a suitable mechanism for IoT stream processing.",
      "axisX": "-0.5005918720010974",
      "axisY": "0.42619933604639704",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140201",
      "title": "Scalability prediction for fundamental performance factors",
      "authors": "Claudia Rosas, Judit Gim\u00e9nez, Jes\u00fas Labarta",
      "keywords": "Scalability prediction for fundamental performance factorsparallel efficiency, curve-fitting, exascale computing, analysis and predictionInferring the expected performance for parallel applications is getting harder than ever; applications need to be modeled for restricted or nonexistent systems and performance analysts are required to identify and extrapolate their behavior using only the available resources. Prediction models can be based on detailed knowledge of the application algorithms or on blindly trying to extrapolate measurements from existing architectures and codes. This paper describes the work done to define an intermediate methodology where the combination of (a) the essential knowledge about fundamental factors in parallel codes, and (b) detailed analysis of the application behavior at low core counts on current platforms, guides the modeling efforts to estimate behavior at very large core counts. Our methodology integrates the use of several components like instrumentation package, visualization tools, simulators, analytical models and very high level information from the application running on systems in production to build a performance model.",
      "axisX": "-0.4139428059386813",
      "axisY": "0.1222751584335814",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140202",
      "title": "Predicting the Energy and Power Consumption of Strong and Weak Scaling HPC Applications",
      "authors": "Hayk Shoukourian, Torsten Wilde, Axel Auweter, Arndt Bode",
      "keywords": "Predicting the Energy and Power Consumption of Strong and Weak Scaling HPC Applicationsadaptive prediction, energy consumption, power consumption, energy capping,\npower capping, AEPCP model, energy measurement, node scaling, EtS prediction, HPCKeeping energy costs in budget and operating within available capacities of power distribution and cooling systems is becoming an important requirement for High Performance Computing (HPC) data centers. It is even more important when considering the estimated power requirements for Exascale computing. Power and energy capping are two of emerging techniques aimed towards controlling and efficient budgeting of power and energy consumption within the data center. Implementation of both techniques requires a knowledge of, potentially unknown, power and energy consumption data of the given parallel HPC applications for different numbers of compute servers (nodes).This paper introduces an Adaptive Energy and Power Consumption Prediction (AEPCP) model capable of predicting the power and energy consumption of parallel HPC applications for different number of compute nodes. The suggested model is application specific and describes the behavior of power and energy with respect to the number of utilized compute nodes, taking as an input the available history power/energy data of an application. It provides a generic solution that can be used for each application but it produces an application specific result. The AEPCP model allows for ahead of time power and energy consumption prediction and adapts with each additional execution of the application improving the associated prediction accuracy. The model does not require any application code instrumentation and does not introduce any application performance degradation. Thus it is a high level application energy and power consumption prediction model. The validity and the applicability of the suggested AEPCP model is shown in this paper through the empirical results achieved using two application-benchmarks on the SuperMUC HPC system (the 10th fastest supercomputer in the world, according to Top500 November 2013 rankings) deployed at Leibniz Supercomputing Centre.",
      "axisX": "-0.48850604974452266",
      "axisY": "-0.2418256387427735",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140205",
      "title": "Data Compression for the Exascale Computing Era - Survey",
      "authors": "Seung Woo Son, Zhengzhang Chen, William Hendrix, Ankit Agrawal, Wei-keng Liao, Alok Choudhary",
      "keywords": "Data Compression for the Exascale Computing Era - SurveyFault tolerance, checkpoint/restart, lossless/lossy compression, error bound, data\nclusteringWhile periodic checkpointing has been an important mechanism for tolerating faults in high-performance computing (HPC) systems, it is cost-prohibitive as the HPC system approaches exascale. Applying compression techniques is one common way to mitigate such burdens by reducing the data size, but they are often found to be less effective for scientific datasets. Traditional lossless compression techniques that look for repeated patterns are ineffective for scientific data in which high-precision data is used and hence common patterns are rare to find. In this paper, we present a comparison of several lossless and lossy data compression algorithms and discuss their methodology under the exascale environment. As data volume increases, we discover an increasing trend of new domain-driven algorithms that exploit the inherent characteristics exhibited in many scientific dataset, such as relatively small changes in data values from one simulation iteration to the next or among neighboring data. In particular, significant data reduction has been observed in lossy compression. This paper also discusses how the errors introduced by lossy compressions are controlled and the tradeoffs with the compression ratio.",
      "axisX": "0.07915427442510778",
      "axisY": "-0.5147212344778526",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140206",
      "title": "Extreme Big Data (EBD): Next Generation Big Data Infrastructure Technologies Towards Yottabyte/Year",
      "authors": "Satoshi Matsuoka, Hitoshi Sato, Osamu Tatebe, Michihiro Koibuchi, Ikki Fujiwara, Shuji Suzuki, Masanori Kakuta, Takashi Ishida, Yutaka Akiyama, Toyotaro Suzumura, Koji Ueno, Hiroki Kanezashi, Takemasa Miyoshi",
      "keywords": "Extreme Big Data (EBD): Next Generation Big Data Infrastructure Technologies Towards Yottabyte/YearBig Data, Supercomputing, Extreme Computing and Big Data Convergence, Data\nIntensive Computing, Non-Volatile MemoryOur claim is that so-called ``Big Data'' will evolve into a new era with proliferation of data from multiple sources such as massive numbers of sensors whose resolution is increasing exponentially, high-resolution simulations generating huge data results, as well as evolution of social infrastructures that allow for ``opening up of data silos'', i.e., data sources being abundant across the world instead of being confined within an institution, much as how scientific data are being handled in the modern era as a common asset openly accessible within and across disciplines. Such a situation would create the need for not only petabytes to zetabytes of capacity and beyond, but also for extreme scale computing power. Our new project, sponsored under the Japanese JST-CREST program is called ``Extreme Big Data\", and aims to achieve the {\\it convergence of extreme supercomputing and big data} in order to cope with such explosion of data. The project consists of six teams, three of which deals with defining future EBD convergent SW/HW architecture and system, and the other three the EBD co-design applications that represent different facets of big data, in metagenomics, social simulation, and climate simulation with real-time data assimilation. Although the project is still early in its lifetime, started in Oct. 2013, we have already achieved several notable results, including becoming world #1 on the Green Graph 500, a benchmark to measure the power efficiency of graph processing that appear in typical big data scenarios.",
      "axisX": "-0.5951886928848154",
      "axisY": "-0.312362218077519",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140207",
      "title": "Scalable parallel performance measurement and analysis tools - state-of-the-art and future challenges",
      "authors": "Bernd Mohr",
      "keywords": "Scalable parallel performance measurement and analysis tools - state-of-the-art and future challengesparallel programming, performance tools, extreme scale computingCurrent large-scale HPC systems consist of complex configurations with a huge number of potentially heterogeneous components. As the systems get larger, their behavior becomes more and more dynamic and unpredictable because of hard- and software re-configurations due to faultrecovery and power usage optimizations. Deep software hierarchies of large, complex system software and middleware components are required to operate such systems. Therefore, porting, adapting and tuning applications to today's complex systems is a complicated and time-consumingtask. Sophisticated integrated performance measurement, analysis, and optimization capabilities are required to efficiently utilize such systems. This article will summarize the state-of-the-art of scalable and portable parallel performance tools and the challenges these tools are facing on future extreme-scale and big data systems.",
      "axisX": "-0.061171389662421985",
      "axisY": "0.5155779208547702",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210201",
      "title": "Accelerating Seismic Redatuming Using Tile Low-Rank Approximations on NEC SX-Aurora TSUBASA",
      "authors": "Yuxi Hong, Hatem Ltaief, Matteo Ravasi, Laurent Gatineau, David Keyes",
      "keywords": "Accelerating Seismic Redatuming Using Tile Low-Rank Approximations on NEC SX-Aurora TSUBASAseismic redatuming, tile low-rank approximations, matrix-vector multiplication,\nload balancing, high bandwidth memory, NEC SX-Aurora TSUBASAWith the aim of imaging subsurface discontinuities, seismic data recorded at the surface of the Earth must be numerically re-positioned inside the subsurface where reflections have originated, a process referred to as redatuming. The recently developed Marchenko method is able to handle full-wavefield data including multiple arrivals. A downside of this approach is that a multi-dimensional convolution operator must be repeatedly evaluated to solve an expensive inverse problem. As such an operator applies multiple dense matrix-vector multiplications (MVM), we identify and leverage the data sparsity structure for each frequency matrix and propose to accelerate the MVM step using tile low-rank (TLR) matrix approximations. We study the TLR impact on time-to-solution for the MVM using different accuracy thresholds whilst at the same time assessing the quality of the resulting subsurface seismic wavefields and show that TLR leads to a minimal degradation in terms of signal-to-noise ratio on a 3D synthetic dataset. We mitigate the load imbalance overhead and provide performance evaluation on two distributed-memory systems. Our MPI+OpenMP TLR-MVM implementation reaches up to 3X performance speedup against the dense MVM counterpart from NEC scientific library on 128 NEC SX-Aurora TSUBASA cards. Thanks to the second generation of high bandwidth memory technology, it further attains up to 67X performance speedup compared to the dense MVM from Intel MKL when running on 128 dual-socket 20-core Intel Cascade Lake nodes with DDR4 memory. This corresponds to 110 TB/s of aggregated sustained bandwidth for our TLR-MVM implementation, without suffering deterioration in the quality of the reconstructed seismic wavefields.",
      "axisX": "0.2875204704973137",
      "axisY": "-0.3393179236278188",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210202",
      "title": "Porting and Optimizing Molecular Docking onto the SX-Aurora TSUBASA Vector Computer",
      "authors": "Leonardo Solis-Vasquez, Erich Focht, Andreas Koch",
      "keywords": "Porting and Optimizing Molecular Docking onto the SX-Aurora TSUBASA Vector Computerapplication porting, performance optimization, molecular docking, AutoDock, vector computing, SX-AuroraIn computer-aided drug design, the rapid identification of drugs is critical for combating\u00a0diseases. A key method in this field is molecular docking, which aims to predict the interactions\u00a0between two molecules. Molecular docking involves long simulations running compute-intensive\u00a0algorithms, and thus, can profit a lot from hardware-based acceleration. In this work, we investigate\u00a0the performance efficiency of the SX-Aurora TSUBASA vector computer for such simulations.\u00a0Specifically, we present our methodology for porting and optimizing AutoDock, a widely-used\u00a0molecular docking program. Using a number of platform-specific code optimizations, we achieved\u00a0executions on the SX-Aurora TSUBASA that are in average 3.6\u00d7 faster than on modern 128-core CPU servers, and up to a certain extent, competitive to V100 and A100 GPUs. To the best of our\u00a0knowledge, this is the first molecular docking implementation for the SX-Aurora TSUBASA.",
      "axisX": "0.06053721792486439",
      "axisY": "0.4165284766119709",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210203",
      "title": "First Experience of Accelerating a Field-Induced Chiral Transition Simulation Using the SX-Aurora TSUBASA",
      "authors": "Shinji Yoshida, Arata Endo, Hirono Kaneyasu, Susumu Date",
      "keywords": "First Experience of Accelerating a Field-Induced Chiral Transition Simulation Using the SX-Aurora TSUBASASX-Aurora TSUBASA, OS Offload, VH Call, VEO, vectorization ratioAn analysis method based on the Ginzburg-Landau equation for the superconductivity is\u00a0applied to the field-induced chiral transition simulation (FICT). However, the FICT is time consuming\u00a0because it takes approximately 10 hours on a single SX-ACE vector processor. Moreover,\u00a0the FICT must be repeatedly performed with parameters changed to understand the mechanism\u00a0of the phenomenon. The newly emerged SX-Aurora TSUBASA, the successor of the SX-ACE processor,\u00a0is expected to provide much higher performance to the programs executed on the SX-ACE\u00a0as is. However, the SX-Aurora TSUBASA processor has changed its architecture of compute nodes\u00a0and gives users three different execution models, which leads to users\u2019 concerns and questions in\u00a0terms of how three execution models should be selectively used. In this paper, we report the first\u00a0experience of using the SX-Aurora TSUBASA processor for the FICT. Specifically, we have developed\u00a0three implementations of the FICT corresponding to the three execution models suggested\u00a0by the SX-Aurora TSUBASA. For acceleration of the FICT, improvement of the vectorization\u00a0ratio in the program execution and the efficient transfer of data to the general purpose processor\u00a0as the vector host from the vector processor as the vector engine is explored. The evaluation in\u00a0this paper shows how acceleration of the FICT is achieved as well as how much effort of users is\u00a0required.",
      "axisX": "-0.19848882599326317",
      "axisY": "-0.4214700846263188",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210204",
      "title": "Evaluating the Performance of OpenMP Offloading on the NEC SX-Aurora TSUBASA Vector Engine",
      "authors": "Tim Cramer, Boris Kosmynin, Simon Moll, Manoel R\u00f6mmer, Erich Focht, Matthias S. M\u00fcller",
      "keywords": "Evaluating the Performance of OpenMP Offloading on the NEC SX-Aurora TSUBASA Vector EngineHPC, OpenMP, offloading, reverse offloading, vector computing, performanceThe NEC SX-Aurora TSUBASA vector engine (VE) follows the tradition of long vector processors for high-performance computing (HPC). The technology combines the vector computing capabilities with the popularity of standard x86 architecture by integrating it as an accelerator. To decrease the burden of code porting for different accelerator types, the OpenMP specification is designed to be single parallel programming model for all of them. Besides the availability of compiler and runtime implementations, the functionality as well as the performance is important for the usability and acceptance of this paradigm. In this work, we present LLVM-based solutions for OpenMP target device offloading from the host to the vector engine and vice versa (reverse offloading). Therefore, we use our source-to-source transformation tool sotoc as well as the native LLVM-VE code path. We assess the functionality and present the first performance numbers of real-world HPC kernels. We discuss the advantages and disadvantage of the different approaches and show that our implementation is competitive to other GPU OpenMP runtime implementations. Our work gives scientific programmers new opportunities and flexibilities for the development of scalable OpenMP offloading applications for SX-Aurora TSUBASA.",
      "axisX": "-0.033524462893652386",
      "axisY": "0.7517077467147255",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210205",
      "title": "Performance and Power Analysis of a Vector Computing System",
      "authors": "Kazuhiko Komatsu, Akito Onodera, Erich Focht, Soya Fujimoto, Yoko Isobe, Shintaro Momose, Masayuki Sato, Hiroaki Kobayashi",
      "keywords": "Performance and Power Analysis of a Vector Computing SystemSX-Aurora TSUBASA, optimization, vector computing, power efficiency, Himeno\nbenchmark, HPCGThe performance of recent computing systems has drastically improved due to the increase in the number of cores. However, this approach is reaching the limitation due to the power constraints of facilities. Instead, this paper focuses on a vector processing with long vector length that has a potential to realize high performance and high power efficiency. This paper discusses the potential through the optimization of two benchmarks, the Himeno and HPCG benchmarks, for the latest vector computing system SX-Aurora TSUBASA. The architecture of SX-Aurora TSUBASA owes the high efficiency to making good of its long vector length. Considering these characteristics, various levels of optimizations required for a large-scale vector computing system are examined such as vectorization, loop unrolling, use of cache, domain decomposition, process mapping, and problem size tuning. The evaluation and analysis suggest that the optimizations improve the sustained performance, power efficiency, and scalability of both benchmarks. Therefore, it is clarified that the SX-Aurora TSUBASA architecture can achieve higher power efficiency due to its high sustained memory bandwidth paired with the long vector computing.",
      "axisX": "-0.14013928845195217",
      "axisY": "-0.3417070679278277",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210206",
      "title": "Distributed Graph Algorithms for Multiple Vector Engines of NEC SX-Aurora TSUBASA Systems",
      "authors": "Ilya V. Afanasyev, Vadim V. Voevodin, Kazuhiko Komatsu, Hiroaki Kobayashi",
      "keywords": "Distributed Graph Algorithms for Multiple Vector Engines of NEC SX-Aurora TSUBASA Systemsvector computers, graph algorithms, graph framework, VGL, optimisationThis paper describes the world-first attempt to develop distributed graph algorithm implementations, aimed for modern NEC SX-Aurora TSUBASA vector systems. Such systems are equipped with up to eight powerful vector engines, which are capable to significantly accelerate graph processsing and simultaneously increase the scale of processed input graphs. This paper describes distributed implementations of three widely-used graph algorithms: Page Rank (PR), Bellman-Ford Single Source Shortest Paths (further referred as SSSP) and Hyperlink-Induced Topic Search (HITS), evaluating their performance and scalability on Aurora 8 system. In this paper we describe graph partitioning strategies, communication strategies, programming models and single-VE optimizations used in these implementations. The developed implementations achieve 40, 6.6 and 1.3 GTEPS performance on PR, SSSP and HITS algorithm on 8 vector engines, at the same time achieving up to 1.5x, 2x and 2.5x acceleration on 2, 4 and 8 vector engines of Aurora 8 systems. Finally, this paper describes an approach to incorporate distributed graph processing support into our previously developed Vector Graph Library (VGL) framework \u2013 a novel framework for graph analytics on NEC SX-Aurora TSUBASA architecture.",
      "axisX": "-0.23378486654793224",
      "axisY": "0.35587796846450065",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210207",
      "title": "Optimizing Load Balance in a Parallel CFD Code for a Large-scale Turbine Simulation on a Vector Supercomputer",
      "authors": "Osamu Watanabe, Kazuhiko Komatsu, Masayuki Sato, Hiroaki Kobayashi",
      "keywords": "Optimizing Load Balance in a Parallel CFD Code for a Large-scale Turbine Simulation on a Vector Supercomputerturbine simulation code, MPI, OpenMP, hybrid parallelization, vector supercomputer, load balanceA turbine for power generation is one of the essential infrastructures in our society. A turbine's failure causes severe social and economic impacts on our everyday life. Therefore, it is necessary to foresee such failures in advance. However, it is not easy to expect these failures from a real turbine. Hence, it is required to simulate various events occurring in the turbine by numerical simulations of the turbine. A multiphysics CFD code, \u2018\u2018Numerical Turbine,\u2019' has been developed on vector supercomputer systems for large-scale simulations of unsteady wet steam flows inside a turbine. To solve this problem, the Numerical Turbine code is a block structure code using MPI parallelization, and the calculation space consists of grid blocks of different sizes. Therefore, load imbalance occurs when executing the code in MPI parallelization. This paper creates an estimation model that finds the calculation time from each grid block's calculation amount and calculation performance. It proposes an OpenMP parallelization method for the load balance of MPI applications. This proposed method reduces the load imbalance by considering the vector performance according to the calculation amount based on the model. Moreover, this proposed method recognizes the need to reduce the load imbalance without pre-execution. The performance evaluation shows that the proposed method improves the load balance from 24.4 % to 9.3 %.",
      "axisX": "-0.2098047480756567",
      "axisY": "-0.4079301822443365",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210301",
      "title": "Evaluating Performance of Mixed Precision Linear Solvers with Iterative Refinement",
      "authors": "Boris I. Krasnopolsky, Alexey V. Medvedev",
      "keywords": "Evaluating Performance of Mixed Precision Linear Solvers with Iterative Refinementsystems of linear algebraic equations, elliptic equations, algebraic multigrid methods, iterative refinement, mixed precision calculationsThe solution of systems of linear algebraic equations is among the time-consuming problems\u00a0when performing the numerical simulations. One of the possible ways of improving the corresponding\u00a0solver performance is the use of reduced precision calculations, which, however, may\u00a0affect the accuracy of the obtained solution. The current paper analyzes the potential of using\u00a0the mixed precision iterative refinement procedure to solve the systems of equations occurring as\u00a0a result of the discretization of elliptic differential equations. The paper compares several inner\u00a0solver stopping criteria and proposes the one allowing to eliminate the residual deviation and\u00a0minimize the number of extra iterations. The presented numerical calculation results demonstrate\u00a0the efficiency of the adopted algorithm and show about the decrease in the solution time by a\u00a0factor of 1.5 for the turbulent flow simulations when using the iterative refinement procedure to\u00a0solve the corresponding pressure Poisson equation.",
      "axisX": "0.3631527267718749",
      "axisY": "-0.5318858495222671",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210302",
      "title": "Fog Computing State of the Art: Concept and Classification of Platforms to Support Distributed Computing Systems",
      "authors": "Alexandra A. Kirsanova, Gleb I. Radchenko, Andrey N. Tchernykh",
      "keywords": "Fog Computing State of the Art: Concept and Classification of Platforms to Support Distributed Computing Systemsbig data processing, fog computing, scheduling, cloud computing, edge computing,\nInternet of ThingsAs the Internet of Things (IoT) becomes a part of our daily life, there is a rapid growth in\u00a0the connected devices. A well-established approach based on cloud computing technologies cannot\u00a0provide the necessary quality of service in such an environment, particularly in terms of reducing\u00a0data latency. Today, fog computing technology is seen as a novel approach for processing large\u00a0amounts of critical and time-sensitive data. This article reviews cloud computing technology and\u00a0analyzes the prerequisites for the evolution of this approach and the emergence of the concept\u00a0of fog computing. As part of an overview of the critical features of fog computing, we analyze\u00a0the frequent confusion of the concepts of fog and edge computing. We provide an overview of\u00a0fog computing technologies: virtualization, containerization, orchestration, scalability, parallel\u00a0computing environments, as well as systematic analysis of the most popular platforms that support\u00a0fog computing. As a result of the analysis, we offer two approaches to classification of the fog\u00a0computing platforms: by the principle of openness/closure of components and by the three-level\u00a0classification based on the provided platform functionality (Deploy-, Platform- and Ecosystem as\u00a0a Service).",
      "axisX": "-0.4433399339051472",
      "axisY": "0.08029118873146962",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210303",
      "title": "VaLiPro: Linear Programming Validator for Cluster Computing Systems",
      "authors": "Leonid B. Sokolinsky, Irina M. Sokolinskaya",
      "keywords": "VaLiPro: Linear Programming Validator for Cluster Computing Systemslinear programming, solution validator, VaLiPro, parallel algorithm, cluster computing system, BSF-skeletonThe article presents and evaluates a scalable algorithm for validating solutions to linear programming problems on cluster computing systems. The main idea of the method is to generate a regular set of points (validation set) on a small-radius hypersphere centered at the solution point submitted to validation. The objective function is computed at each point of the validation that belongs to the feasible region. If all the values are less than or equal to the value of the objective function at the point that is to be validated, then this point is the correct solution. The parallel implementation of the VaLiPro algorithm is written in C++ through the parallel BSF-skeleton, which encapsulates all aspects related to the MPI-based parallelization of the program. We provide the results of large-scale computational experiments on a cluster computing system to study the scalability of the VaLiPro algorithm.",
      "axisX": "0.2808736017105201",
      "axisY": "0.20461462250053866",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210304",
      "title": "A Review of Supercomputer Performance Monitoring Systems",
      "authors": "Konstantin S. Stefanov, Sucheta Pawar, Ashish Ranjan, Sanjay Wandhekar, Vladimir V. Voevodin",
      "keywords": "A Review of Supercomputer Performance Monitoring Systemsmonitoring, supercomputers, performance monitoring, reviewHigh Performance Computing is now one of the emerging fields in computer science and\u00a0its applications. Top HPC facilities, supercomputers, offer great opportunities in modeling diverse\u00a0processes thus allowing to create more and greater products without full-scale experiments.\u00a0Current supercomputers and applications for them are very complex and thus are hard to use\u00a0efficiently. Performance monitoring systems are the tools that help to understand the efficiency of\u00a0supercomputing applications and overall supercomputer functioning. These systems collect data\u00a0on what happens on a supercomputer (performance data, performance metrics) and present them\u00a0in a way allowing to make conclusions about performance issues in programs running on the supercomputer.\u00a0In this paper we give an overview of existing performance monitoring systems designed\u00a0for or used on supercomputers. We give a comparison of performance monitoring systems found\u00a0in literature, describe problems emerging in monitoring large scale HPC systems, and outline our\u00a0vision on future direction of HPC monitoring systems development.",
      "axisX": "-0.6886413072658429",
      "axisY": "-0.10192027330385038",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210305",
      "title": "Administration, Monitoring and Analysis of Supercomputers in Russia: a Survey of 10 HPC Centers",
      "authors": "Vadim V. Voevodin, Roman A. Chulkevich, Pavel S. Kostenetskiy, Vyacheslav I. Kozyrev, Anton K. Maliutin, Dmitry A. Nikitenko, Sergey G. Rykovanov, Artemiy B. Shamsutdinov, Yurii N. Shkandybin, Sergey A. Zhumatiy",
      "keywords": "Administration, Monitoring and Analysis of Supercomputers in Russia: a Survey of 10 HPC Centerssupercomputer, high-performance computing, administration, survey, monitoring,\nperformanceSupercomputer technologies are in demand for solving many important and computationallyintensive\u00a0tasks in various fields of science and technology. Therefore, it is not surprising that\u00a0there are several dozen supercomputer centers only in Russia. However, the goals of creating such\u00a0centers, as well as the range of tasks solved in them, can vary greatly, therefore the structure\u00a0of supercomputers and the policies for their usage can significantly differ. This leads to the fact\u00a0that many supercomputer centers live an isolated life \u2013 the administrators of such centers tend to\u00a0solve administration-related tasks on their own, despite the fact that solutions for many similar\u00a0tasks have already been developed and applied in other centers. This can happen due to different\u00a0reasons, but in any case, this situation could and should be improved. To do this, it is worth\u00a0establishing a closer connection between supercomputer centers, which will allow more actively\u00a0exchanging experience or jointly developing desired system software. In order to understand the\u00a0current situation in this area, a survey was conducted of representatives among 10 large supercomputer\u00a0centers in Russia, and its results are presented in this paper. Two relevant topics about\u00a0using monitoring data in practice and real-life examples of supercomputer functioning improvement\u00a0are also discussed here in more detail. Their vision on these topics is provided by the system\u00a0administrators of HSE University, Skoltech and Moscow State University.",
      "axisX": "-0.6479201845680815",
      "axisY": "-0.38719140064002144",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210306",
      "title": "Efficient Implementation of Liquid Crystal Simulation Software on Modern HPC Platforms",
      "authors": "Ilya V. Afanasyev, Dmitry I. Lichmanov, Vladimir Yu. Rudyak, Vadim V. Voevodin",
      "keywords": "Efficient Implementation of Liquid Crystal Simulation Software on Modern HPC PlatformsNVIDIA GPU, NEC SX-Aurora TSUBASA, liquid crystals, HPC, co-design, performance optimization, Monte Carlo, cubic latticeIn this paper we demonstrate the process of efficient porting a software package for Markov\u00a0chain Monte Carlo (MCMC) simulations on a finite cubic lattice on multiple modern architectures:\u00a0Pascal, Volta and Turing NVIDIA GPUs, NEC SX-Aurora TSUBASA vector engines and Intel\u00a0Xeon Gold processors. In the studied software, MCMC methodology is used for simulations of\u00a0liquid crystal structures, but it can be as well employed in a wide range of problems of mathematical\u00a0physics and numerical methods. The main goals of this work are to determine the best software\u00a0optimization strategy for this class of algorithms and to examine the speed and the efficiency of\u00a0such simulations on modern HPC platforms. We evaluate the effects of various optimizations, such\u00a0as using more suitable memory access patterns, multitasking for efficient utilization of massive\u00a0parallelism on the target architectures, improved cache hit-rates, parallel workload balancing,\u00a0etc. We perform a detailed performance analysis for each target platform using software tools\u00a0such as nvprof, Ftrace and VTune. On this basis, we evaluate and compare the efficiency of the\u00a0developed computational kernels on different platforms and subsequently rank these platforms\u00a0by their performance. The results show that NVIDIA GPU and NEC SX-Aurora TSUBASA\u00a0platforms, although at first glance seem very different, require similar optimization approaches in\u00a0many cases due to similarities in data processing principles.",
      "axisX": "0.29355668660174145",
      "axisY": "0.5228141532940772",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210401",
      "title": "Technology for Supercomputer Simulation of Turbulent Flows in the Good New Days of Exascale Computing",
      "authors": "Andrey V. Gorobets, Alexey P. Duben",
      "keywords": "Technology for Supercomputer Simulation of Turbulent Flows in the Good New Days of Exascale Computingcomputational fluid dynamics, turbulent flows, scale-resolving simulation, hybrid\nRANS-LES approach, CPU+GPU, MPI+OpenMP+OpenCLA technology for scale-resolving simulations of turbulent flows in the problems of aerodynamics\u00a0and aeroacoustics is presented. It is based on the higher accuracy numerical schemes on\u00a0unstructured mixed-element meshes and latest non-zonal hybrid approaches combining Reynoldsaveraged\u00a0Navier \u2013 Stokes (RANS) and Large eddy simulation (LES) methods for turbulence modeling.\u00a0It targets a wide range of high performance computing (HPC) systems, from a compute\u00a0server or small cluster to an exascale supercomputer. The advantages of the key components of\u00a0the technology are summarized. These key components are a hybrid RANS-LES turbulence modeling\u00a0method, a numerical scheme for discretization in space, a parallel algorithm, and a portable\u00a0software implementation for modern hybrid systems with extra massive parallelism. Examples of\u00a0our simulations are given and parallel performance on various HPC systems is presented.",
      "axisX": "0.26645309145516294",
      "axisY": "0.23294382572019737",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210402",
      "title": "Improving the Computational Efficiency of the Global SL-AV Numerical Weather Prediction Model",
      "authors": "Mikhail A. Tolstykh, Rostislav Yu. Fadeev, Vladimir V. Shashkin, Gordey S. Goyman",
      "keywords": "Improving the Computational Efficiency of the Global SL-AV Numerical Weather Prediction Modelnumerical weather prediction, global atmosphere model, computational efficiency,\nI/O optimizationThe recent works on improving the efficiency of the Russian SL-AV global numerical weather\u00a0prediction model both for medium- and long-range forecasts are described. The algorithmic improvements\u00a0of SL-AV dynamical core, implementation of parallel I/O and several code optimizations\u00a0are presented. We investigate the impact of single precision computations in some parts of\u00a0the code on present climate simulations. As a result of efforts described in this article, we are now\u00a0able to compute a 24-hour forecast for the model version having about 10 km horizontal resolution\u00a0and 104 vertical levels in 13 min using 2916 processor cores of Cray XC40 system. This timing\u00a0allows multiple experiments for tuning this new model and fits the requirements for operational\u00a0weather forecast. The single long-range forecast with low-resolution SL-AV version now takes just\u00a089 minutes instead of 111. We have also verified that the partial utilization of single precision\u00a0computations produces approximately the same model climate as the previous version with fully\u00a0double precision computations.",
      "axisX": "-0.07422007527056997",
      "axisY": "-0.14889996473995148",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210403",
      "title": "The Influence of Autumn Eurasian Snow Cover on the Atmospheric Dynamics Anomalies during the Next Winter in INMCM5 Model Data",
      "authors": "Maria A. Tarasevich, Evgeny M. Volodin",
      "keywords": "The Influence of Autumn Eurasian Snow Cover on the Atmospheric Dynamics Anomalies during the Next Winter in INMCM5 Model Dataclimate model, seasonal hindcasts, North Atlantic Oscillation, Eurasian snow\ncover, teleconnectionThe influence of autumn Eurasian snow cover on the atmospheric dynamics anomalies during\u00a0the following winter is studied based on the INM RAS climate model data. The North Atlantic\u00a0Oscillation is the leading pattern that causes the weather and climate variability in the Northern\u00a0hemisphere. We evaluate the up-to-date model version (INMCM5) ability of the autumn Eurasian\u00a0snow \u2013 winter NAO teleconnection simulation on different timescales. Maximum covariance analysis\u00a0(MCA) is used to find winter atmospheric signals that are significantly correlated with autumn\u00a0snow cover anomalies. Using MCA we conclude that Autumn Eurasian snow \u2013 winter NAO teleconnection\u00a0is present in INMCM5 experiments on pre-industrial and present-day climate simulation.\u00a0However, this method fails to show this phenomenon in experiments on a seasonal timescale. We\u00a0conduct additional experiments on a seasonal timescale to assess the sensitivity of North Atlantic\u00a0Oscillation index predictability to initial snow cover perturbations. These experiments demonstrate\u00a0the absence of direct autumn Eurasian snow impact on the NAO index.",
      "axisX": "0.015522978804550799",
      "axisY": "-0.567169189614073",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210404",
      "title": "Representation of Spatial Data Processing Pipelines Using Relational Database",
      "authors": "Igor G. Okladnikov",
      "keywords": "Representation of Spatial Data Processing Pipelines Using Relational Databasespatial data, information systems, databases, workflow, directed multigraph, processing pipeline, climate researchA methodology for representation of spatial data processing pipelines using relational database\u00a0within the framework of the computing backend of the online information-analytical system \u201cClimate\u201d\u00a0(http://climate.scert.ru) is proposed. Each pipeline is represented by a sequence of\u00a0instructions for the computing backend describing how to run data processing modules and pass\u00a0datasets between them (from the output of one module to the input of another one), including\u00a0raw data and final computational results obtained in graphical or binary formats. Using relational\u00a0database for storing descriptions of processing pipelines used in the \u201cClimate\u201d system provides\u00a0flexibility and efficiency while adding and developing spatial data processing modules. It also\u00a0provides computing pipelines scaling for further implementation for multiprocessor systems.",
      "axisX": "0.20905792245123928",
      "axisY": "0.6153798209440605",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210405",
      "title": "Direct Numerical Simulation of Stratified Turbulent Flows and Passive Tracer Transport on HPC Systems: Comparison of CPU Architectures",
      "authors": "Evgeny V. Mortikov, Andrey V. Debolskiy",
      "keywords": "Direct Numerical Simulation of Stratified Turbulent Flows and Passive Tracer Transport on HPC Systems: Comparison of CPU Architecturesturbulence, direct numerical simulation, ARM, supercomputingIn this paper we assess the influence of CPU architectures commonly used in HPC systems\u00a0on the efficiency of the implementation of algorithms used for direct numerical simulation (DNS)\u00a0of turbulent flows. We consider a stably stratified turbulent plane Couette flow as a benchmark\u00a0problem supplemented with the additional transport of passive substances. The comparison includes\u00a0the Intel Xeon, AMD Rome x86 CPU architecture processors and the Huawei Kunpeng\u00a0ARM CPU processor. We discuss the role of memory-oriented optimizations on the efficiency of\u00a0tracer transport implementation on each platform.",
      "axisX": "0.34732132203191224",
      "axisY": "0.6165745917975011",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210406",
      "title": "Scalability as a Key Property of Mapping Computational Tasks to Supercomputer Architecture",
      "authors": "Alexander S. Antonov",
      "keywords": "Scalability as a Key Property of Mapping Computational Tasks to Supercomputer Architecturescalability, supercomputer, AlgoWiki, parallel structure, problems, methods, algorithms, implementations, computing platformsWhen solving complex computational problems on modern supercomputers, an increasingly\u00a0important role is played by the scalability property, which characterizes the ability of applications\u00a0to adapt to various degrees of parallelism of computing systems.",
      "axisX": "-0.0562362032492429",
      "axisY": "0.30790421845509963",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210407",
      "title": "High-performance Shallow Water Model for Use on Massively Parallel and Heterogeneous Computing Systems",
      "authors": "Andrey V. Chaplygin, Anatoly V. Gusev, Nikolay A. Diansky",
      "keywords": "High-performance Shallow Water Model for Use on Massively Parallel and Heterogeneous Computing Systemsshallow water, supercomputer modeling, heterogeneous computing systems, MPI,\nOpenMP, CUDAThis paper presents the shallow water model, formulated from the ocean general circulation\u00a0sigma model INMOM (Institute of Numerical Mathematics Ocean Model). The shallow water\u00a0model is based on software architecture, which separates the physics-related code from parallel\u00a0implementation features, thereby simplifying the model\u2019s support and development. As an improvement\u00a0of the two-dimensional domain decomposition method, we present the blocked-based\u00a0decomposition proposing load-balanced and cache-friendly calculations on CPUs. We propose various\u00a0hybrid parallel programming patterns in the shallow water model for effective calculation on\u00a0massively parallel and heterogeneous computing systems and evaluate their scaling performances\u00a0on the Lomonosov-2 supercomputer. We demonstrate that performance per a single grid point on\u00a0GPUs dramatically decreases for small grid sizes starting from 219 points per node, while performance\u00a0on CPUs scales up to 217 well. Although, calculations on GPUs outperform calculations on\u00a0CPUs by a factor of 4.7 at 30 nodes using 60 GPUs and 360 CPU cores at 6100 x 4460 grid size.\u00a0We demonstrate that overlapping kernel execution with data transfers on GPUs increases performance\u00a0by 28%. Furthermore, we demonstrate the advantage of using the load-balancing method\u00a0in the Azov Sea model on CPUs and GPUs.",
      "axisX": "0.2177803126045211",
      "axisY": "0.24651453867092427",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210408",
      "title": "PLUMED Plugin Integration into High Performance Pmemd Program for Enhanced Molecular Dynamics Simulations",
      "authors": "Viktor V. Drobot, Evgeny M. Kirilin, Kirill E. Kopylov, Vytas K. \u0160vedas",
      "keywords": "PLUMED Plugin Integration into High Performance Pmemd Program for Enhanced Molecular Dynamics Simulationshigh performance pmemd program, enhanced molecular dynamics simulations,\nPLUMED plugin integration, metadynamics, CUDA, GPUMetadynamics as an enhanced sampling procedure of molecular dynamics simulations is an\u00a0effective tool to simulate complex molecular motions, conformations and reactivity, including enzyme\u00a0plasticity and catalysis. The classic non-enhanced molecular simulation tools have reached\u00a0unprecedently high performance utilizing GPU units, however their implementation for enhanced\u00a0sampling are still on demand. The widespread AMBER (molecular dynamics package) + PLUMED\u00a0(metadynamics plugin) still does not take advantage of GPU computing or the CPU utilization\u00a0optimization included in the AMBER pmemd program. In this work we have developed\u00a0PLUMED binding to pmemd program resolving performance issues within hybrid molecular dynamics/metadynamics runs. Preliminary checks and test results of the model system have validated\u00a0this implementation.",
      "axisX": "0.140640600041253",
      "axisY": "0.2380172306643793",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi210409",
      "title": "Turbulent Length Scale for Multilayer RANS Model of Urban Canopy and Its Evaluation Based on Large-Eddy Simulations",
      "authors": "Andrey V. Glazunov, Andrey V. Debolskiy, Evgeny V. Mortikov",
      "keywords": "Turbulent Length Scale for Multilayer RANS Model of Urban Canopy and Its Evaluation Based on Large-Eddy Simulationsatmospheric boundary layer, numerical simulation of turbulence, urban canopy,\nscalar turbulent transportLarge-Eddy Simulation (LES) numerical experiments of neutrally-stratified turbulent flow\u00a0over an urban-type surface and passive scalar transport by this flow are performed. A simple\u00a0parameterization of the turbulent length scale containing only one empirical constant is proposed.\u00a0Multilayer Reynolds-Averaged Navier-Stokes (RANS) model of turbulent flow and turbulent scalar\u00a0diffusion is constructed. The results of the RANS model are compared with the LES experiments.\u00a0It is shown that the proposed approach allows predicting the average flow velocity and the scalar\u00a0concentration inside and above the urban canopy.",
      "axisX": "0.6068380135775328",
      "axisY": "-0.4810490310046925",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220101",
      "title": "4D Technology of Variational Data Assimilation for Sea Dynamics Problems",
      "authors": "Victor P. Shutyaev, Valery I. Agoshkov, Vladimir B. Zalesny, Eugene I. Parmuzin, Natalia B. Zakharova",
      "keywords": "4D Technology of Variational Data Assimilation for Sea Dynamics Problemssea dynamics modeling, variational data assimilation, observations, sea surface\ntemperatureThe technology aimed at high-performance computing is presented for modeling the sea dynamics\u00a0problems based on 4D variational data assimilation technique developed at the Marchuk\u00a0Institute of Numerical Mathematics, Russian Academy of Sciences (INM RAS). The technology\u00a0is based on the multicomponent splitting method for the mathematical model of sea dynamics\u00a0and the minimization of cost functionals related to the observation data by solving an optimality\u00a0system that involves the adjoint equations with observation data and observation error covariances.\u00a0Efficient algorithms for solving the variational data assimilation problems are presented\u00a0based on modern iterative processes with a special choice of iterative parameters. The technology\u00a0is illustrated for the Baltic Sea dynamics model with variational data assimilation to restore the\u00a0initial states and the heat fluxes on the sea surface.",
      "axisX": "0.4924315679423736",
      "axisY": "0.05013733092133917",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220102",
      "title": "A Supercomputer-Based Modeling System for Short-Term Prediction of Urban Surface Air Quality",
      "authors": "Alexander V. Starchenko, Evgeniy A. Danilkin, Sergei A. Prokhanov, Lubov I. Kizhner, Elena A. Shelmina",
      "keywords": "A Supercomputer-Based Modeling System for Short-Term Prediction of Urban Surface Air Qualityparallel computations, numerical weather prediction, mesoscale models, urban air\nquality, MPIThis paper proposes a mathematical model and an effective supercomputer-based numerical\u00a0method for short-term prediction of extreme meteorological conditions and atmospheric air quality\u00a0over limited stretches of land encompassing large population centers. The mathematical model\u00a0includes a pollutant transport model with a reduced chemical mechanism and a non-hydrostatic\u00a0mesoscale meteorological model with a modern moisture microphysics parametrization scheme.\u00a0The numerical method relies on the use of the finite volume method and semi-implicit difference\u00a0schemes of the second order of approximation, which are solved using the TDMA method\u00a0with a linear dependence of the number of arithmetic operations on the size of the grid. This\u00a0property of the numerical method ensures high efficiency when parallelized: not less than 70%\u00a0when using up to 256 computing cores with a horizontal grid size of 0.5\u20131.0 km. Development\u00a0of parallel programs was carried out using the Message Passing Interface parallel programming\u00a0technology, two-dimensional decomposition of the grid area along horizontal (west to east and\u00a0south to north) directions, and introduction of additional fictitious grid nodes along the perimeter\u00a0of the decomposition subdomains.",
      "axisX": "0.3099401631110029",
      "axisY": "-0.06743542517649967",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220103",
      "title": "River Routing in the INM RAS-MSU Land Surface Model: Numerical Scheme and Parallel Implementation on Hybrid Supercomputers",
      "authors": "Victor M. Stepanenko",
      "keywords": "River Routing in the INM RAS-MSU Land Surface Model: Numerical Scheme and Parallel Implementation on Hybrid Supercomputersland surface model, soil, river network, MPI, OpenMPThe land surface model (LSM) is a necessary compartment of any numerical weather forecast\u00a0system or the Earth system model. This paper presents a new version of the INM RAS-MSU\u00a0land surface model where the river hydrodynamic and thermodynamic scheme is embedded into\u00a0the parallel execution framework using MPI and OpenMP. Numerical experiments have been\u00a0performed for the East European domain with resolution\u00a00.5\u00b0\u00d7 0.5\u00b0.\u00a0The soil model parallel\u00a0efficiency at 1\u2013144 MPI cores was 0.52\u20130.79 and limited by the presence of ocean area, and by\u00a0imbalance of computational load between soil columns. The acceleration of the river model at\u00a0MPI level was defined by the size of the largest river basin in the domain. At the OpenMP level,\u00a0the potential for acceleration of large river basin simulation is shown to be close to number of\u00a0threads used, based on fractal properties of the river networks. This acceleration was hindered in\u00a0our numerical experiments by the reduced river orders at the coarse land surface model resolution,\u00a0so that the optimal speedup for the Volga river basin was 2.5\u20133 times attained at 4\u20136 threads.\u00a0This performance is projected to improve with refinement of the LSM spatial resolution.",
      "axisX": "0.18180231934515656",
      "axisY": "-0.14900546545444315",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220104",
      "title": "Machine Learning Approaches to Extreme Weather Events Forecast in Urban Areas: Challenges and Initial Results",
      "authors": "Fabio Porto, Mariza Ferro, Eduardo Ogasawara, Thiago  Moeda, Claudio Daniel Tenorio de Barros, Anderson Chaves Silva, Rocio Zorrilla, Rafael  Silva Pereira, Rafaela  Nascimento Castro, Jo\u00e3o Victor Silva, Rebecca Salles, Augusto Jos\u00e9 Fonseca, Juliana Hermsdorff, Marcelo Magalh\u00e3es, Vitor S\u00e1, Ant\u00f4nio Adolfo Sim\u00f5es, Carlos Cardoso, Eduardo  Bezerra",
      "keywords": "Machine Learning Approaches to Extreme Weather Events Forecast in Urban Areas: Challenges and Initial Resultsmachine learning, rainfall forecast, extreme eventsWeather forecast services in urban areas face an increasingly hard task of alerting the population\u00a0on extreme weather events. The hardness of the problem is due to the dynamics of the\u00a0phenomenon, which challenges numerical weather prediction models and opens an opportunity for\u00a0Machine Learning (ML) based models that may learn complex mappings between input-output\u00a0from data. In this paper, we present an ongoing research project which aims at building ML predictive\u00a0models for extreme precipitation forecast in urban areas, in particular in the Rio de Janeiro\u00a0City. We present the techniques that we have been developing to improve rainfall prediction and\u00a0extreme rainfall forecast, along with some initial experimental results. Finally, we discuss some\u00a0challenges that remain to be tackled in this project.",
      "axisX": "-0.3461572036572392",
      "axisY": "-0.5062546037442828",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220105",
      "title": "Data Assimilation by Neural Network for Ocean Circulation: Parallel Implementation",
      "authors": "Haroldo F. Campos Velho, Helaine C. M. Furtado, Sabrina B. M. Sambatti, Carla Barros Osthoff Ferreira de Barros, Maria E. S. Welter, Roberto P. Souto, Diego  Carvalho, Douglas O. Cardoso",
      "keywords": "Data Assimilation by Neural Network for Ocean Circulation: Parallel Implementationdata assimilation, artificial neural network, shallow water equations, parallel processingData assimilation (DA) is an essential issue for operational prediction centers, where a computer\u00a0code is applied to simulate physical phenomena by solving differential equations. The procedure\u00a0to determine the best initial condition combining data from observation and previous\u00a0forecasting (background) is carried out by a data assimilation method. The Kalman filter (KF) is\u00a0a technique for data assimilation, but it is computationally expensive. An approach to reduce the\u00a0computational effort for DA is to emulate the KF by a neural network. The multi-layer perceptron\u00a0neural network (MLP-NN) is employed to emulate the Kalman in a 2D ocean circulation model,\u00a0and algorithmic complexity to KF and NN is presented. A shallow-water system models the ocean\u00a0dynamics. Synthetic measurements are used for evaluating the MLP-NN for the data assimilation\u00a0process. Here, a parallel version for the DA procedure by the neural network is described and\u00a0tested, showing the performance improvement for a parallel version of the NN-DA.",
      "axisX": "0.31390049877264836",
      "axisY": "-0.002386147820654492",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220106",
      "title": "Multistage Iterative Method to Tackle Inverse Problems of Wave Tomography",
      "authors": "Alexander V. Goncharsky, Sergey Y. Romanov, Sergey Y. Seryozhnikov",
      "keywords": "Multistage Iterative Method to Tackle Inverse Problems of Wave Tomographyultrasound tomography, coefficient inverse problem, gradient method, numerical\nsimulationThis paper is concerned with developing the methods for solving inverse problems of lowfrequency\u00a0ultrasound tomography under scalar wave models using supercomputer technologies.\u00a0Unlike X-ray tomography, the inverse problem considered is posed as a problem of minimizing a\u00a0non-convex residual functional. The multistage iterative method (MSM) is proposed as a method\u00a0for obtaining an approximate solution to the inverse problem. Convergence of the method to the\u00a0exact solution is achieved via the use of low-frequency sounding signals at the initial stages of the\u00a0iterative method. The method is illustrated on model problems focused on ultrasound tomographic\u00a0diagnostics of soft tissues in medicine. Finite-difference time-domain method is used to solve the\u00a0wave equation, which accounts for most of the computational complexity of the method. The\u00a0multistage method reduces the computation time, since the initial stages use low-resolution finite\u00a0difference grids. The effectiveness of the MSM method is investigated on GPU and SIMD-capable\u00a0CPU computing platforms. Numerical simulations showed that modern processors equipped with\u00a0AVX-512 FPUs are capable of solving small-scale problems of wave tomography. For large-scale\u00a0tasks, GPUs equipped with fast on-board memory are preferred. The numerical algorithm is\u00a0data-parallel and well-suited for GPU architecture. The proposed method can be used in medical\u00a0imaging and nondestructive testing applications.",
      "axisX": "0.6052902553794975",
      "axisY": "0.28115688605583583",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220201",
      "title": "Computational Characterization of N-acetylaspartylglutamate Synthetase: From the Protein Primary Sequence to Plausible Catalytic Mechanism",
      "authors": "Igor V. Polyakov, Artem E. Kniga, Alexander V. Nemukhin",
      "keywords": "Computational Characterization of N-acetylaspartylglutamate Synthetase: From the Protein Primary Sequence to Plausible Catalytic Mechanismmolecular dynamics, quantum mechanics/molecular mechanics, QM/MM MD,\nGPU-accelerated algorithms, N-acetylaspartylglutamate synthetase, enzyme-substrate complexes,\nreaction intermediatesThe methods of supercomputer molecular modeling are applied to characterize structure and dynamics of one of the key human brain enzymes, N-acetylaspartylglutamate synthetase. The three-dimensional all-atom models of the enzyme with the reactants in the active site are constructed in several steps, starting from pilot protein structure in the apo-form obtained with the AlphaFold2 from the protein primary sequence. Deposition of reactant molecules into the protein cavity, construction of the reaction intermediate and relaxation of the complex are carried out with the help of large-scale classical molecular dynamics calculations. On the top of the construct, molecular dynamics simulations with the quantum mechanics/molecular mechanics interaction potentials are performed for the most promising conformations of the model system. Analysis of the latter allows us to propose plausible catalytic mechanisms of chemical reactions in the enzyme active site. The applied computational strategy opens the way towards ab initio enzymology using modern supercomputer simulations.",
      "axisX": "0.6817867319669292",
      "axisY": "-0.05238366961313153",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220202",
      "title": "Predicting the Activity of Boronate Inhibitors Against Metallo-\u03b2-lactamase Enzymes",
      "authors": "Elena O. Levina, Maria G. Khrenova, Vladimir G. Tsirelson",
      "keywords": "Predicting the Activity of Boronate Inhibitors Against Metallo-\u03b2-lactamase Enzymesmetallo- \u03b2-lactamase, boronate inhibitors, MD, QM/MM MD, quantum theory of\natoms in molecules (QTAIM), GPU-accelerated algorithmsPotency of boronate inhibitors against metallo-\u03b2-lactamases (M\u03b2Ls) has been found to be dependent on the electrophilicity of the boron atom. It forms a covalent bond with the oxygen atom of the catalytic OH\u2212 ion in the active site of the enzyme. The ability of the boronate inhibitor to influence the protein conformation also affects the binding potency. Molecular dynamics (MD) simulations of cyclic and non-cyclic boronate complexes with NDM-1 M\u03b2L show their higher impact on the inhibitor efficiency compared with the electrophilicity of the boron atom. Therefore, we focus on the hardware impact on the computational speedup of the GPU-accelerated MD. Using this data, we propose a comprehensive protocol for in silico prediction of the activity of boronate molecules against M\u03b2L enzymes, which includes MD simulations, combined quantum mechanics / molecular mechanics (QM/MM) computations and molecular dynamics simulations with the QM/MM potentials (QM/MM MD).",
      "axisX": "0.6466972044834595",
      "axisY": "-0.16481265088938013",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220203",
      "title": "Predicting Binding Free Energies for DPS Protein-DNA Complexes and Crystals Using Molecular Dynamics",
      "authors": "Eduard V. Tereshkin, Ksenia B. Tereshkina, Yurii F. Krupyanskii",
      "keywords": "Predicting Binding Free Energies for DPS Protein-DNA Complexes and Crystals Using Molecular Dynamicsmolecular dynamics, slow-growth thermodynamic integration method, DPS protein,\nDNA stabilization, DNA-DPS binding free energyThe interaction between deoxyribonucleic acid (DNA) and deoxyribonucleic acid-binding protein from starved cells (DPS) in bacterial cells leads to intracellular crystallization of the genetic material of bacteria, which contributes to the survival of bacteria under stress factors, including antibacterial agents. Molecular modeling can help explain the molecular mechanisms of DNA binding to this protein. In this paper, we report a supercomputer simulation of the molecular dynamics of several types DNA-DPS complexes and crystals ranging from DPS+DNA dimer to DNA in periodic crystal channels of Escherichia coli DPS protein using a coarse-grained Martini force field. By modeling DNA of 24 base pairs, comparable in size to the diameter of the DPS protein, we use the slow-growth thermodynamic integration method to find binding protein-DNA free energy and discuss the contribution of ions and the length of trajectories sufficient for this type of simulations. The results obtained are important for further research in the field of simulation of biological DNA-protein crystals and the study of the molecular mechanisms of DNA interaction with the DPS protein.",
      "axisX": "0.5191976374160099",
      "axisY": "-0.4163223226472886",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220204",
      "title": "Computational Modeling of the Interaction of Molecular Oxygen with the Flavin-dependent Enzyme RutA",
      "authors": "Igor V. Polyakov, Tatiana M. Domratcheva, Anna M. Kulakova, Alexander V. Nemukhin, Bella L. Grigorenko",
      "keywords": "Computational Modeling of the Interaction of Molecular Oxygen with the Flavin-dependent Enzyme RutAcomputational modeling, molecular dynamics, quantum mechanics/molecular mechanics, protein-oxygen interaction, flavin-dependent enzymesSupercomputer molecular modeling methods are applied to characterize structure and dynamics of the flavin-dependent enzyme RutA in the complex with molecular oxygen. Following construction of a model protein system, molecular dynamics (MD) simulations were carried out using either classical force field interaction potentials or the quantum mechanics/molecular mechanics (QM/MM) potentials. Several oxygen-binding pockets in the protein cavities were located in these simulations. The QM/MM-based MD calculations rely on the interface between the quantum chemistry package TeraChem and the MD package NAMD. The results show a stable localization of the oxygen molecule in the enzyme active site. Static QM/MM calculations carried out with two different packages, NWChem and TURBOMOLE, allowed us to establish the structure of the RutA-O2 complex. Biochemical perspectives of the hallmark reaction of incorporating oxygen into organic compounds emerged from these simulations are formulated.",
      "axisX": "0.6165790957064285",
      "axisY": "-0.03171499300914592",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220205",
      "title": "Analysis of Ion Atmosphere Around Nucleosomes Using Supercomputer MD Simulations",
      "authors": "Nikita A. Kosarim, Grigoriy A. Armeev, Mikhail P. Kirpichnikov, Alexey K. Shaytan",
      "keywords": "Analysis of Ion Atmosphere Around Nucleosomes Using Supercomputer MD Simulationsmolecular modeling, molecular dynamics simulations, nucleosomes, protein-DNA\ninteractions, monovalent cations, sodium, potassiumThe nucleosome is the basic unit of eukaryotic DNA compaction. It consists of about 147 base pairs wrapped around an octamer of histone proteins. Nucleosomal dynamics provides the availability of packaged DNA for various factors that carry out the vital processes associated with chromatin. It is not completely known how the structure and dynamics of the nucleosome depends on the ionic environment. The current researches do not give an unambiguous answer and often contradict each other. In this paper, we demonstrate supercomputer molecular dynamics simulations of nucleosome models surrounded by monovalent sodium and potassium cations. Analyzing the trajectories, we have shown the details of the distribution of sodium and potassium ions around the linker DNA, nucleosomal DNA at the sites of nucleosomal opening, and histone residues involved in the process of nucleosomal breathing. We have demonstrated the mobility of DNA linkers and the process of nucleosomal unwrapping in various ionic environments, and also assessed the probable mechanisms of the dependence of nucleosome unwrapping on the type of ions in the system. Our study is intended to emphasize the importance of understanding the role of the ionic environment in the functioning of chromatin.",
      "axisX": "0.4831787652689734",
      "axisY": "-0.16837629417739425",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220206",
      "title": "Molecular Modeling of Penicillin Acylase Binding with a Penicillin Nucleus by High Performance Computing: Can Enzyme or its Mutants Possess \u03b2-lactamase Activity?",
      "authors": "Evgeny M. Kirilin, Anna A. Bochkova, Nikolay V. Panin, Igor V. Pochinok, Vytas \u0160vedas",
      "keywords": "Molecular Modeling of Penicillin Acylase Binding with a Penicillin Nucleus by High Performance Computing: Can Enzyme or its Mutants Possess \u03b2-lactamase Activity?moonlighting protein, penicillin acylase engineering, \u03b2-lactam antibiotics resistance, \u03b2-lactamase design, metadynamicsHigh-performance computing has been used for molecular modeling of penicillin acylase interaction with a penicillin nucleus 6-aminopenicillanic acid (6-APA) to assess whether the wild-type enzyme or its mutants could possess \u03b2-lactamase activity. Applying parallel hybrid GPU/CPU computing technologies for metadynamics calculations with the PLUMED library in conjunction with AMBER software suite it has been shown that trace amounts of wild-type penicillin acylase6-APA complexes leading to a \u03b2-lactamase reaction can be formed. Higher \u03b2-lactamase activity can be observed in enzyme mutants by introducing charged residue in the substrate binding pocket and its proper positioning with respect to a catalytic nucleophile, including stabilization of the tetrahedral intermediate in the oxyanion hole. Thus, it has been shown that the certain mutations facilitate the orientation of the substrate required for the manifestation of \u03b2-lactamase activity in the penicillin acylase active center.",
      "axisX": "0.6339990870267816",
      "axisY": "-0.1372630866101844",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi220207",
      "title": "Search for Ligands Complementary to the 430-cavity of Influenza Virus Neuraminidase by Virtual Screening",
      "authors": "Dmitry K. Nilov, Michaela Schmidtke, Vadim A. Makarov, Vytas K. \u0160vedas",
      "keywords": "Search for Ligands Complementary to the 430-cavity of Influenza Virus Neuraminidase by Virtual Screeninginfluenza, neuraminidase, 430-cavity, inhibitor, anthrapyrazoleAn anthrapyrazole derivative STK663786 has been identified as a selective ligand of the socalled 430-cavity of influenza virus neuraminidase at virtual screening of a library of low-molecularweight compounds. It is able to form favorable contacts with hydrophobic residues as well as cation\u03c0 interaction and hydrogen bonds with the polar Arg371 residue. The experimentally determined EC\u2085\u2080 values have been found to be 19 and 30 \u00b5M for viruses H1N1 and H3N2, respectively. Complementarity of STK663786 to the 430-cavity adjacent to the sialic acid binding subsite in the active center of neuraminidase makes this compound a valuable structural fragment at construction of bifunctional inhibitors of the enzyme.",
      "axisX": "0.5088928431222857",
      "axisY": "-0.2543227954572969",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150201",
      "title": "Exascale Machines Require New Programming Paradigms and Runtimes",
      "authors": "Georges Da Costa, Thomas Fahringer, Juan Antonio Rico Gallego, Ivan Grasso, Atanas Hristov, Helen D. Karatza, Alexey Lastovetsky, Fabrizio Marozzo, Dana Petcu, Georgios L. Stavrinides, Domenico Talia, Paolo Trunfio, Hrachya Astsatryan",
      "keywords": "Exascale Machines Require New Programming Paradigms and Runtimesprogramming models, ultrascale, runtimes, extreme scaleExtreme scale parallel computing systems will have tens of thousands of optionally accelerator-equiped nodes with hundreds of cores each, as well as deep memory hierarchies and complex interconnect topologies. Such Exascale systems will provide hardware parallelism at multiple levels and will be energy constrained. Their extreme scale and the rapidly deteriorating reliablity of their hardware components means that Exascale systems will exhibit low mean-time-between-failure values. Furthermore, existing programming models already require heroic programming and optimisation efforts to achieve high efficiency on current supercomputers. Invariably, these efforts are platform-specific and non-portable. In this paper we will explore the shortcomings of existing programming models and runtime systems for large scale computing systems. We then propose and discuss important features of programming paradigms and runtime system to deal with large scale computing systems with a special focus on data-intensive applications and resilience.Finally, we also discuss code sustainability issues and propose several software metrics that are of paramount importance for code development for large scale computing systems.",
      "axisX": "-0.40337276471879274",
      "axisY": "0.28919809028364873",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150202",
      "title": "Acceleration of MPI mechanisms for sustainable HPC applications",
      "authors": "Jesus Carretero, Javier Garcia-Blas, David E. Singh, Florin Isaila, Alexey Lastovetsky, Thomas Fahringer, Radu Prodan, Peter Zangerl, Christi Symeonidou, Afshin Fassihi, Horacio P\u00e9rez-S\u00e1nchez",
      "keywords": "Acceleration of MPI mechanisms for sustainable HPC applicationsMPI, MPI sustainability, programming models, resilience, data management, MPI\napplicationsUltrascale computing systems are meant to reach a growth of two or three orders of magnitude of today computing systems. However, to achieve the performances required, we will need to design and implement more sustainable solutionsfor ultra-scale computing systems, understanding sustainability in a holistic manner to address challenges as economy-of-scale, agile elastic scalability, heterogeneity, programmability, fault resilience, energy efficiency, and scalable storage. Some of those solutions could be provided into MPI, but other should be devised as higher level concepts, less generalists, but adapted to applicative domains, possibly as programming patterns or or libraries. In this paper, we show some proposals to extend MPI trying to cover major domains that are relevant towards sustainability: MPI programming optimizations and programming models, resilience, data management, and their usage from applications.",
      "axisX": "-0.4304990043256997",
      "axisY": "0.09819601748204566",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150203",
      "title": "Resilience within Ultrascale Computing System: Challenges and Opportunities from Nesus Project",
      "authors": "Pascal Bouvry, Rudolf Mayer, Jakub Muszy\u0144ski, Dana Petcu, Andreas Rauber, Gianluca Tempesti, Tuan Trinh, S\u00e9bastien Varrette",
      "keywords": "Resilience within Ultrascale Computing System: Challenges and Opportunities from Nesus Projecthigh performance computing, fault tolerance, algorithm-based fault tolerance, extreme data, evolutionary algorithm, ultrascale computing systemUltrascale computing is a new computing paradigm that comes naturally from the necessity of computing systems that should be able to handle massive data in possibly very large scale \u00a0distributed systems, enabling new forms of applications that can serve a very large amount of \u00a0users and in a timely manner that we have never experienced before. However, besides the benefits, \u00a0ultrascale computing systems do not come without challenges. One of the challenges is the resilience \u00a0of ultrascale computing systems. Although resilience is already an established field in system \u00a0science and many methodologies and approaches are available to deal with it, the unprecedented \u00a0scales of computing, of the massive data to be managed, new network technologies, and drastically \u00a0new forms of massive scale applications bring new challenges that need to be addressed. This paper \u00a0reviews the challenges and approaches of resilience in ultrascale computing systems from multiple \u00a0perspectives involving and addressing the resilience aspects of hardware-software co-design for \u00a0ultrascale systems, resilience against (security) attacks, new approaches and methodologies to \u00a0resilience in ultrascale systems, applications and case studies.",
      "axisX": "-0.747399973817449",
      "axisY": "-0.3503846522949159",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150204",
      "title": "Energy Measurement Tools for Ultrascale Computing: A Survey",
      "authors": "Francisco Almeida, Javier Arteaga, Vicente Blanco, Alberto Cabrera",
      "keywords": "Energy Measurement Tools for Ultrascale Computing: A Surveyenergy measurement, power measurement, data acquisition tools, infrastructure\nmanagement, ultrascale computingWith energy efficiency one of the main challenges on the way towards ultrascale systems, there is great need for access to high-quality energy consumption data. Such data would enable researchers and designers to pinpoint energy inefficiencies at all levels of the computing stack, from whole nodes down to critical regions of code. However, measurement capabilities are often missing, and significantly differ between platforms where they exist. A standard is yet \u00a0to be established. To that end, this paper attempts an extensive survey of energy measurement tools currently available at both the hardware and software level, comparing their features with respect to energy monitoring.",
      "axisX": "-0.7109242109490882",
      "axisY": "-0.058089606581829115",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150205",
      "title": "Energy-efficient Algorithms for Ultrascale Systems",
      "authors": "Jesus Carretero, Salvatore Distefano, Dana Petcu, Daniel Pop, Thomas Rauber, Gudula R\u00fcnger, David E. Singh",
      "keywords": "Energy-efficient Algorithms for Ultrascale Systemsenergy-awareness, energy-efficient algorithms, ultrascale computingThe chances to reach Exascale or Ultrascale Computing are strongly connected with the problem of the energy consumption for processing applications. For physical as well as economical reasons, the energy consumption has to be reduced significantly to make Ultrascale Computing possible. The research efforts towards energy-saving mechanisms of the hardware has already led to energy-aware hardware systems available today. However, hardware mechanisms can only obtain an energy reduction if software can exploit them such that energy-efficient computing actually results. In the software area, there also exists a multitude of research approaches towards energy saving. These research approaches and results are often isolated either on the system software level or the application organization level, reflecting the expertise of the corresponding research group. The challenge of reducing the energy consumption dramatically to make Ultrascale Computing possible are so ambitions that a concerted action combining all these software levels and research efforts seems reasonable. In this article, we demonstrate the current research efforts and results related to energy in the diverse areas of software. Moreover, we conclude with open problems and questions concerning energy-related techniques with an emphasis on the application algorithmic side.",
      "axisX": "-0.6119259530411107",
      "axisY": "-0.5198090416778032",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150206",
      "title": "Energy Efficiency for Ultrascale Systems: Challenges and Trends from Nesus Project",
      "authors": "Michel Bagein, Jorge Barbosa, Vicente Blanco, Ivona Brandic, Samuel Cremer, Sebastien Fremal, Helen Karatza, Laurent Lefevre, Toni Mastelic, Ariel Oleksiak, Anne-Cecile Orgerie, Georgios L. Stavrinides, Sebastien Varrette",
      "keywords": "Energy Efficiency for Ultrascale Systems: Challenges and Trends from Nesus Projectenergy and power measurement, data acquisition tools, energy modeling, scheduling,\napplications, heterogeneous infrastructures, ultrascale computingEnergy consumption is one of the main limiting factors for designing and deploying ultrascale systems. Therefore, this paper presents challenges and trends associated with energy efficiency for ultrascale systems based on current activities of the working group on \"Energy Efficiency\" in the European COST Action Nesus IC1305. The analysis contains major areas that are related to studies of energy efficiency in ultrascale systems: heterogeneous and low power hardware architectures, power monitoring at large scale, modeling and simulation of ultrascale systems, energy-aware scheduling and resource management, and energy-efficient application design.",
      "axisX": "-0.44373924756418964",
      "axisY": "-0.09812955848029488",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140302",
      "title": "Research Problems and Opportunities in Memory Systems",
      "authors": "Onur Mutlu, Lavanya Subramanian",
      "keywords": "Research Problems and Opportunities in Memory Systemsmemory systems, scaling, DRAM, flash, non-volatile memory, QoS, reliabilityThe memory system is a fundamental performance and energy bottleneckin almost all computing systems. Recent system design, application,and technology trends that require more capacity, bandwidth,efficiency, and predictability out of the memory system make it aneven more important system bottleneck. At the same time, DRAMtechnology is experiencing difficult {\\em technology scaling}challenges that make the maintenance and enhancement of its capacity,energy-efficiency, and reliability significantly more costly withconventional techniques.In this article, after describing the demands and challenges faced bythe memory system, we examine some promising research and designdirections to overcome challenges posed by memoryscaling. Specifically, we describe three major {\\em new} researchchallenges and solution directions: 1) enabling new DRAMarchitectures, functions, interfaces, and better integration of theDRAM and the rest of the system (an approach we call {\\em system-DRAM\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 co-design}), 2) designing a memory system that employs emergingnon-volatile memory technologies and takes advantage of multipledifferent technologies (i.e., {\\em hybrid memory systems}), 3)providing predictable performance and QoS to applications sharing thememory system (i.e., {\\em QoS-aware memory systems}). We also brieflydescribe our ongoing related work in combating scaling challenges ofNAND flash memory.\u00a0",
      "axisX": "-0.6867124443537331",
      "axisY": "-0.09202864730985622",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140303",
      "title": "Early evaluation of direct large-scale InfiniBand networks with adaptive routing",
      "authors": "Alexander N. Daryin, Anton A. Korzh",
      "keywords": "Early evaluation of direct large-scale InfiniBand networks with adaptive routingadaptive routing, InfiniBand, high-radix topology, network simulationWe assess the problem of choosing optimal direct topology for InfiniBand networks in terms of performance. Newest\u00a0topologies like Dragonfly, Flattened butterfly and Slim Fly are considered, as well as standard Tori and Hypercubes.We consider some reasonable extensions to InfiniBand hardware which could be implemented by vendors easily and may\u00a0allow reasonable routing algorithms for such topologies. A number of routing algorithms are proposed and compared for various traffic patterns. Mapping algorithms for Dragonfly and Flattened Butterfly are proposed. Based on this\u00a0research it has been decided to use Flattened Butterfly topology for system #22 in November 2014 Top 500 list.",
      "axisX": "-0.08527943067958194",
      "axisY": "-0.14662175152442572",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140304",
      "title": "Heterogeneous parallel computing: from clusters of workstations to hierarchical hybrid platforms",
      "authors": "Alexey Lastovetsky",
      "keywords": "Heterogeneous parallel computing: from clusters of workstations to hierarchical hybrid platformsparallel computing, heterogeneous computing, data partitioningThe paper overviews the state of the art in design and implementation of data parallelscientic applications on heterogeneous platforms. It covers both traditional approaches originallydesigned for clusters of heterogeneous workstations and the most recent methods developed in thecontext of modern multicore and multi-accelerator heterogeneous platforms.",
      "axisX": "0.019150460366215982",
      "axisY": "0.6404399628508388",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi140305",
      "title": "Co-design of Parallel Numerical Methods for Plasma Physics and Astrophysics",
      "authors": "Boris M. Glinskiy, Igor M. Kulikov, Alexey V. Snytnikov, Alexey A. Romanenko, Igor G. Chernykh, Vitaly A. Vshivkov",
      "keywords": "Co-design of Parallel Numerical Methods for Plasma Physics and AstrophysicsCo-design, hybrid supercomputers, Particle-In-Cell method, Godunov method,\nGPUPhysically meaningful simulations in plasma physics and astrophysics need powerful hybrid supercomputers\u00a0equipped with computation accelerators. The development of parallel numerical codes for such supercomputers is a complex scientific problem. In order to solve it the concept of co-design is employed. The co-design is defined as considering the architecture of the supercomputer at all stages of the development of the code. The use of co-design is shown by the example of two physical problems: the interaction of an electron beam with plasma and the collision of galaxies. The resulting speedup and efficiency are shown.",
      "axisX": "0.4291928496611726",
      "axisY": "0.225872719775042",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150101",
      "title": "AlgoWiki: an Open Encyclopedia of Parallel Algorithmic Features",
      "authors": "Vladimir V. Voevodin, Alexander S. Antonov, Jack Dongarra",
      "keywords": "AlgoWiki: an Open Encyclopedia of Parallel Algorithmic Featuresalgorithm structure, resource of parallelism, parallel computing, efficiency, performance, supercomputers, scalability, data locality, encyclopedia of algorithmic featuresThe main goal of this project is to formalize the mapping of algorithms onto the architecture of parallel computing systems. The basic idea is that features of algorithms are independent of any computing system. A detailed description of a given algorithm with a special emphasis on its parallel properties is made once, and after that it can be used repeatedly for various implementations of the algorithm on different computing platforms. Machine-dependent, part of this work is devoted to describing features of algorithms implementation for different parallel architectures. The proposed description of algorithms includes many non-trivial features such as: parallel algorithm complexity, resource of parallelism and its properties, features of the informational graph, computational cost of algorithms, data locality analysis as well as analysis of scalability potential, and many others. Descriptions of algorithms form the basis of AlgoWiki, which allows for collaboration with the computing community in order to produce different implementations and achieve improvement. Project website: http://algowiki-project.org/en/",
      "axisX": "-0.2688554043210781",
      "axisY": "0.2928869874570338",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150102",
      "title": "Applications for ultrascale computing",
      "authors": "Milan Mihajlovic, Lars Ailo Bongo, Raimondas Ciegis, Neki Frasheri, Dragi Kimovski, Peter Kropf, Svetozar Margenov, Maya Neytcheva, Thomas Rauber, Gudula Runger, Roman Trobec, Roel Wuyts, Roman Wyrzykowski, Jing Gong",
      "keywords": "Applications for ultrascale computingsustainable ultrascale systems, impact factors on applications, multiscale and multiphysics applications, computational modellingStudies of complex physical and engineering systems, represented by multi-scale and multi-physics computer simulations have an increasing demand for computing power, especially when the simulations of realistic problems are considered. This demand is driven by the increasing size and complexity of the studied systems or the time constraints. Ultrascale computing systems offer a possible solution to this problem. Future ultrascale systems will be large-scale complex computing systems combining technologies from high performance computing, distributed systems, big data, and cloud computing. Thus, the challenge of developing and programming complex algorithms on these systems is twofold. Firstly, the complex algorithms have to be either developed from scratch, or redesigned in order to yield high performance, while retaining correct functional behaviour. Secondly, ultrascale computing systems impose a number of non-functional cross-cutting concerns, such as fault tolerance or energy consumption, which can significantly impact the deployment of applications on large complex systems. This article discusses the state-of-the-art of programming for current and future large scale systems with an emphasis on complex applications. We derive a number of programming and execution support requirements by studying several computing applications that the authors are currently developing and discuss their potential and necessary upgrades for ultrascale execution.",
      "axisX": "-0.490296026159412",
      "axisY": "-0.3568283809985381",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150103",
      "title": "Dense Matrix Computations on NUMA Architectures with Distance-Aware Work Stealing",
      "authors": "Rabab Al-Omairy, Guillermo Miranda, Hatem Ltaief, Rosa M. Badia, Xavier Martorell, Jesus Labarta, David Keyes",
      "keywords": "Dense Matrix Computations on NUMA Architectures with Distance-Aware Work StealingDense Matrix Computations, Dynamic Runtime Systems, Software Productivity,\nNon-Uniform Memory Access, Data Locality, Work Stealing, High Performance ComputingWe employ the dynamic runtime system OmpSs to decrease the overhead of data motion in the now ubiquitous non-uniform memory access (NUMA) high concurrency environment of multicore processors. The dense numerical linear algebra algorithms of Cholesky factorization and symmetric matrix inversion are employed as representative benchmarks. Work stealing occurs within an innovative NUMA-aware scheduling policy to reduce data movement between NUMA nodes. The overall approach achieves separation of concerns by abstracting the complexity of the hardware from the end users so that high productivity can be achieved. Performance results on a large NUMA system outperform the state-of-the-art existing implementations up to a two fold speedup for the Cholesky factorization, as well as the symmetric matrix inversion, while the OmpSs-enabled code maintains strong similarity to its original sequential version.",
      "axisX": "0.4646310963680279",
      "axisY": "0.39652612928939324",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150104",
      "title": "Neo-hetergeneous Programming and Parallelized Optimization of a Human Genome Re-sequencing Analysis Software Pipeline on TH-2 Supercomputer",
      "authors": "Xiangke Liao, Shaoliang Peng, Yutong Lu, Yingbo Cui, Chengkun Wu, Heng Wang, Jiajun Wen",
      "keywords": "Neo-hetergeneous Programming and Parallelized Optimization of a Human Genome Re-sequencing Analysis Software Pipeline on TH-2 Supercomputerbiological big data; parallelized optimization; TH-2; sequence alignment; SNP detection; whole genome re-sequencingThe growing velocity of biological big data is way beyond Moore's Law of compute power growth. The amount of genomic data has been explosively accumulating, which calls for an enormous amount of computing power, while current computation methods cannot scale out with the data explosion. In this paper, we try to utilize huge computing resources to solve thebig dataproblems of genome processing on TH-2 supercomputer. TH-2supercomputer adopts neo-heterogeneous architecture and owns 16,000 compute nodes: 32000 Intel Xeon CPUs + 48000 Xeon Phi MICs. The heterogeneity, scalability, and parallel efficiency pose great challenges forthe deployment of the genomeanalysis software pipeline on TH-2. Runtime profiling shows that SOAP3-dp and SOAPsnp are the most time-consuming parts (up to 70% of total runtime) in the whole pipeline, which need parallelized optimization deeply and large-scale deployment. To address this issue, we first designa series of new parallel algorithms for SOAP3-dp and SOAPsnp, respectively, to eliminatethe spatial-temporal redundancy. Then we propose a CPU/MIC collaboratedparallel computing method in one node to fully fill the CPU/MIC time slots. We also propose a series ofscalable parallel algorithms and large scaleprogramming methods to reduce the amount of communications between different nodes. Moreover, we deploy and evaluate our works on the TH-2 supercomputer in different scales. At the most large scale, the whole process takes 8.37 hours using 8192 nodes to finish the analysis of a 300TB dataset of whole genome sequences from 2,000 human beings, which can take as long as 8 months on a commodity server. The speedup is about 700x.",
      "axisX": "-0.21904983784507998",
      "axisY": "0.07337388740640396",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150301",
      "title": "Data Exploration at the Exascale",
      "authors": "Hank Childs",
      "keywords": "Data Exploration at the Exascalescientific visualization, high-performance computing, Lagrangian flow analysisIn situ processing - i.e., coupling visualization routines to a simulation code to generate images in real-time - is predicted to be the dominant form for visualization on upcoming supercomputers. Unfortunately, traditional in situ techniques are largely incongruent with exploratory visualization, which is an important activity to enable understanding of simulation data. In re- sponse, a new paradigm is emerging: data is transformed and massively reduced in situ and then the resulting form is explored post hoc. The fundamental tension in this approach is between the extent of the data reduction and the loss in integrity in the resulting data. However, new oppor- tunities, in terms of increased access to data, may blunt this tension and allow for both sufficient data reduction and also more accurate analysis. With this paper, we describe the trends behind \"data exploration at the exascale\" and also summarize some recent results that confirmed that this new paradigm can produce superior results compared to the traditional one.\u00a0",
      "axisX": "-0.29654898349174813",
      "axisY": "-0.4652662763825422",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150302",
      "title": "InfiniCloud: Leveraging the Global InfiniCortex Fabric and OpenStack Cloud for Borderless High Performance Computing of Genomic Data",
      "authors": "Kenneth Hon Kim Ban, Jakub Chrzeszczyk, Andrew Howard, Dongyang Li, Tin Wee Tan",
      "keywords": "InfiniCloud: Leveraging the Global InfiniCortex Fabric and OpenStack Cloud for Borderless High Performance Computing of Genomic DataGenomics, Cloud-Computing, InfiniBand, Trans-continental, Virtualization, SRIOV, OpenStack, HPCAt the Supercomputing Frontiers Conference in Singapore in 2015, A*CRC (Singapore) and NCI (Canberra, Australia) presented InfiniCloud, a geographically distributed, high performance InfiniBand HPC Cloud which aims to enable borderless processing of genomic data as part of the InfiniCortex project. This paper provides a high-level technical overview of the architecture of InfiniCloud and how it can be used for high performance computation of genomic data in geographically distant sites by encapsulation of workflows/applications in Virtual Machines (VM) coupled with on-the-fly configuration of clusters and high speed transfer of data via long range InfiniBand.\u00a0",
      "axisX": "-0.024646844384793413",
      "axisY": "0.5520199927374428",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150303",
      "title": "Performance Assessment of InfiniBand HPC Cloud Instances on Intel Haswell and Intel Sandy Bridge Architectures",
      "authors": "Jonathan Low, Jakub Chrzeszczyk, Andrew Howard, Andrzej Chrzeszczyk",
      "keywords": "Performance Assessment of InfiniBand HPC Cloud Instances on Intel Haswell and Intel Sandy Bridge ArchitecturesCloud-Computing, InfiniBand, Trans-continental, Benchmarking, Virtualization,\nSRIOV, BeeGFS, OpenStack, HPCThis paper aims to establish a performance baseline of a HPC installation of OpenStack. We created InfiniCloud - a distributed High Performance Cloud hosted on remote nodes of InfiniCortex. InfiniCloud compute nodes use high performance Intel (R) Haswell and Sandy Bridge CPUs, SSD storage and 64-256GB RAM. All computational resources are connected by high performance IB interconnects and are capable of trans-continental IB communication using Obsidian Longbow range extenders.We benchmark the performance of our test-beds using micro-benchmarks for TCP bandwidth, IB bandwidth and latency, file creation performance, MPI collectives and Linpack. This paper compares different CPU generations across virtual and bare-metal environments.The results show modest improvements in TCP and IB bandwidth and latency on Haswell; performance being largely dependent on the IB hardware. Virtual overheads were minimal and near-native performance is possible for sufficiently large messages. From the Linpack testing, users can expect more than twice the performance in their applications on Haswell-provisioned VMs. On Haswell hardware, native and virtual performance differences is still significant for MPI collective operations. Finally, our parallel filesystem testing revealed virtual performance coming close to native only for non-sync/fsync file operations.",
      "axisX": "0.03920491875581813",
      "axisY": "0.4709952971078209",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150304",
      "title": "The L-CSC cluster: Optimizing power efficiency to become the greenest supercomputer in the world in the Green500 list of November 2014",
      "authors": "David Rohr, Gvozden Neskovic, Volker Lindenstruth",
      "keywords": "The L-CSC cluster: Optimizing power efficiency to become the greenest supercomputer in the world in the Green500 list of November 2014L-CSC, HPL, Linpack, Green500, GPU, Energy Efficiency, HPC, LQCDThe L-CSC (Lattice Computer for Scientific Computing) is a general purpose compute cluster built with commodity hardware installed at GSI. Its main operational purpose is Lattice QCD (LQCD) calculations for physics simulations. Quantum Chromo Dynamics (QCD) is the physical theory describing the strong force, one of the four known fundamental interactions in the universe. L-CSC leverages a multi-GPU design accommodating the huge demand of LQCD for memory bandwidth. In recent years, heterogeneous clusters with accelerators such as GPUs have become more and more powerful while supercomputers in general have shown enormous increases in power consumption making electricity costs and cooling a significant factor in the total cost of ownership. Using mainly GPUs for processing, L-CSC is very power-efficient, and its architecture was optimized to provide the greatest possible power efficiency. This paper presents the cluster design as well as optimizations to improve the power efficiency. It examines the power measurements performed for the Green500 list of the most power-efficient supercomputers in the world which led to the number 1 position as the greenest supercomputer in November 2014.",
      "axisX": "-0.13967839096646623",
      "axisY": "0.15219808072755434",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150305",
      "title": "An Autonomic Performance Environment for Exascale",
      "authors": "Kevin A. Huck, Allan Porterfield, Nick Chaimov, Hartmut Kaiser, Allen D. Malony, Thomas Sterling, Rob Fowler",
      "keywords": "An Autonomic Performance Environment for ExascaleParalleX, HPX, exascale, performance measurement, adaptive runtimesExascale systems will require \u00a0new approaches to performance observation, analysis, and runtime decision-making to optimize for performance and efficiency. The standard \"first-person\" model, in which multiple operating system processes and threads observe themselves and record first-person performance profiles or traces for offline analysis, is not adequate to observe and capture interactions at shared resources in highly concurrent, dynamic systems. Further, it does not support mechanisms for runtime adaptation. Our approach, called APEX (Autonomic Performance Environment for eXascale), provides mechanisms for sharing information among the layers of the software stack, including hardware, operating and runtime systems, and application code, both new and legacy. The performance measurement components share information \u00a0across layers, merging first-person data sets with information collected by \u00a0third-person tools observing shared hardware and software states at \u00a0node- and global-levels. Critically, APEX provides a policy engine designed to guide runtime adaptation mechanisms to make algorithmic changes, re-allocate resources, or change scheduling rules when appropriate conditions occur.",
      "axisX": "-0.3576043966257643",
      "axisY": "0.3242516957840615",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150306",
      "title": "Visualization for Exascale: Portable Performance is Critical",
      "authors": "Kenneth Moreland, Matthew Larsen, Hank Childs",
      "keywords": "Visualization for Exascale: Portable Performance is Criticalscientific visualization, exascale, performance portability, data parallel primitivesResearchers face a daunting task to provide scientific visualization\u00a0capabilities for exascale computing. Of the many fundamental changes we\u00a0are seeing in HPC systems, one of the most profound is a reliance on new\u00a0processor types optimized for execution bandwidth over latency hiding.\u00a0Multiple vendors create such accelerator processors, each with\u00a0significantly different features and performance characteristics. To\u00a0address these visualization needs across multiple platforms, we are\u00a0embracing the use of data parallel primitives that encapsulate highly\u00a0efficient parallel algorithms that can be used as building blocks for\u00a0conglomerate visualization algorithms. We can achieve performance\u00a0portability by optimizing this small set of data parallel primitives\u00a0whose tuning conveys to the conglomerates.",
      "axisX": "-0.23223739434579868",
      "axisY": "0.4606424994008461",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150307",
      "title": "A Case for Embedded FPGA-based SoCs in Energy-Efficient Acceleration of Graph Problems",
      "authors": "Nachiket Kapre, Pradeep Moorthy",
      "keywords": "A Case for Embedded FPGA-based SoCs in Energy-Efficient Acceleration of Graph Problemsenergy efficiency, sparse graphs, embedded SoCs, FPGAsSparse graph problems are notoriously hard to accelerate on conventional platforms due to irregular memory access patterns resulting in underutilization of memory bandwidth. These bottlenecks on traditional x86-based systems mean that sparse graph problems scale very poorly, both in terms of performance and power efficiency. A cluster of embedded SoCs (systems-on-chip) with closely-coupled FPGA accelerators can support distributed memory accesses with better matched low-power processing. We first conduct preliminary experiments across a range of COTS (commercial off-the-shelf) embedded SoCs to establish promise for energy-efficiency acceleration of sparse problems. We select the Xilinx Zynq SoC with FPGA accelerators to construct a prototype 32-node Beowulf cluster. We develop specialized MPI routines and memory DMA offload engines to support irregular communication efficiently. In this setup, we use the ARM processor as a data marshaller for local DMA traffic as well as remote MPI traffic while the FPGA may be used as a programmable accelerator. Across a set of benchmark graphs, we show that 32-node embedded SoC cluster can exceed the energy efficiency of an Intel E5-2407 by as much as 1.7\u00d7 at a total graph processing capacity of 91\u201395 MTEPS for graphs as large as 32 million nodes and edges.\u00a0",
      "axisX": "-0.01173363668503952",
      "axisY": "0.39037601757300594",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150401",
      "title": "Live Programming in Scientific Simulation",
      "authors": "Ben Swift, Andrew Sorensen, Henry Gardner, Peter Davis, Viktor K. Decyk",
      "keywords": "Live Programming in Scientific Simulationlive programming, particle-in-cell, JIT-compilationWe demonstrate that a live-programming environment can be used to harness and add run-time interactivity to scientific simulation codes. Through a set of examples using a Particle-In-Cell (PIC) simulation framework we show how the real-time, human-in-the-loop interactivity of live programming can be incorporated into a traditional, \u201coffline\u201d, development workflow. We discuss how live programming tools and techniques can be productively integrated into the existing HPC landscape to increase productivity and enhance exploration and discovery.",
      "axisX": "-0.13477721434865525",
      "axisY": "0.5690422836724126",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150402",
      "title": "Creating interconnect topologies by algorithmic edge removal: MOD and SMOD graphs",
      "authors": "Marek T. Michalewicz, \u0141ukasz P. Or\u0142owski, Yuefan Deng",
      "keywords": "Creating interconnect topologies by algorithmic edge removal: MOD and SMOD graphssupercomputer interconnects, big data, exascale computing, graph theory, topology\nof graphs, classes of graphs, graph generationWe introduce a method of constructing classes of graphs by algorithmic removal of entire groups of edges. Our approach to creating new classes of graphs is to focus entirely on the structure and properties of the adjacency matrix. At an initialisation step of the algorithm we start with a complete (fully connected) graph. In Part I we present MOD and arrested MOD graphs resulting from removal of square blocks of edges at each iteration and substitution of removed blocks with a diagonal matrix with one extra pivotal element along the main diagonal. The MOD graphs possess unique and useful properties. All important graph measures are easily expressed in analytical form and are presented in the paper. Several important properties of MOD graphs compare very favourably with graphs representing common interconnect topologies: hypercube, 3D and 5D tori, TOFU and dragony. This lead us to consider MOD and arrested MOD graphs as interesting candidats for eective supercomputer interconnects.In Part II, at each iterative step we successively remove triangular shapes from adjacency matrix. This iterative process leads to the nal matrix which has two Sierpinski gaskets aligned along the main diagonal. It will be shown below, that this new class of graphs is not a Sierpinski graph, since it is the adjacency matrix which has a structure of a Sierpinski gasket, and not a graph described by this matrix. We call this new class of graphs Sierpinski-Michalewicz-Or lowski-Deng (SMOD) graphs. The most remarkable property of the SMOD class of graphs, is that irrespective of the graph order, the diameter is constant and equals 2. The size of the graph, or the total number of edges, is about 10% of the size of a complete graph of the same order. We analyse important graph theoretic characte-ristics related to the topology such as diameter as a function of graph order, size, mean path length, ratio of the graph size to the size of a complete graph of the same order, and some spectral properties.Keywords: supercomputer interconnects, big data, exascale computing, graph theory,topology of graphs, classes of graphs, graph generation.",
      "axisX": "0.3334087909337235",
      "axisY": "-0.2549071766564863",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150403",
      "title": "Multi-Scale Supercomputing of Large Molecular Aggregates: A Case Study of the Light-Harvesting Photosynthetic Center",
      "authors": "Alexander V. Nemukhin, Igor V. Polyakov, Alexander I. Moskovsky",
      "keywords": "Multi-Scale Supercomputing of Large Molecular Aggregates: A Case Study of the Light-Harvesting Photosynthetic Centerquantum chemistry, fragmentation, multi-scale approaches, parallel algorithmsNumerical solution of the quantum mechanical Schr\u00f6dinger equation is required to model electronic excitations in the light-harvesting photosynthetic complexes composed of up to millions of atoms. We demonstrate that the modern supercomputers can be used to treat electronic structure calculations in such large molecular aggregates if proper multi-scale massive-parallel approaches are applied. We show that the three-level parallelization scheme based on the novel numerical algorithms assuming fragmentation of a light-harvesting complex allows us to reduce considerably the high scaling of ab initio quantum chemistry methods. More specifically we applied the time-dependent density functional theory based upon the fragment molecular orbital presentation (FMO-TDDFT) implemented at the modern supercomputers to obtain a realistic estimate of the electronic excitation in the complex. The application shows a good overall scaling. \u00a0",
      "axisX": "0.738761228866444",
      "axisY": "-0.12436798065693136",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150404",
      "title": "Parallel software platform INMOST: a framework for numerical modeling",
      "authors": "Alexander A. Danilov, Kirill M. Terekhov, Igor N. Konshin, Yuri V. Vassilevski",
      "keywords": "Parallel software platform INMOST: a framework for numerical modelingdistributed mesh, polyhedral mesh, parallel framework, numerical modelingThe INMOST mathematical modeling toolkit helps a user to formulate and solve a problem of partial differential equations on general meshes in parallel. The current work covers: data structure description for efficient distributed unstructured mesh representation, interrelation of mesh elements with maximal flexibility of supported types of the mesh, treatment of ghost cells and distribution of mesh data for parallel execution, flexible templates for the implementation of numerical schemes, convenient framework for parallel linear systems assembly and solution. We also present aspects of the implementation and a simple example of application of INMOST to the solution of anisotropic diffusion problem. On this example we demonstrate the application of INMOST for all the stages of numerical modeling: construction of the distributed mesh, assignment of the problem data to the elements, problem discretization on local domain, solution of linear system in parallel. INMOST is a newly developed, flexible and efficient numerical analysis framework that provides scientists the infrastructure for designing highly scalable high performance applications for mathematical modeling.",
      "axisX": "0.3710889019400838",
      "axisY": "0.3641838570980449",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi150405",
      "title": "Parallel Programming Models for Dense Linear Algebra on Heterogeneous Systems",
      "authors": "Jack Dongarra, M. Abalenkovs, A. Abdelfattah, M. Gates, A. Haidar, J. Kurzak, P. Luszczek, S. Tomov, I. Yamazaki, A. YarKhan",
      "keywords": "Parallel Programming Models for Dense Linear Algebra on Heterogeneous SystemsProgramming models, runtime, HPC, GPU, multicore, dense linear algebraWe present a review of the current best practices in parallel programming models for dense linear algebra (DLA) on heterogeneous architectures. We consider multicore CPUs, stand alone manycore coprocessors, GPUs, and combinations of these. Of interest is the evolution of the programming models for DLA libraries { in particular, the evolution from the popular LAPACK and ScaLAPACK libraries to their modernized counterparts PLASMA (for multicore CPUs) and MAGMA (for heterogeneous architectures), as well as other programming models and libraries.Besides providing insights into the programming techniques of the libraries considered, we outline our view of the current strengths and weaknesses of their programming models { especially in regards to hardware trends and ease of programming high-performance numerical software that current applications need { in order to motivate work and future directions for the next generation of parallel programming models for high-performance linear algebra libraries on heterogeneous systems.",
      "axisX": "-0.1375535193945472",
      "axisY": "0.4365535117291838",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160101",
      "title": "NR-MPI: A Non-stop and Fault Resilient MPI Supporting Programmer Defined Data Backup and Restore for E-scale Super Computing Systems",
      "authors": "Suo Guang",
      "keywords": "NR-MPI: A Non-stop and Fault Resilient MPI Supporting Programmer Defined Data Backup and Restore for E-scale Super Computing SystemsMessage passing interface, fault tolerant MPI, NR-MPI, Application-level Checkpoint/RestartFault resilience has became a major issue for HPC systems, particularly, in the perspective of future E-scale systems, which will consist of millions of CPU cores and other components. MPI-level fault tolerant constructs, such as ULFM, are being proposed to support software level fault tolerance. However, there are few systematic evaluations by application programmers using benchmarks or pseudo applications. This paper proposes NR-MPI, a \\emph{N}on-stop and Fault \\emph{R}esilient \\emph{MPI}, supporting programmer defined data backup and restore. To help programmers write fault tolerant programs, NR-MPI provides a set of friendly programming interfaces and a state transition diagram for data backup and restore. This paper focuses on design, implementation and evaluation of NR-MPI. Specifically,this paper puts emphases on failure detection in MPI library, friendly programming interface extending for NR-MPI and examples of fault tolerant programs based NR-MPI. Furthermore, to support failure recovery of applications, NR-MPI implements data backup interfaces based on double in-memory checkpoint/restart. We conduct experiments with both NPB benchmarks and Sweep3D on TH supercomputer in NSCC-TJ. Experimental results show that NR-MPI based fault tolerant programs can recover from failures online without restarting, and the overhead is small even for applications with tens of thousands of cores.",
      "axisX": "-0.6097856966712522",
      "axisY": "0.25009997048902477",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160102",
      "title": "Reconfigurable computer systems: from the first FPGAs towards liquid cooling systems",
      "authors": "Ilya I. Levin, Alexey I. Dordopulo, Alexander M. Fedorov, Igor A. Kalyaev",
      "keywords": "Reconfigurable computer systems: from the first FPGAs towards liquid cooling systemsFPGA, reconfigurable computer systems, immersion liquid cooling systemThe paper covers the history of development of design technologies of reconfigurable computer systems based on FPGAs of various families. Five generations of reconfigurable computer systems with high placement density, designed on the base of various FPGA families, from Xilinx Virtex-E to modern Virtex UltraScale, are described. The last achievements in the domain of design of energetic effective reconfigurable computer systems with high real performance are presented. One of such achievements is the developed liquid cooling system for Virtex UltraScale FPGAs. It provides independent circulation of the cooling liquid in the 3U computational module with the 19\u2019\u2019 height for cooling of 96-128 FPGA chips that in total generate 9.6-12.8 kWatt of heat. The distinctive features of the designed immersion liquid cooling system are high cooling efficiency with power reserve for the designed perspective FPGA families, resistance to leaks and their consequences, and compatibility with traditional water cooling systems based on industrial chillers.",
      "axisX": "-0.07303320697871524",
      "axisY": "0.05834804091259144",
      "class": "0"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160103",
      "title": "Supercomputer technologies in tomographic imaging applications",
      "authors": "Alexander V. Goncharsky, Sergey Y. Romanov, Sergey Y. Seryozhnikov",
      "keywords": "Supercomputer technologies in tomographic imaging applicationssupercomputer, GPU cluster, wave-tomography imaging, coefficient inverse problems, scalar wave equation, breast cancerCurrently, tomographic imaging is widely used in medical and industrial non-destructive testing applications. X-ray tomography is the prevalent imaging technology. Modern medical X-ray CT scanners provide up to 1\u00a0mm spatial resolution. The disadvantage of X-ray tomography is that it cannot be used for regular medical examinations. Early breast cancer diagnosis is one of the most pressing issues in modern healthcare. Ultrasound tomography devices are being developed in USA, Germany and Russia to address this problem. One of the main challenges in ultrasound tomographic imaging is the development of efficient algorithms for solving inverse problems of wave tomography, which are nonlinear three-dimensional coefficient inverse problems for a hyperbolic differential equation. Solving such computationally-expensive problems requires the use of supercomputers.",
      "axisX": "0.43688532944446296",
      "axisY": "0.011319963326838326",
      "class": "1"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160104",
      "title": "Server Level Liquid Cooling: Do Higher System Temperatures Improve Energy Efficiency?",
      "authors": "Alexander A. Moskovsky, Egor A. Druzhinin, Alexey B. Shmelev, Vladimir V. Mironov, Andrey Semin",
      "keywords": "Server Level Liquid Cooling: Do Higher System Temperatures Improve Energy Efficiency?hot liquid cooling, cold plates, energy efficiencyLiquid cooling is now a mainstream approach to boost energy efficiency for high performance computing systems. Higher coolant temperature is usually considered as an advantage, since it allows heat reuse/recuperation and simplifies datacenter infrastructure by eliminating the need of chiller machine. However, the use of hot coolant imposes high requirements for cooling equipment. A promising approach is to utilize coldplates with channel structure and liquid circulation for heat removal from semiconductor components. We have designed a coldplate with low heat-resistance that ensures effective cooling with only 2030\u00b0 temperature difference between coolant and electronic parts of a server. Under stress-test conditions the coolant temperature was up to 65 \u00b0C while server operation was undisturbed. We also studied power efficiency (expressed in floating point operations per watt) dependence on the coolant temperature (19-65 \u00b0C) on theindividualserverlevel (based on Intel Grantley platform with dual Intel Xeon E5-2697v3 processors). \u0438The power performance ratio shows moderate (\u224810%) efficiency drop from 19 to 65\u00b0C due to increase of leak age current in chipset components and reduction of processor frequency resulted into proportional reduction of DGEMM benchmark performance. It must be taken into account by datacenter designers, that the amount of recuperated energy from 65 \u00b0C should be at least\u224810% to justify the choice of high temperature coolant solution.",
      "axisX": "-0.12139363167322384",
      "axisY": "-0.2956341083020934",
      "class": "2"
    },
    {
      "doi": "https://doi.org/10.14529/jsfi160105",
      "title": "Data Compression for Climate Data",
      "authors": "Michael Kuhn, Julian Kunkel, Thomas Ludwig",
      "keywords": "Data Compression for Climate Datadata compression, storage system, climate data, cost efficiencyThe different rates of increase for computational power and storage capabilities of supercomputers turn data storage into a technical and economical problem.\u00a0Because storage capabilities are lagging behind, investments and operational costs for storage systems have increased to keep up with the supercomputers' I/O requirements.\u00a0One promising approach is to reduce the amount of data that is stored.\u00a0In this paper, we take a look at the impact of compression on performance and costs of high performance systems.\u00a0To this end, we analyze the applicability of compression on all layers of the I/O stack, that is, main memory, network and storage.\u00a0Based on the Mistral system of the German Climate Computing Center (Deutsches Klimarechenzentrum, DKRZ), we illustrate potential performance improvements and cost savings.\u00a0Making use of compression on a large scale can decrease investments and operational costs by 50% without negatively impacting performance.\u00a0Additionally, we present ongoing work for supporting enhanced adaptive compression in the parallel distributed file system Lustre and application-specific compression.",
      "axisX": "-0.5531669028224642",
      "axisY": "-0.22987930822394842",
      "class": "2"
    }
  ],
  "clusters": [
    {
      "number": 0,
      "keywords": "system, computing, performance, data, using, paper, application, architecture, parallel, hardware"
    },
    {
      "number": 1,
      "keywords": "method, simulation, model, using, algorithm, numerical, approach, dynamic, used, calculation"
    },
    {
      "number": 2,
      "keywords": "system, performance, paper, data, result, computing, application, problem, method, approach"
    }
  ],
  "clusterQuantity": 3
}
